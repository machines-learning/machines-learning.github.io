<!DOCTYPE html>












  


<html class="theme-next gemini use-motion" lang="en">
<head><meta name="generator" content="Hexo 3.8.0">
  <meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
























<link rel="stylesheet" href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2">

<link rel="stylesheet" href="/css/main.css?v=7.1.0">


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=7.1.0">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=7.1.0">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=7.1.0">


  <link rel="mask-icon" href="/images/logo.svg?v=7.1.0" color="#222">







<script id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Gemini',
    version: '7.1.0',
    sidebar: {"position":"right","display":"always","offset":12,"onmobile":false,"dimmer":false},
    back2top: true,
    back2top_sidebar: false,
    fancybox: false,
    fastclick: false,
    lazyload: false,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>


  




  <meta name="description" content="本文在PyTorch官方文档的基础上，增加了描述性的文字与示例，对于一些文档没有写清楚的API的用法，参数的类型等进行了补充说明。最后一部分，深入源码的底层探索研究了Storage。您也可以前往 Google Colab 运行这篇文章中的代码。">
<meta property="og:type" content="article">
<meta property="og:title" content="PyTorch 文档 &gt; torch.Storage">
<meta property="og:url" content="https://artificial-intelligence.net.cn/2019/06/11/1560232044/index.html">
<meta property="og:site_name" content="Machine Learning">
<meta property="og:description" content="本文在PyTorch官方文档的基础上，增加了描述性的文字与示例，对于一些文档没有写清楚的API的用法，参数的类型等进行了补充说明。最后一部分，深入源码的底层探索研究了Storage。您也可以前往 Google Colab 运行这篇文章中的代码。">
<meta property="og:locale" content="en">
<meta property="og:image" content="https://raw.githubusercontent.com/machines-learning/image-repo/master/pytorch-doc-torch-storage/bc179ebf.png">
<meta property="og:image" content="https://raw.githubusercontent.com/machines-learning/image-repo/master/pytorch-doc-torch-storage/408c0d88.png">
<meta property="og:updated_time" content="2019-06-11T06:59:53.785Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="PyTorch 文档 &gt; torch.Storage">
<meta name="twitter:description" content="本文在PyTorch官方文档的基础上，增加了描述性的文字与示例，对于一些文档没有写清楚的API的用法，参数的类型等进行了补充说明。最后一部分，深入源码的底层探索研究了Storage。您也可以前往 Google Colab 运行这篇文章中的代码。">
<meta name="twitter:image" content="https://raw.githubusercontent.com/machines-learning/image-repo/master/pytorch-doc-torch-storage/bc179ebf.png">



  <link rel="alternate" href="/atom.xml" title="Machine Learning" type="application/atom+xml">



  
  
  <link rel="canonical" href="https://artificial-intelligence.net.cn/2019/06/11/1560232044/">



<script id="page.configurations">
  CONFIG.page = {
    sidebar: "",
  };
</script>

  <title>PyTorch 文档 > torch.Storage | Machine Learning</title>
  












  <noscript>
  <style>
  .use-motion .motion-element,
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-title { opacity: initial; }

  .use-motion .logo,
  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="en">

  
  
    
  

  <div class="container sidebar-position-right page-post-detail">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta">
    

    <div class="custom-logo-site-title">
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">Machine Learning</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
    
      
        <p class="site-subtitle">machine-learning's blog</p>
      
    
    
  </div>

  <div class="site-nav-toggle">
    <button aria-label="Toggle navigation bar">
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>



<nav class="site-nav">
  
    <ul id="menu" class="menu">
      
        
        
        
          
          <li class="menu-item menu-item-home">

    
    
    
      
    

    

    <a href="/" rel="section"><i class="menu-item-icon fa fa-fw fa-home"></i> <br>Home</a>

  </li>
        
        
        
          
          <li class="menu-item menu-item-about">

    
    
    
      
    

    

    <a href="/about/" rel="section"><i class="menu-item-icon fa fa-fw fa-user"></i> <br>About</a>

  </li>
        
        
        
          
          <li class="menu-item menu-item-tags">

    
    
    
      
    

    

    <a href="/tags/" rel="section"><i class="menu-item-icon fa fa-fw fa-tags"></i> <br>Tags</a>

  </li>
        
        
        
          
          <li class="menu-item menu-item-categories">

    
    
    
      
    

    

    <a href="/categories/" rel="section"><i class="menu-item-icon fa fa-fw fa-th"></i> <br>Categories</a>

  </li>
        
        
        
          
          <li class="menu-item menu-item-archives">

    
    
    
      
    

    

    <a href="/archives/" rel="section"><i class="menu-item-icon fa fa-fw fa-archive"></i> <br>Archives</a>

  </li>
        
        
        
          
          <li class="menu-item menu-item-schedule">

    
    
    
      
    

    

    <a href="/schedule/" rel="section"><i class="menu-item-icon fa fa-fw fa-calendar"></i> <br>Schedule</a>

  </li>

      
      
    </ul>
  

  

  
</nav>



  

<script type="text/javascript">
  var headband=document.getElementsByClassName("headband")[0];
  var githubContainer=document.createElement("div");
  githubContainer.innerHTML="" +
    "<a href='https://github.com/machines-learning' " +
    "   class=\"github-corner\" " +
    "   aria-label=\"View source on Github\">" +
    "   <svg width=\"80\" " +
    "        height=\"80\" " +
    "        viewBox=\"0 0 250 250\" " +
    "        style=\"fill:#151513; " +
    "        color:#fff; " +
    "        position: absolute; " +
    "        top: 0; " +
    "        border: 0; " +
    "        left: 0; " +
    "        transform: " +
    "        scale(-1, 1);\" " +
    "        aria-hidden=\"true\">" +
    "        <path d=\"M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z\"></path>" +
    "        <path d=\"M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 " +
    "120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 " +
    "125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2\" " +
    "              fill=\"currentColor\" " +
    "              style=\"transform-origin: 130px 106px;\" " +
    "              class=\"octo-arm\">" +
    "        </path>" +
    "        <path d=\"M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 " +
    "C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 " +
    "154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 " +
    "178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 " +
    "C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 " +
    "203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 " +
    "152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z\" " +
    "             fill=\"currentColor\" " +
    "             class=\"octo-body\">" +
    "         </path>" +
    "   </svg>" +
    "   <style>" +
    "         .github-corner:hover " +
    "         .octo-arm{animation:octocat-wave 560ms ease-in-out}" +
    "         @keyframes " +
    "         octocat-wave{0%,100%{transform:rotate(0)}20%," +
    "60%{transform:rotate(-25deg)}40%,80%{transform:rotate(10deg)}}" +
    "         @media (max-width:500px){.github-corner:hover .octo-arm{animation:none}.github-corner " +
    "         .octo-arm{animation:octocat-wave 560ms ease-in-out}}</style>" +
    "</a>";
  headband.appendChild(githubContainer);
</script>


</div>
    </header>

    


    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          
            

          
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://artificial-intelligence.net.cn/2019/06/11/1560232044/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="machine-learning">
      <meta itemprop="description" content="Computers are able to see, hear and learn. Welcome to the future.">
      <meta itemprop="image" content="/images/avatar.png">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Machine Learning">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">PyTorch 文档 > torch.Storage

              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              

              
                
              

              <time title="Created: 2019-06-11 13:47:24 / Modified: 14:59:53" itemprop="dateCreated datePublished" datetime="2019-06-11T13:47:24+08:00">2019-06-11</time>
            

            
              

              
            
          </span>

          

          
            
            
              
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
            
                <span class="post-meta-item-text">Comments: </span>
                <a href="/2019/06/11/1560232044/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count disqus-comment-count" data-disqus-identifier="2019/06/11/1560232044/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          
            <span id="/2019/06/11/1560232044/" class="leancloud_visitors" data-flag-title="PyTorch 文档 > torch.Storage">
              <span class="post-meta-divider">|</span>
              <span class="post-meta-item-icon">
                <i class="fa fa-eye"></i>
              </span>
              
                <span class="post-meta-item-text">Views: </span>
              
                <span class="leancloud-visitors-count"></span>
            </span>
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        <blockquote>
<p>本文在PyTorch官方文档的基础上，增加了描述性的文字与示例，对于一些文档没有写清楚的API的用法，参数的类型等进行了补充说明。最后一部分，深入源码的底层探索研究了Storage。您也可以前往 <a href="https://colab.research.google.com/drive/1G38Gum83Dx-cMTR-rI2pD4yX_57-vxgg" target="_blank" rel="noopener">Google Colab</a> 运行这篇文章中的代码。</p>
</blockquote>
<a id="more"></a>
<h2 id="什么是-torch-Storage"><a href="#什么是-torch-Storage" class="headerlink" title="什么是 torch.Storage"></a>什么是 torch.Storage</h2><p>每一个张量（torch.Tensor）都有一个相关联的存储空间（torch.Storage），用来存储它的数据。所以张量实际数据不是直接保存在torch.Tensor中，而是保存在名为torch.Storage的数据结构上。</p>
<h2 id="共有哪些存储空间"><a href="#共有哪些存储空间" class="headerlink" title="共有哪些存储空间"></a>共有哪些存储空间</h2><h3 id="查看-StorageBase的子类"><a href="#查看-StorageBase的子类" class="headerlink" title="查看_StorageBase的子类"></a>查看<code>_StorageBase</code>的子类</h3><p>通过查看PyTorch的源码，我们可以找到一个名为<a href="https://github.com/pytorch/pytorch/blob/master/torch/storage.py" target="_blank" rel="noopener"><code>_StorageBase</code></a>的类，它是所有Storage的父类。下面我们来看一下它的子类有哪些？</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch._StorageBase.__subclasses__()</span><br></pre></td></tr></table></figure>
<p>输出：</p>
<blockquote>
<p>[ torch.DoubleStorage,<br> torch.FloatStorage,<br> torch.HalfStorage,<br> torch.LongStorage,<br> torch.IntStorage,<br> torch.ShortStorage,<br> torch.CharStorage,<br> torch.ByteStorage,<br> torch.BoolStorage,<br> torch.cuda.DoubleStorage,<br> torch.cuda.FloatStorage,<br> torch.cuda.LongStorage,<br> torch.cuda.IntStorage,<br> torch.cuda.ShortStorage,<br> torch.cuda.CharStorage,<br> torch.cuda.ByteStorage,<br> torch.cuda.HalfStorage,<br> torch.cuda.BoolStorage ]</p>
</blockquote>
<p>可以看到，一共有两大类Storage，一类存储在CPU上，这类Storage在torch包下；另一类存储在GPU上，这类Storage在torch.cuda上。我们知道，PyTorch中的张量可以在CPU上计算，也可以在GPU上计算，所以张量在CPU与GPU上时，持有不同包下的Storage。</p>
<h3 id="示例1：创建张量并查看张量的存储空间的类别"><a href="#示例1：创建张量并查看张量的存储空间的类别" class="headerlink" title="示例1：创建张量并查看张量的存储空间的类别"></a>示例1：创建张量并查看张量的存储空间的类别</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.tensor([<span class="number">1</span>, <span class="number">2</span>]).storage()</span><br></pre></td></tr></table></figure>
<p>输出：</p>
<blockquote>
<p>1<br>2<br>[ torch.LongStorage of size 2 ]</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.tensor([<span class="number">1.</span>, <span class="number">2.</span>]).storage()</span><br></pre></td></tr></table></figure>
<p>输出：</p>
<blockquote>
<p>1.0<br>2.0<br>[torch.FloatStorage of size 2]</p>
</blockquote>
<h3 id="示例2：将张量移到GPU并查看张量的存储空间的类别"><a href="#示例2：将张量移到GPU并查看张量的存储空间的类别" class="headerlink" title="示例2：将张量移到GPU并查看张量的存储空间的类别"></a>示例2：将张量移到GPU并查看张量的存储空间的类别</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.tensor([<span class="number">1</span>, <span class="number">2</span>]).cuda().storage()</span><br></pre></td></tr></table></figure>
<p>输出：</p>
<blockquote>
<p>1<br>2<br>[ torch.cuda.LongStorage of size 2 ]</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.tensor([<span class="number">1.</span>, <span class="number">2.</span>]).cuda().storage()</span><br></pre></td></tr></table></figure>
<p>输出：</p>
<blockquote>
<p>1.0<br>2.0<br>[ torch.cuda.FloatStorage of size 2 ]</p>
</blockquote>
<h2 id="官方文档API介绍与简单示例"><a href="#官方文档API介绍与简单示例" class="headerlink" title="官方文档API介绍与简单示例"></a>官方文档API介绍与简单示例</h2><p><code>torch.Storage</code>是一个单一数据类型的连续一维数组.</p>
<p>每一个<a href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor" title="torch.Tensor" target="_blank" rel="noopener"><code>torch.Tensor</code></a>都有一个相同数据类型的对应存储.</p>
<blockquote>
<h5 id="CLASS-torch-FloatStorage"><a href="#CLASS-torch-FloatStorage" class="headerlink" title="CLASS torch.FloatStorage"></a><font color="red">CLASS</font> <code>torch.FloatStorage</code></h5></blockquote>
<ul>
<li><p><strong><code>bool()</code></strong>   </p>
<ul>
<li>将此存储类型转换为bool类型</li>
<li><p>示例</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.tensor([<span class="number">0</span>, <span class="number">2</span>]).storage().bool()</span><br></pre></td></tr></table></figure>
<p>输出：</p>
<blockquote>
<p>False<br>True<br>[ torch.BoolStorage of size 2 ]</p>
</blockquote>
</li>
</ul>
</li>
</ul>
<ul>
<li><p><strong><code>byte()</code></strong></p>
<ul>
<li>将此存储类型转换为byte类型</li>
<li><p>示例</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.tensor([<span class="number">256</span>, <span class="number">257</span>]).storage().byte()</span><br></pre></td></tr></table></figure>
<p>输出：</p>
<blockquote>
<p>0<br>1<br>[ torch.ByteStorage of size 2 ]</p>
</blockquote>
</li>
</ul>
</li>
</ul>
<ul>
<li><p><strong><code>char()</code></strong></p>
<ul>
<li>将此存储类型转换为char类型</li>
<li><p>示例</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.tensor([<span class="number">256</span>, <span class="number">257</span>]).storage().char()</span><br></pre></td></tr></table></figure>
<p>输出：</p>
<blockquote>
<p>0<br>1<br>[ torch.CharStorage of size 2 ]</p>
</blockquote>
</li>
</ul>
</li>
</ul>
<ul>
<li><p><strong><code>clone()</code></strong></p>
<ul>
<li>返回此存储的副本</li>
<li><p>示例</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">tensor_storage = torch.tensor([<span class="number">1</span>, <span class="number">2</span>]).storage()</span><br><span class="line">tensor_storage_copy = tensor_storage.clone()</span><br><span class="line">tensor_storage_copy</span><br></pre></td></tr></table></figure>
<p>输出：</p>
<blockquote>
<p>1<br>2<br>[ torch.LongStorage of size 2 ] </p>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">tensor_storage_copy[<span class="number">0</span>] = <span class="number">3</span> </span><br><span class="line">tensor_storage_copy</span><br></pre></td></tr></table></figure>
<p>输出：</p>
<blockquote>
<p>3<br>2<br>[ torch.LongStorage of size 2 ]</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tensor_storage</span><br></pre></td></tr></table></figure>
<p>输出：</p>
<blockquote>
<p>1<br>2<br>[ torch.LongStorage of size 2 ]</p>
</blockquote>
</li>
</ul>
</li>
</ul>
<ul>
<li><p><strong><code>copy_()</code></strong></p>
<ul>
<li>返回此存储的副本</li>
<li><p>示例</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">tensor_storage = torch.tensor([<span class="number">1</span>, <span class="number">2</span>]).storage()</span><br><span class="line">tensor_storage_copy = tensor_storage.copy_(tensor_storage)</span><br><span class="line">tensor_storage_copy</span><br></pre></td></tr></table></figure>
<p>输出：</p>
<blockquote>
<p>1<br>2<br>[ torch.LongStorage of size 2 ]</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tensor_storage_copy[<span class="number">0</span>] = tensor_storage_copy</span><br></pre></td></tr></table></figure>
<p>输出：</p>
<blockquote>
<p>3<br>2<br>[ torch.LongStorage of size 2 ]</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tensor_storage</span><br></pre></td></tr></table></figure>
<p>输出：</p>
<blockquote>
<p>3<br>2<br>[ torch.LongStorage of size 2 ]</p>
</blockquote>
</li>
<li><p><font color="red">注意</font></p>
<ul>
<li><p><code>copy_()</code>方法的调用方式与文档不太一样，不加参数会报错</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">tensor_storage = torch.tensor([<span class="number">1</span>, <span class="number">2</span>]).storage()</span><br><span class="line">tensor_storage.copy_()</span><br></pre></td></tr></table></figure>
<p>输出：<br><img width="70%" src="https://raw.githubusercontent.com/machines-learning/image-repo/master/pytorch-doc-torch-storage/bc179ebf.png"></p>
</li>
<li>调用<code>copy_()</code>方法后，改变副本的值，原本的值也发生改变</li>
<li>个人建议用<code>clone()</code>方法代替<code>copy_()</code>方法，因为一般我们创建副本时不希望改变原来的对象或值，目前看来<code>copy_()</code>方法貌似违背设计初衷，而且这个方法实际调用与文档不一致，可能存在BUG。本人Google之后暂时未找到原因。</li>
</ul>
</li>
</ul>
</li>
</ul>
<ul>
<li><p><strong><code>cpu()</code></strong></p>
<ul>
<li>如果此存储不在CPU上，返回此存储在CPU上的副本</li>
<li><p>示例</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.tensor([<span class="number">1</span>, <span class="number">2</span>]).storage().cpu()</span><br></pre></td></tr></table></figure>
<p>输出：</p>
<blockquote>
<p>1<br>2<br>[ torch.LongStorage of size 2 ]</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.tensor([<span class="number">1</span>, <span class="number">2</span>], device=<span class="string">'cuda'</span>).storage().cpu()</span><br></pre></td></tr></table></figure>
<p>输出：</p>
<blockquote>
<p>1<br>2<br>[ torch.LongStorage of size 2 ]</p>
</blockquote>
</li>
</ul>
</li>
</ul>
<ul>
<li><p><b><code>cuda(device=None, non_blocking=False, **kwargs)</code></b></p>
<ul>
<li>返回此存储在CUDA内存上的副本。如果此存储已经在正确的设备上的CUDA内存上，不执行复制操作，返回原始对象。</li>
<li>参数<ul>
<li><strong>device</strong>(<a href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)" target="_blank" rel="noopener"><em>int</em></a>) – 目标GPU的id，默认为当前设备</li>
<li><strong>non_blocking</strong>(<a href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.7)" target="_blank" rel="noopener"><em>bool</em></a>) – 如果将这个参数设为<code>True</code>并且数据源位于固定内存中，则副本相对于宿主是异步的。否则，这个参数不起任何作用</li>
<li><b>**kwargs</b>– 为了兼容性，可以用关键字<code>async</code>替代参数<code>non_blocking</code></li>
</ul>
</li>
<li><p>示例</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.tensor([<span class="number">1</span>, <span class="number">2</span>]).storage().cuda()</span><br></pre></td></tr></table></figure>
<p>输出：</p>
<blockquote>
<p>1<br>2<br>[ torch.cuda.LongStorage of size 2 ]</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.tensor([<span class="number">1</span>, <span class="number">2</span>], device=<span class="string">'cuda'</span>).storage().cuda()</span><br></pre></td></tr></table></figure>
<p>输出：</p>
<blockquote>
<p>1<br>2<br>[ torch.cuda.LongStorage of size 2 ]</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.tensor([<span class="number">1</span>, <span class="number">2</span>]).storage().cuda(non_blocking=<span class="keyword">True</span>)</span><br></pre></td></tr></table></figure>
<p>输出：</p>
<blockquote>
<p>1<br>2<br>[ torch.cuda.LongStorage of size 2 ]</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.tensor([<span class="number">1</span>, <span class="number">2</span>]).storage().cuda(<span class="keyword">async</span>=<span class="keyword">True</span>)</span><br></pre></td></tr></table></figure>
<p>输出：</p>
<blockquote>
<p>/usr/local/lib/python3.6/dist-packages/torch/_utils.py:85: UserWarning: ‘async’ is deprecated; use ‘non_blocking’<br>warnings.warn(“‘async’ is deprecated; use ‘non_blocking’”)<br>1<br>2<br>[ torch.cuda.LongStorage of size 2 ]</p>
</blockquote>
</li>
</ul>
</li>
<li><p><strong><code>data_ptr()</code></strong></p>
<ul>
<li>返回数据所在的地址</li>
<li><p>示例</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.tensor([<span class="number">1</span>, <span class="number">2</span>]).storage().data_ptr()</span><br></pre></td></tr></table></figure>
<p>输出：</p>
</li>
</ul>
<blockquote>
<p>2991586816</p>
</blockquote>
</li>
<li><p><strong><code>device</code></strong></p>
<ul>
<li>存储所在的设备</li>
<li><p>示例</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.tensor([<span class="number">1</span>, <span class="number">2</span>]).storage().device</span><br></pre></td></tr></table></figure>
<p>输出：</p>
<blockquote>
<p>device(type=’cpu’)</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.tensor([<span class="number">1</span>, <span class="number">2</span>], device=<span class="string">"cuda"</span>).storage().device</span><br></pre></td></tr></table></figure>
<p>输出：<br>device(type=’cuda’, index=0)</p>
</li>
</ul>
</li>
</ul>
<ul>
<li><p><strong><code>double()</code></strong></p>
<ul>
<li>将此存储类型转换为doule类型</li>
<li><p>示例</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.tensor([<span class="number">1</span>, <span class="number">2</span>]).storage().double()</span><br></pre></td></tr></table></figure>
<p>输出：</p>
<blockquote>
<p>1.0<br>2.0<br>[ torch.DoubleStorage of size 2 ]</p>
</blockquote>
</li>
</ul>
</li>
</ul>
<ul>
<li><p><strong><code>dtype</code></strong></p>
<ul>
<li>此存储中的数据类型</li>
<li><p>示例</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.tensor([<span class="number">1</span>, <span class="number">2</span>]).storage().dtype</span><br></pre></td></tr></table></figure>
<p>输出：</p>
<blockquote>
<p>torch.int64</p>
</blockquote>
</li>
</ul>
</li>
</ul>
<ul>
<li><p><strong><code>element_size()</code></strong></p>
<ul>
<li>元素大小，以字节为单位</li>
<li><p>示例</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.tensor([<span class="number">1</span>]).storage().int().element_size()</span><br></pre></td></tr></table></figure>
<p>输出：</p>
<blockquote>
<p>4</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.tensor([<span class="number">1</span>]).storage().long().element_size()</span><br></pre></td></tr></table></figure>
<p>输出：</p>
<blockquote>
<p>8</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.tensor([<span class="number">1</span>]).storage().byte().element_size()</span><br></pre></td></tr></table></figure>
<p>输出：</p>
<blockquote>
<p>1</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.tensor([<span class="number">1</span>]).storage().float().element_size()</span><br></pre></td></tr></table></figure>
<p>输出：</p>
<blockquote>
<p>4</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.tensor([<span class="number">1</span>]).storage().double().element_size()</span><br></pre></td></tr></table></figure>
<p>输出：</p>
<blockquote>
<p>8</p>
</blockquote>
</li>
</ul>
</li>
</ul>
<ul>
<li><p><strong><code>fill_()</code></strong></p>
<ul>
<li>用整型或布尔类型填充</li>
<li><p>示例</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.tensor([<span class="number">1</span>, <span class="number">2</span>]).storage().fill_(<span class="number">5</span>)</span><br></pre></td></tr></table></figure>
<p>输出：</p>
<blockquote>
<p>5<br>5<br>[ torch.LongStorage of size 2 ]</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.tensor([<span class="keyword">True</span>, <span class="keyword">True</span>]).storage().fill_(<span class="keyword">False</span>)</span><br></pre></td></tr></table></figure>
<p>输出：</p>
<blockquote>
<p>0<br>0<br>[ torch.ByteStorage of size 2 ]</p>
</blockquote>
</li>
</ul>
</li>
</ul>
<ul>
<li><p><strong><code>float()</code></strong></p>
<ul>
<li>将此数据类型转换为float类型</li>
<li><p>示例：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.tensor([<span class="number">1</span>, <span class="number">2</span>]).storage().float()</span><br></pre></td></tr></table></figure>
<p>输出：</p>
<blockquote>
<p>1.0<br>2.0<br>[ torch.FloatStorage of size 2 ]</p>
</blockquote>
</li>
</ul>
</li>
<li><p><font color="red">STATIC</font> <b><code>from_buffer()</code></b></p>
<ul>
<li><p>示例：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.FloatStorage.from_buffer(<span class="string">b"hell"</span>, <span class="string">'big'</span>)</span><br></pre></td></tr></table></figure>
<p>输出：</p>
<blockquote>
<p>4.333687820629062e+24<br>[ torch.FloatStorage of size 1 ]</p>
</blockquote>
</li>
</ul>
</li>
</ul>
<ul>
<li><p><font color="red">STATIC</font> <b><code>from_file(filename,shared=False,size=0)</code>→ Storage</b></p>
<ul>
<li>如果将 <i>shared</i> 设为 True, 那么会在所有进程之间共享内存。所有的改变都会写入文件中。如果将 <i>shared</i> 设为 False，那么在内存中做的改变不会影响到文件。<i>size</i> 是存储中元素的数量。 如果将 <i>shared</i> 设为 False, 那么文件必须包含至少sizeof(Type)字节（Type是存储的字节数）。如果将 <i>shared</i> 设为 True, 如果必要的话将创建文件</li>
<li>参数<ul>
<li><b>filename</b>(<i><a href="https://docs.python.org/3/library/stdtypes.html#str" target="_blank" rel="noopener">str</a></i>) – 被映射的文件的名字</li>
<li><b>shared</b>(<i><a href="https://docs.python.org/3/library/functions.html#bool" target="_blank" rel="noopener">bool</a></i>) – 是否共享内存</li>
<li><b>size</b>(<i><a href="https://docs.python.org/3/library/functions.html#int" target="_blank" rel="noopener">int</a></i>) – 存储中元素的数量</li>
</ul>
</li>
<li><p>示例</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">!touch tensor_file &amp; echo <span class="string">"1"</span> &gt; tensor_file</span><br><span class="line">!cat tensor_file</span><br></pre></td></tr></table></figure>
<p>输出：</p>
<blockquote>
<p>1</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.FloatStorage.from_file(<span class="string">'tensor_file'</span>)</span><br></pre></td></tr></table></figure>
<p>输出：</p>
<blockquote>
<p>[ torch.FloatStorage of size 0 ]</p>
</blockquote>
</li>
<li><p><font color="red">注意</font></p>
<ul>
<li>文档中并没有写文件的格式是怎样的，扒了源码以及C层代码后依旧未发现，容我日后慢慢研究研究。。。</li>
</ul>
</li>
</ul>
</li>
</ul>
<ul>
<li><p><strong><code>int</code></strong></p>
<ul>
<li>将此存储类型转换为int类型</li>
<li><p>示例</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.tensor([<span class="number">1.</span>, <span class="number">2.</span>]).storage().int()</span><br></pre></td></tr></table></figure>
<p>输出：</p>
<blockquote>
<p> 1<br> 2<br> [ torch.IntStorage of size 2 ]</p>
</blockquote>
</li>
</ul>
</li>
</ul>
<ul>
<li><p><strong><code>half()</code></strong></p>
<ul>
<li>将此存储类型转换为half类型</li>
<li><p>示例</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.tensor([<span class="number">65519</span>, <span class="number">65520</span>]).storage().half()</span><br></pre></td></tr></table></figure>
<p>输出：</p>
<blockquote>
<p>65504.0<br>inf<br>[ torch.HalfStorage of size 2 ]</p>
</blockquote>
</li>
<li><font color="red">注意</font>

<ul>
<li>半精度浮点数 <a href="https://en.wikipedia.org/wiki/Half-precision_floating-point_format" target="_blank" rel="noopener">Half-precision floating-point format</a> 是一种被计算机使用的二进制浮点数据类型。半精度浮点数使用2个字节（16位）来存储。在IEEE 754-2008中，它被称作binary16。这种数据类型只适合存储对精度要求不高的数字，不适合用来计算。</li>
</ul>
</li>
</ul>
</li>
</ul>
<ul>
<li><p><b><code>is_cuda</code>= <font color="red">FALSE</font></b></p>
<ul>
<li>是否存储在cuda上</li>
<li><p>示例</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.tensor([<span class="number">1</span>, <span class="number">2</span>]).storage().is_cuda</span><br></pre></td></tr></table></figure>
<p>输出：</p>
<blockquote>
<p>False</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.tensor([<span class="number">1</span>, <span class="number">2</span>], device=<span class="string">"cuda"</span>).storage().is_cuda</span><br></pre></td></tr></table></figure>
<p>输出：</p>
<blockquote>
<p>True</p>
</blockquote>
</li>
</ul>
</li>
</ul>
<ul>
<li><p><strong><code>is_pinned()</code></strong></p>
<ul>
<li>是否位于固定内存</li>
<li><p>示例：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.tensor([<span class="number">1</span>, <span class="number">2</span>]).storage().is_pinned()</span><br></pre></td></tr></table></figure>
<p>输出：</p>
<blockquote>
<p>False</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.tensor([<span class="number">1</span>, <span class="number">2</span>], device=<span class="string">"cuda"</span>).storage().is_pinned()</span><br></pre></td></tr></table></figure>
<p>输出：</p>
<blockquote>
<p>False</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.tensor([<span class="number">1</span>, <span class="number">2</span>]).storage().pin_memory().is_pinned()</span><br></pre></td></tr></table></figure>
<p>输出：</p>
<blockquote>
<p>True</p>
</blockquote>
</li>
</ul>
</li>
</ul>
<ul>
<li><p><strong><code>is_shared()</code></strong></p>
<ul>
<li>是否内存共享</li>
<li><p>示例：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.tensor([<span class="number">1</span>, <span class="number">2</span>]).storage().is_shared()</span><br></pre></td></tr></table></figure>
<p>输出：</p>
<blockquote>
<p>False</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.tensor([<span class="number">1</span>, <span class="number">2</span>]).storage().share_memory_().is_shared()</span><br></pre></td></tr></table></figure>
<p>输出：</p>
<blockquote>
<p>True</p>
</blockquote>
</li>
</ul>
</li>
</ul>
<ul>
<li><p><strong><code>is_sparse</code></strong>= <font color="red">FALSE</font></p>
<ul>
<li>是否稀疏</li>
<li><p>示例</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.tensor([[<span class="number">1</span>, <span class="number">0</span>], [<span class="number">0</span>, <span class="number">0</span>]]).storage().is_sparse</span><br></pre></td></tr></table></figure>
<p>输出：</p>
<blockquote>
<p>False</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">i = torch.LongTensor([[<span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>], [<span class="number">2</span>, <span class="number">0</span>, <span class="number">2</span>]])</span><br><span class="line">v = torch.FloatTensor([<span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>])</span><br><span class="line">torch.sparse.FloatTensor(i, v, torch.Size([<span class="number">2</span>,<span class="number">3</span>])).storage().is_sparse</span><br></pre></td></tr></table></figure>
<p>输出：<br><img width="70%" src="https://raw.githubusercontent.com/machines-learning/image-repo/master/pytorch-doc-torch-storage/408c0d88.png"></p>
</li>
<li><p><font color="red">注意</font></p>
<ul>
<li>按照稀疏矩阵的定义，如果矩阵中0元素占大多数，该矩阵即为稀疏矩阵。但根据输出来看，并不是单纯按照这个定义来判断的。</li>
<li>通过查看文档，发现了一个名为<a href="https://pytorch.org/docs/stable/sparse.html" target="_blank" rel="noopener"><code>torch.sparse</code></a>的包，目前这个包下的API是实验性的，未来可能会改变。</li>
<li>根据官方示例，推测PyTorch为稀疏矩阵单独定义了一套数据的存储方式，其张量位于<code>torch.sparse</code>包下，例如<code>torch.sparse.FloatTensor</code></li>
<li>但是根据输出结果，<code>torch.sparse.FloatTensor</code>未持有Storage，所以这个字段可以暂时忽略，待官方完善后再学习如何使用</li>
</ul>
</li>
</ul>
</li>
</ul>
<ul>
<li><p><strong><code>long()</code></strong></p>
<ul>
<li>将此存储类型转换为long类型</li>
<li><p>示例</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.tensor([<span class="number">1.</span>, <span class="number">2.</span>]).storage().long()</span><br></pre></td></tr></table></figure>
<p>输出：</p>
<blockquote>
<p>1<br>2<br>[ torch.LongStorage of size 2 ]</p>
</blockquote>
</li>
</ul>
</li>
</ul>
<ul>
<li><p><strong><code>new()</code></strong></p>
<ul>
<li>由已有的Storage创建出新的Storage，新的Storage的类型与已有的Storage的类型相同</li>
<li><p>示例</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.tensor([<span class="number">1</span>, <span class="number">2</span>]).storage().new()</span><br></pre></td></tr></table></figure>
<p>输出：</p>
<blockquote>
<p>[ torch.LongStorage of size 0 ]</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.tensor([<span class="number">1.</span>, <span class="number">2.</span>]).storage().new()</span><br></pre></td></tr></table></figure>
<p>输出：</p>
<blockquote>
<p>[ torch.FloatStorage of size 0 ]</p>
</blockquote>
</li>
</ul>
</li>
</ul>
<ul>
<li><p><strong><code>pin_memory()</code></strong></p>
<ul>
<li>将存储复制到固定的内存(如果它还没有固定)</li>
<li><p>示例  </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.tensor([<span class="number">1</span>, <span class="number">2</span>]).storage().pin_memory()</span><br></pre></td></tr></table></figure>
<p>输出：</p>
<blockquote>
<p>1<br>2<br>[ torch.LongStorage of size 2 ]</p>
</blockquote>
</li>
</ul>
</li>
</ul>
<ul>
<li><p><strong><code>resize_()</code></strong></p>
<ul>
<li>改变张量的形状</li>
<li><p>示例</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.tensor([[<span class="number">0</span>, <span class="number">0</span>], [<span class="number">0</span>, <span class="number">0</span>]]).resize_(<span class="number">1</span>, <span class="number">4</span>)</span><br></pre></td></tr></table></figure>
<p>输出：</p>
<blockquote>
<p>tensor([[0, 0, 0, 0]])</p>
</blockquote>
</li>
</ul>
</li>
</ul>
<ul>
<li><p><strong><code>share_memory_()</code></strong></p>
<ul>
<li>将此存储移动到共享内存中<br>对于已经在共享内存中的Storage和CUDA Storage，无需移动以在进程之间共享。共享内存中的Storage无法调整大小。<br>返回：自己</li>
<li><p>示例：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.tensor([<span class="number">1</span>, <span class="number">2</span>]).storage().share_memory_()</span><br></pre></td></tr></table></figure>
<p>输出：</p>
<blockquote>
<p>1<br>2<br>[ torch.LongStorage of size 2 ]</p>
</blockquote>
</li>
</ul>
</li>
</ul>
<ul>
<li><p><strong><code>short()</code></strong></p>
<ul>
<li>将此存储类型转化为short类型</li>
<li><p>示例</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.tensor([<span class="number">1</span>, <span class="number">2</span>]).storage().short()</span><br></pre></td></tr></table></figure>
<p>输出：</p>
<blockquote>
<p>1<br>2<br>[ torch.ShortStorage of size 2 ]</p>
</blockquote>
</li>
</ul>
</li>
</ul>
<ul>
<li><p><strong><code>size()</code></strong></p>
<ul>
<li>存储中元素的个数</li>
<li><p>示例</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.tensor([<span class="number">1</span>, <span class="number">2</span>]).storage().size()</span><br></pre></td></tr></table></figure>
<p>输出：</p>
<blockquote>
<p>2</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.zeros(<span class="number">2</span>, <span class="number">2</span>).storage().size()</span><br></pre></td></tr></table></figure>
<p>输出：</p>
<blockquote>
<p>4</p>
</blockquote>
</li>
</ul>
</li>
</ul>
<ul>
<li><p><strong><code>tolist()</code></strong></p>
<ul>
<li>返回一个包含了存储中所有元素的列表</li>
<li><p>示例：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.zeros(<span class="number">2</span>, <span class="number">2</span>).storage().tolist()</span><br></pre></td></tr></table></figure>
<p>输出：</p>
<blockquote>
<p>[ 0.0, 0.0, 0.0, 0.0 ]</p>
</blockquote>
</li>
</ul>
</li>
</ul>
<ul>
<li><p><b><code>type(dtype=None,non_blocking=False,**kwargs)</code></b></p>
<ul>
<li>如果没有提供<em>dtype</em>，返回此Storage的类型，否则将此Stroage的类型强制转换为指定的类型R。<br>如果已经是正确的类型了，不执行复制操作，返回原始对象。</li>
<li>参数<ul>
<li><strong>dtype</strong>(<a href="https://docs.python.org/3/library/functions.html#type" title="(in Python v3.7)" target="_blank" rel="noopener"><em>type</em></a><em>or__string</em>) – 指定类型</li>
<li><strong>non_blocking</strong>(<a href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.7)" target="_blank" rel="noopener"><em>bool</em></a>) – 如果将这个参数设为<code>True</code>，并且数据源存储在固定内存中，目标在GPU中，反之亦然，则相对于宿主异步执行复制操作。否则，这个参数不起任何作用。</li>
<li><b>**kwargs</b>– 为了兼容性，可以用关键字<code>async</code>替代参数<code>non_blocking</code>。<code>async</code>参数已经弃用.</li>
</ul>
</li>
<li><p>示例</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.tensor([<span class="number">1.</span>, <span class="number">2.</span>]).storage().type(torch.LongStorage)</span><br></pre></td></tr></table></figure>
<p>输出：</p>
<blockquote>
<p>1<br>2<br>[ torch.LongStorage of size 2 ]</p>
</blockquote>
</li>
</ul>
</li>
</ul>
<h2 id="深入理解张量存储空间"><a href="#深入理解张量存储空间" class="headerlink" title="深入理解张量存储空间"></a>深入理解张量存储空间</h2><blockquote>
<p>正在编写中</p>
</blockquote>
<h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><ul>
<li><a href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor" target="_blank" rel="noopener">torch.Tensor — PyTorch master documentation</a></li>
<li><a href="https://pytorch.org/docs/stable/storage.html" target="_blank" rel="noopener">torch.Storage — PyTorch master documentation</a></li>
<li><a href="https://pytorch.org/docs/stable/sparse.html" target="_blank" rel="noopener">torch.sparse — PyTorch master documentation</a></li>
<li><a href="http://blog.christianperone.com/2018/03/pytorch-internal-architecture-tour" target="_blank" rel="noopener">PyTorch – Internal Architecture Tour | Terra Incognita</a></li>
<li><a href="https://github.com/pytorch/pytorch/blob/master/torch/storage.py" target="_blank" rel="noopener">pytorch/storage.py at master · pytorch/pytorch · GitHub</a></li>
<li><a href="https://github.com/pytorch/pytorch/tree/master/aten/src/TH" target="_blank" rel="noopener">pytorch/aten/src/TH at master · pytorch/pytorch · GitHub</a></li>
<li><a href="https://github.com/pytorch/pytorch/tree/master/aten/src/THC" target="_blank" rel="noopener">pytorch/aten/src/THC at master · pytorch/pytorch · GitHub</a></li>
<li><a href="https://github.com/pytorch/pytorch/tree/master/torch/sparse" target="_blank" rel="noopener">pytorch/torch/sparse at master · pytorch/pytorch · GitHub</a></li>
<li><a href="https://en.wikipedia.org/wiki/Sparse_matrix" target="_blank" rel="noopener">Sparse matrix - Wikipedia</a></li>
</ul>

      
    </div>

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2019/06/05/1559711811/" rel="next" title="深度学习 - LeCun、Bengio 和 Hinton 的联合综述">
                <i class="fa fa-chevron-left"></i> 深度学习 - LeCun、Bengio 和 Hinton 的联合综述
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
          </div>
        </div>
      

      
      
    </footer>
  </div>
  
  
  
  </article>


  </div>


          </div>
          

  
    <div class="comments" id="comments">
      <div id="disqus_thread">
        <noscript>Please enable JavaScript to view the comments powered by Disqus.</noscript>
      </div>
    </div>

  



        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
            Table of Contents
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview-wrap">
            Overview
          </li>
        </ul>
      

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <img class="site-author-image" itemprop="image" src="/images/avatar.png" alt="machine-learning">
            
              <p class="site-author-name" itemprop="name">machine-learning</p>
              <div class="site-description motion-element" itemprop="description">Computers are able to see, hear and learn. Welcome to the future.</div>
          </div>

          
            <nav class="site-state motion-element">
              
                <div class="site-state-item site-state-posts">
                
                  <a href="/archives/">
                
                    <span class="site-state-item-count">12</span>
                    <span class="site-state-item-name">posts</span>
                  </a>
                </div>
              

              

              
            </nav>
          

          
            <div class="feed-link motion-element">
              <a href="/atom.xml" rel="alternate">
                <i class="fa fa-rss"></i>
                RSS
              </a>
            </div>
          

          

          
            <div class="links-of-author motion-element">
              
                <span class="links-of-author-item">
                  
                  
                    
                  
                  
                    
                  
                  <a href="https://github.com/machines-learning" title="GitHub &rarr; https://github.com/machines-learning" rel="noopener" target="_blank"><i class="fa fa-fw fa-github"></i>GitHub</a>
                </span>
              
                <span class="links-of-author-item">
                  
                  
                    
                  
                  
                    
                  
                  <a href="mailto:machine-learning@qq.com" title="E-Mail &rarr; mailto:machine-learning@qq.com" rel="noopener" target="_blank"><i class="fa fa-fw fa-envelope"></i>E-Mail</a>
                </span>
              
            </div>
          

          

          
          

          
            
          
          
        </div>
      </div>

      
      <!--noindex-->
        <div class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
            
            
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#什么是-torch-Storage"><span class="nav-number">1.</span> <span class="nav-text">什么是 torch.Storage</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#共有哪些存储空间"><span class="nav-number">2.</span> <span class="nav-text">共有哪些存储空间</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#查看-StorageBase的子类"><span class="nav-number">2.1.</span> <span class="nav-text">查看_StorageBase的子类</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#示例1：创建张量并查看张量的存储空间的类别"><span class="nav-number">2.2.</span> <span class="nav-text">示例1：创建张量并查看张量的存储空间的类别</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#示例2：将张量移到GPU并查看张量的存储空间的类别"><span class="nav-number">2.3.</span> <span class="nav-text">示例2：将张量移到GPU并查看张量的存储空间的类别</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#官方文档API介绍与简单示例"><span class="nav-number">3.</span> <span class="nav-text">官方文档API介绍与简单示例</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#CLASS-torch-FloatStorage"><span class="nav-number">3.0.0.1.</span> <span class="nav-text">CLASS torch.FloatStorage</span></a></li></ol></li></ol><li class="nav-item nav-level-2"><a class="nav-link" href="#深入理解张量存储空间"><span class="nav-number">4.</span> <span class="nav-text">深入理解张量存储空间</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#参考资料"><span class="nav-number">5.</span> <span class="nav-text">参考资料</span></a></li></div>
            

          </div>
        </div>
      <!--/noindex-->
      

      

    </div>
  </aside>
  


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; 2018 – <span itemprop="copyrightYear">2019</span>
  <span class="with-love" id="animate">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">machine-learning</span>

  

  
</div>


  <div class="powered-by">Powered by <a href="https://hexo.io" class="theme-link" rel="noopener" target="_blank">Hexo</a> v3.8.0</div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">Theme – <a href="https://theme-next.org" class="theme-link" rel="noopener" target="_blank">NexT.Gemini</a> v7.1.0</div>




        








        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

    

    
  </div>

  

<script>
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>


























  
  <script src="/lib/jquery/index.js?v=2.1.3"></script>

  
  <script src="/lib/velocity/velocity.min.js?v=1.2.1"></script>

  
  <script src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>


  


  <script src="/js/utils.js?v=7.1.0"></script>

  <script src="/js/motion.js?v=7.1.0"></script>



  
  


  <script src="/js/affix.js?v=7.1.0"></script>

  <script src="/js/schemes/pisces.js?v=7.1.0"></script>




  
  <script src="/js/scrollspy.js?v=7.1.0"></script>
<script src="/js/post-details.js?v=7.1.0"></script>



  


  <script src="/js/next-boot.js?v=7.1.0"></script>


  

  

  

  
  
  <script id="dsq-count-scr" src="https://machine-learning.disqus.com/count.js" async></script>


<script>
  var disqus_config = function() {
    this.page.url = "https://artificial-intelligence.net.cn/2019/06/11/1560232044/";
    this.page.identifier = "2019/06/11/1560232044/";
    this.page.title = 'PyTorch 文档 > torch.Storage';
    };
  function loadComments() {
    var d = document, s = d.createElement('script');
    s.src = 'https://machine-learning.disqus.com/embed.js';
    s.setAttribute('data-timestamp', '' + +new Date());
    (d.head || d.body).appendChild(s);
  }
  
    loadComments();
  
</script>





  


  




  
  
  <script>
    
    function addCount(Counter) {
      var $visitors = $('.leancloud_visitors');
      var url = $visitors.attr('id').trim();
      var title = $visitors.attr('data-flag-title').trim();

      Counter('get', '/classes/Counter', { where: JSON.stringify({ url }) })
        .done(function({ results }) {
          if (results.length > 0) {
            var counter = results[0];
            
            Counter('put', '/classes/Counter/' + counter.objectId, JSON.stringify({ time: { '__op': 'Increment', 'amount': 1 } }))
            
              .done(function() {
                var $element = $(document.getElementById(url));
                $element.find('.leancloud-visitors-count').text(counter.time + 1);
              })
            
              .fail(function ({ responseJSON }) {
                console.log('Failed to save Visitor num, with error message: ' + responseJSON.error);
              })
          } else {
            
              Counter('post', '/classes/Counter', JSON.stringify({ title: title, url: url, time: 1 }))
                .done(function() {
                  var $element = $(document.getElementById(url));
                  $element.find('.leancloud-visitors-count').text(1);
                })
                .fail(function() {
                  console.log('Failed to create');
                });
            
          }
        })
        .fail(function ({ responseJSON }) {
          console.log('LeanCloud Counter Error: ' + responseJSON.code + ' ' + responseJSON.error);
        });
    }
    

    $(function() {
      $.get('https://app-router.leancloud.cn/2/route?appId=' + '4MkhJfDS1HKNXXak66r0L0GT-gzGzoHsz')
        .done(function({ api_server }) {
          var Counter = function(method, url, data) {
            return $.ajax({
              method: method,
              url: 'https://' + api_server + '/1.1' + url,
              headers: {
                'X-LC-Id': '4MkhJfDS1HKNXXak66r0L0GT-gzGzoHsz',
                'X-LC-Key': 'GDo2dy3UKTJkiL6Oqq6c009Y',
                'Content-Type': 'application/json',
              },
              data: data
            });
          };
          
            addCount(Counter);
          
        });
    });
  </script>



  

  

  

  

  

  

  

  

  

  

  

  

  

</body>
</html>
