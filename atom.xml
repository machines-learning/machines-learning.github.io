<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Machine Learning</title>
  
  <subtitle>machine-learning&#39;s blog</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="https://artificial-intelligence.net.cn/"/>
  <updated>2019-06-05T06:16:26.973Z</updated>
  <id>https://artificial-intelligence.net.cn/</id>
  
  <author>
    <name>machine-learning</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>深度学习 - LeCun、Bengio 和 Hinton 的联合综述</title>
    <link href="https://artificial-intelligence.net.cn/2019/06/05/1559711811/"/>
    <id>https://artificial-intelligence.net.cn/2019/06/05/1559711811/</id>
    <published>2019-06-05T05:16:51.000Z</published>
    <updated>2019-06-05T06:16:26.973Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>深度学习可以让那些拥有多个处理层的计算模型来学习具有多层次抽象的数据的表示。这些方法在许多方面都带来了显著的改善，包括最先进的语音识别、视觉对象识别、对象检测和许多其它领域，例如药物发现和基因组学等。深度学习能够发现大数据中的复杂结构。它是利用BP算法来完成这个发现过程的。BP算法能够指导机器如何从前一层获取误差而改变本层的内部参数，这些内部参数可以用于计算表示。深度卷积网络在处理图像、视频、语音和音频方面带来了突破，而递归网络在处理序列数据，比如文本和语音方面表现出了闪亮的一面。</p></blockquote><a id="more"></a><p>机器学习技术在现代社会的各个方面表现出了强大的功能：从Web搜索到社会网络内容过滤，再到电子商务网站上的商品推荐都有涉足。并且它越来越多地出现在消费品中，比如相机和智能手机。</p><p>机器学习系统被用来识别图片中的目标，将语音转换成文本，匹配新闻元素，根据用户兴趣提供职位或产品，选择相关的搜索结果。逐渐地，这些应用使用一种叫深度学习的技术。传统的机器学习技术在处理未加工过的数据时，体现出来的能力是有限的。几十年来，想要构建一个模式识别系统或者机器学习系统，需要一个精致的引擎和相当专业的知识来设计一个特征提取器，把原始数据（如图像的像素值）转换成一个适当的内部特征表示或特征向量，子学习系统，通常是一个分类器，对输入的样本进行检测或分类。特征表示学习是一套给机器灌入原始数据，然后能自动发现需要进行检测和分类的表达的方法。<strong>深度学习就是一种特征学习方法，把原始数据通过一些简单的但是非线性的模型转变成为更高层次的，更加抽象的表达。通过足够多的转换的组合，非常复杂的函数也可以被学习。</strong>对于分类任务，高层次的表达能够强化输入数据的区分能力方面，同时削弱不相关因素。比如，一副图像的原始格式是一个像素数组，那么在第一层上的学习特征表达通常指的是在图像的特定位置和方向上有没有边的存在。第二层通常会根据那些边的某些排放而来检测图案，这时候会忽略掉一些边上的一些小的干扰。第三层或许会把那些图案进行组合，从而使其对应于熟悉目标的某部分。随后的一些层会将这些部分再组合，从而构成待检测目标。<strong>深度学习的核心方面是，上述各层的特征都不是利用人工工程来设计的，而是使用一种通用的学习过程从数据中学到的。</strong></p><p>深度学习正在取得重大进展，解决了人工智能界的尽最大努力很多年仍没有进展的问题。它已经被证明，它能够擅长发现高维数据中的复杂结构，因此它能够被应用于科学、商业和政府等领域。除了在图像识别、语音识别等领域打破了纪录，它还在另外的领域击败了其他机器学习技术，包括预测潜在的药物分子的活性、分析粒子加速器数据、重建大脑回路、预测在非编码DNA突变对基因表达和疾病的影响。也许更令人惊讶的是，深度学习在自然语言理解的各项任务中产生了非常可喜的成果，特别是主题分类、情感分析、自动问答和语言翻译。我们认为，<strong>在不久的将来，深度学习将会取得更多的成功，因为它需要很少的手工工程，它可以很容易受益于可用计算能力和数据量的增加。目前正在为深度神经网络开发的新的学习算法和架构只会加速这一进程。</strong></p><h1 id="监督学习"><a href="#监督学习" class="headerlink" title="监督学习"></a>监督学习</h1><p>机器学习中，不论是否是深层，最常见的形式是监督学习。试想一下，我们要建立一个系统，它能够对一个包含了一座房子、一辆汽车、一个人或一个宠物的图像进行分类。我们先收集大量的房子，汽车，人与宠物的图像的数据集，并对每个对象标上它的类别。在训练期间，机器会获取一副图片，然后产生一个输出，这个输出以向量形式的分数来表示，每个类别都有一个这样的向量。我们希望所需的类别在所有的类别中具有最高的得分，但是这在训练之前是不太可能发生的。通过计算一个目标函数可以获得输出分数和期望模式分数之间的误差（或距离）。然后机器会修改其内部可调参数，以减少这种误差。这些可调节的参数，通常被称为权值，它们是一些实数，可以被看作是一些“旋钮”，定义了机器的输入输出功能。在典型的深学习系统中，有可能有数以百万计的样本和权值，和带有标签的样本，用来训练机器。为了正确地调整权值向量，该学习算法计算每个权值的梯度向量，表示了如果权值增加了一个很小的量，那么误差会增加或减少的量。权值向量然后在梯度矢量的相反方向上进行调整。我们的目标函数，所有训练样本的平均，可以被看作是一种在权值的高维空间上的多变地形。负的梯度矢量表示在该地形中下降方向最快，使其更接近于最小值，也就是平均输出误差低最低的地方。</p><p>在实际应用中，大部分从业者都使用一种称作随机梯度下降的算法（SGD）。它包含了提供一些输入向量样本，计算输出和误差，计算这些样本的平均梯度，然后相应的调整权值。通过提供小的样本集合来重复这个过程用以训练网络，直到目标函数停止增长。它被称为随机的是因为小的样本集对于全体样本的平均梯度来说会有噪声估计。这个简单过程通常会找到一组不错的权值，同其他精心设计的优化技术相比，它的速度让人惊奇。训练结束之后，系统会通过不同的数据样本——测试集来显示系统的性能。这用于测试机器的泛化能力——对于未训练过的新样本的识别能力。</p><p>当前应用中的许多机器学习技术使用的是线性分类器来对人工提取的特征进行分类。一个2类线性分类器会计算特征向量的加权和。当加权和超过一个阈值之后，输入样本就会被分配到一个特定的类别中。从20世纪60年代开始，我们就知道了线性分类器只能够把样本分成非常简单的区域，也就是说通过一个超平面把空间分成两部分。</p><p>但像图像和语音识别等问题，它们需要的输入-输出函数要对输入样本中不相关因素的变化不要过于的敏感，如位置的变化，目标的方向或光照，或者语音中音调或语调的变化等，但是需要对于一些特定的微小变化非常敏感（例如，一只白色的狼和跟狼类似的白色狗——萨莫耶德犬之间的差异）。在像素这一级别上，两条萨莫耶德犬在不同的姿势和在不同的环境下的图像可以说差异是非常大的，然而，一只萨摩耶德犬和一只狼在相同的位置并在相似背景下的两个图像可能就非常类似。</p><center><img width="100%" height="100%" src="https://raw.githubusercontent.com/machines-learning/image-repo/master/nature-review-deep-learning/4a4b5e28.png"></center><center>图1 多层神经网络和BP算法</center><ol><li><strong>多层神经网络（用连接点表示）可以对输入空间进行整合，使得数据（红色和蓝色线表示的样本）线性可分</strong>。注意输入空间中的规则网格（左侧）是如何被隐藏层转换的（转换后的在右侧）。这个例子中只用了两个输入节点，两个隐藏节点和一个输出节点，但是用于目标识别或自然语言处理的网络通常包含数十个或者数百个这样的节点。获得 <a href="[http://colah.github.io/">C.Olah</a> 的许可后重新构建的这个图。</li><li><strong>链式法则告诉我们两个小的变化（x和y的微小变化，以及y和z的微小变化）是怎样组织到一起的。</strong>x的微小变化量Δx首先会通过乘以∂y/∂x（偏导数）转变成y的变化量Δy。类似的，Δy会给z带来改变Δz。通过链式法则可以将一个方程转化到另外的一个——也就是Δx通过乘以∂y/∂x和∂z/∂y（英文原文为∂z/∂x，系笔误——编辑注）得到Δz的过程。当x，y，z是向量的时候，可以同样处理（使用雅克比矩阵）。</li><li><strong>具有两个隐层一个输出层的神经网络中计算前向传播的公式。</strong>每个都有一个模块构成，用于反向传播梯度。在每一层上，我们首先计算每个节点的总输入z，z是前一层输出的加权和。然后利用一个非线性函数f(.)来计算节点的输出。简单期间，我们忽略掉了阈值项。神经网络中常用的非线性函数包括了最近几年常用的校正线性单元（ReLU）f(z) = max(0,z)，和更多传统sigmoid函数，比如双曲线正切函数f(z) = (exp(z) − exp(−z))/(exp(z) + exp(−z)) 和logistic函数f(z) = 1/(1 + exp(−z))。</li><li><strong>计算反向传播的公式。</strong>在隐层，我们计算每个输出单元产生的误差，这是由上一层产生的误差的加权和。然后我们将输出层的误差通过乘以梯度f(z)转换到输入层。在输出层上，每个节点的误差会用成本函数的微分来计算。如果节点l的成本函数是0.5*(yl-tl)^2, 那么节点的误差就是yl-tl，其中tl是期望值。一旦知道了∂E/∂zk的值，节点j的内星权向量wjk就可以通过yj ∂E/∂zk来进行调整。</li></ol><p>一个线性分类器或者其他操作在原始像素上的浅层分类器不能够区分后两者，虽然能够将前者归为同一类。这就是为什么浅分类要求有良好的特征提取器用于解决选择性不变性困境——提取器会挑选出图像中能够区分目标的那些重要因素，但是这些因素对于分辨动物的位置就无能为力了。为了加强分类能力，可以使用泛化的非线性特性，如核方法，但这些泛化特征，比如通过高斯核得到的，并不能够使得学习器从学习样本中产生较好的泛化效果。<strong>传统的方法是手工设计良好的特征提取器，这需要大量的工程技术和专业领域知识。但是如果通过使用通用学习过程而得到良好的特征，那么这些都是可以避免的了。这就是深度学习的关键优势。</strong></p><p>深度学习的体系结构是简单模块的多层栈，所有（或大部分）模块的目标是学习，还有许多计算非线性输入输出的映射。栈中的每个模块将其输入进行转换，以增加表达的可选择性和不变性。比如说，具有一个5到20层的非线性多层系统能够实现非常复杂的功能，比如输入数据对细节非常敏感——能够区分白狼和萨莫耶德犬，同时又具有强大的抗干扰能力，比如可以忽略掉不同的背景、姿势、光照和周围的物体等。</p><h1 id="反向传播来训练多层神经网络"><a href="#反向传播来训练多层神经网络" class="headerlink" title="反向传播来训练多层神经网络"></a>反向传播来训练多层神经网络</h1><p>在最早期的模式识别任务中，研究者的目标一直是使用可以训练的多层网络来替代经过人工选择的特征，虽然使用多层神经网络很简单，但是得出来的解很糟糕。直到20世纪80年代，使用简单的随机梯度下降来训练多层神经网络，这种糟糕的情况才有所改变。只要网络的输入和内部权值之间的函数相对平滑，使用梯度下降就凑效，梯度下降方法是在70年代到80年代期间由不同的研究团队独立发明的。  </p><p>用来求解目标函数关于多层神经网络权值梯度的反向传播算法（BP）只是一个用来求导的链式法则的具体应用而已。<strong>反向传播算法的核心思想是：目标函数对于某层输入的导数（或者梯度）可以通过向后传播对该层输出（或者下一层输入）的导数求得（如图1）。</strong>反向传播算法可以被重复的用于传播梯度通过多层神经网络的每一层：从该多层神经网络的最顶层的输出（也就是改网络产生预测的那一层）一直到该多层神经网络的最底层（也就是被接受外部输入的那一层），一旦这些关于（目标函数对）每层输入的导数求解完，我们就可以求解每一层上面的（目标函数对）权值的梯度了。</p><p>很多深度学习的应用都是使用前馈式神经网络（如图1），该神经网络学习一个从固定大小输入（比如输入是一张图）到固定大小输出（例如，到不同类别的概率）的映射。从第一层到下一层，计算前一层神经元输入数据的权值的和，然后把这个和传给一个非线性激活函数。当前最流行的非线性激活函数是rectified linear unit(ReLU)，函数形式：$f(z)=max(z,0)$。过去的几十年中，神经网络使用一些更加平滑的非线性函数，比如$tanh(z)$和$1/(1+exp(-z))$，但是ReLU通常会让一个多层神经网络学习的更快，也可以让一个深度网络直接有监督的训练（不需要无监督的pre-train）。</p><p>达到之前那种有pre-train的效果。通常情况下，输入层和输出层以外的神经单元被称为隐藏单元。隐藏层的作用可以看成是使用一个非线性的方式打乱输入数据，来让输入数据对应的类别在最后一层变得线性可分。</p><p>在20世纪90年代晚期，神经网络和反向传播算法被大多数机器学习团队抛弃，同时也不受计算机视觉和语音识别团队的重视。人们普遍认为，<strong>学习有用的、多级层次结构的、使用较少先验知识进行特征提取的这些方法都不靠谱。确切的说是因为简单的梯度下降会让整个优化陷入到不好的局部最小解</strong>。</p><p>实践中，如果在大的网络中，不管使用什么样的初始化条件，局部最小解并不算什么大问题，系统总是得到效果差不多的解。最近的理论和实验表明，局部最小解还真不是啥大问题。相反，解空间中充满了大量的鞍点（梯度为0的点），同时鞍点周围大部分曲面都是往上的。所以这些算法就算是陷入了这些局部最小值，关系也不太大。</p><p>2006年前后，CIFAR（加拿大高级研究院）把一些研究者聚集在一起，人们对深度前馈式神经网络重新燃起了兴趣。研究者们提出了一种<strong>非监督的学习方法</strong>，这种方法可以创建一些网络层来检测特征而不使用带标签的数据，这些网络层可以用来重构或者对特征检测器的活动进行建模。通过预训练过程，深度网络的权值可以被初始化为有意思的值。然后一个输出层被添加到该网络的顶部，并且使用标准的反向传播算法进行微调。这个工作对手写体数字的识别以及行人预测任务产生了显著的效果，尤其是带标签的数据非常少的时候。</p><p>使用这种与训练方法做出来的第一个比较大的应用是关于语音识别的，并且是在GPU上做的，这样做是因为写代码很方便，并且在训练的时候可以得到10倍或者20倍的加速。2009年，这种方法被用来映射短时间的系数窗口，该系统窗口是提取自声波并被转换成一组概率数字。它在一组使用很少词汇的标准的语音识别基准测试程序上达到了惊人的效果，然后又迅速被发展到另外一个更大的数据集上，同时也取得惊人的效果。从2009年到到2012年底，较大的语音团队开发了这种深度网络的多个版本并且已经被用到了安卓手机上。对于小的数据集来说，无监督的预训练可以防止过拟合，同时可以带来更好的泛化性能当有标签的样本很小的时候。一旦深度学习技术重新恢复，这种预训练只有在数据集合较少的时候才需要。</p><p>然后，还有一种深度前馈式神经网络，这种网络更易于训练并且比那种全连接的神经网络的泛化性能更好。这就是卷积神经网络（CNN）。当人们对神经网络不感兴趣的时候，卷积神经网络在实践中却取得了很多成功，如今它被计算机视觉团队广泛使用。</p><h1 id="卷积神经网络"><a href="#卷积神经网络" class="headerlink" title="卷积神经网络"></a>卷积神经网络</h1><p>卷积神经网络被设计用来处理到多维数组数据的，比如一个有3个包含了像素值2-D图像组合成的一个具有3个颜色通道的彩色图像。很多数据形态都是这种多维数组的：1D用来表示信号和序列包括语言，2D用来表示图像或者声音，3D用来表示视频或者有声音的图像。卷积神经网络使用4个关键的想法来利用自然信号的属性：局部连接、权值共享、池化以及多网络层的使用。</p><center><img width="100%" height="100%" src="https://raw.githubusercontent.com/machines-learning/image-repo/master/nature-review-deep-learning/1e9cdbad.png"></center><center>图2 卷积神经网络内部</center><p>一个典型的卷积神经网络结构（如图2）是由一系列的过程组成的。最初的几个阶段是由卷积层和池化层组成，卷积层的单元被组织在特征图中，在特征图中，每一个单元通过一组叫做滤波器的权值被连接到上一层的特征图的一个局部块，然后这个局部加权和被传给一个非线性函数，比如ReLU。在一个特征图中的全部单元享用相同的过滤器，不同层的特征图使用不同的过滤器。使用这种结构处于两方面的原因。首先，在数组数据中，比如图像数据，一个值的附近的值经常是高度相关的，可以形成比较容易被探测到的有区分性的局部特征。其次，不同位置局部统计特征不太相关的，也就是说，在一个地方出现的某个特征，也可能出现在别的地方，所以不同位置的单元可以共享权值以及可以探测相同的样本。在数学上，这种由一个特征图执行的过滤操作是一个离线的卷积，卷积神经网络也是这么得名来的。</p><p>卷积层的作用是探测上一层特征的局部连接，然而池化层的作用是在语义上把相似的特征合并起来，这是因为形成一个主题的特征的相对位置不太一样。一般地，池化单元计算特征图中的一个局部块的最大值，相邻的池化单元通过移动一行或者一列来从小块上读取数据，因为这样做就减少的表达的维度以及对数据的平移不变性。两三个这种的卷积、非线性变换以及池化被串起来，后面再加上一个更多卷积和全连接层。在卷积神经网络上进行反向传播算法和在一般的深度网络上是一样的，可以让所有的在过滤器中的权值得到训练。</p><p>深度神经网络利用的很多自然信号是层级组成的属性，在这种属性中高级的特征是通过对低级特征的组合来实现的。在图像中，局部边缘的组合形成基本图案，这些图案形成物体的局部，然后再形成物体。这种层级结构也存在于语音数据以及文本数据中，如电话中的声音，因素，音节，文档中的单词和句子。当输入数据在前一层中的位置有变化的时候，池化操作让这些特征表示对这些变化具有鲁棒性。</p><p>卷积神经网络中的卷积和池化层灵感直接来源于视觉神经科学中的简单细胞和复杂细胞。这种细胞的是以LNG-V1-V2-V4-IT这种层级结构形成视觉回路的。当给一个卷积神经网络和猴子一副相同的图片的时候，卷积神经网络展示了猴子下颞叶皮质中随机160个神经元的变化。卷积神经网络有神经认知的根源，他们的架构有点相似，但是在神经认知中是没有类似反向传播算法这种端到端的监督学习算法的。一个比较原始的1D卷积神经网络被称为时延神经网络，可以被用来识别语音以及简单的单词。</p><p>20世纪90年代以来，基于卷积神经网络出现了大量的应用。最开始是用时延神经网络来做语音识别以及文档阅读。这个文档阅读系统使用一个被训练好的卷积神经网络和一个概率模型，这个概率模型实现了语言方面的一些约束。20世纪90年代末，这个系统被用来美国超过10%的支票阅读上。后来，微软开发了基于卷积神经网络的字符识别系统以及手写体识别系统。20世纪90年代早期，卷积神经网络也被用来自然图形中的物体识别，比如脸、手以及人脸识别（face recognition ）。</p><h1 id="使用深度卷积网络进行图像理解"><a href="#使用深度卷积网络进行图像理解" class="headerlink" title="使用深度卷积网络进行图像理解"></a>使用深度卷积网络进行图像理解</h1><p>21世纪开始，卷积神经网络就被成功的大量用于检测、分割、物体识别以及图像的各个领域。这些应用都是使用了大量的有标签的数据，比如交通信号识别，生物信息分割，面部探测，文本、行人以及自然图形中的人的身体部分的探测。近年来，卷积神经网络的一个重大成功应用是人脸识别。</p><p>值得一提的是，图像可以在像素级别进行打标签，这样就可以应用在比如自动电话接听机器人、自动驾驶汽车等技术中。像Mobileye以及NVIDIA公司正在把基于卷积神经网络的方法用于汽车中的视觉系统中。其它的应用涉及到自然语言的理解以及语音识别中。</p><center><img width="100%" height="100%" src="https://raw.githubusercontent.com/machines-learning/image-repo/master/nature-review-deep-learning/79d9d520.png"></center><center>图3 从图像到文字</center><p>尽管卷积神经网络应用的很成功，但是它被计算机视觉以及机器学习团队开始重视是在2012年的ImageNet竞赛。在该竞赛中，深度卷积神经网络被用在上百万张网络图片数据集，这个数据集包含了1000个不同的类。该结果达到了前所未有的好，几乎比当时最好的方法降低了一半的错误率。这个成功来自有效地利用了GPU、ReLU、一个新的被称为dropout的正则技术，以及通过分解现有样本产生更多训练样本的技术。这个成功给计算机视觉带来一个革命。如今，卷积神经网络用于几乎全部的识别和探测任务中。最近一个更好的成果是，利用卷积神经网络结合回馈神经网络用来产生图像标题。</p><p>如今的卷积神经网络架构有10-20层采用ReLU激活函数、上百万个权值以及几十亿个连接。然而训练如此大的网络两年前就只需要几周了，现在硬件、软件以及算法并行的进步，又把训练时间压缩到了几小时。</p><p>基于卷积神经网络的视觉系统的性能已经引起了大型技术公司的注意，比如Google、Facebook、Microsoft、IBM，yahoo！、Twitter和Adobe等，一些快速增长的创业公司也同样如是。</p><p>卷积神经网络很容易在芯片或者现场可编程门阵列（FPGA）中高效实现，许多公司比如NVIDIA、Mobileye、Intel、Qualcomm以及Samsung，正在开发卷积神经网络芯片，以使智能机、相机、机器人以及自动驾驶汽车中的实时视觉系统成为可能。</p><h1 id="分布式特征表示与语言处理"><a href="#分布式特征表示与语言处理" class="headerlink" title="分布式特征表示与语言处理"></a>分布式特征表示与语言处理</h1><p>与不使用<strong>分布式特征表示</strong>（distributed representations ）的经典学习算法相比，深度学习理论表明深度网络具有两个不同的巨大的优势。这些优势来源于网络中各节点的权值，并取决于具有合理结构的底层生成数据的分布。首先，学习<strong>分布式特征表示</strong>能够泛化适应新学习到的特征值的组合（比如，n元特征就有2n种可能的组合）。其次，深度网络中组合表示层带来了另一个指数级的优势潜能（指数级的深度）。</p><p>多层神经网络中的隐层利用网络中输入的数据进行特征学习，使之更加容易预测目标输出。下面是一个很好的示范例子，比如将本地文本的内容作为输入，训练多层神经网络来预测句子中下一个单词。内容中的每个单词表示为网络中的N分之一的向量，也就是说，每个组成部分中有一个值为1其余的全为0。在第一层中，每个单词创建不同的激活状态，或单词向量（如图4）。在语言模型中，网络中其余层学习并转化输入的单词向量为输出单词向量来预测句子中下一个单词，可以通过预测词汇表中的单词作为文本句子中下一个单词出现的概率。网络学习了包含许多激活节点的、并且可以解释为词的独立特征的单词向量，正如第一次示范的文本学习分层表征文字符号的例子。这些语义特征在输入中并没有明确的表征。而是在利用“微规则”（‘micro-rules’,本文中直译为：微规则）学习过程中被发掘，并作为一个分解输入与输出符号之间关系结构的好的方式。当句子是来自大量的真实文本并且个别的微规则不可靠的情况下，学习单词向量也一样能表现得很好。利用训练好的模型预测新的事例时，一些概念比较相似的词容易混淆，比如星期二（Tuesday）和星期三（Wednesday），瑞典（Sweden）和挪威（Norway）。这样的表示方式被称为分布式特征表示，因为他们的元素之间并不互相排斥，并且他们的构造信息对应于观测到的数据的变化。这些单词向量是通过学习得到的特征构造的，这些特征不是由专家决定的，而是由神经网络自动发掘的。从文本中学习得单词向量表示现在广泛应用于自然语言中。</p><center><img width="100%" height="100%" src="https://raw.githubusercontent.com/machines-learning/image-repo/master/nature-review-deep-learning/20367861.png"></center><center>图4 词向量学习可视化</center><p>特征表示问题争论的中心介于对基于逻辑启发和基于神经网络的认识。在逻辑启发的范式中，一个符号实体表示某一事物，因为其唯一的属性与其他符号实体相同或者不同。该符号实例没有内部结构，并且结构与使用是相关的，至于理解符号的语义，就必须与变化的推理规则合理对应。相反地，神经网络利用了大量活动载体、权值矩阵和标量非线性化，来实现能够支撑简单容易的、具有常识推理的快速“直觉”功能。</p><p>在介绍神经语言模型前，简述下标准方法，其是基于统计的语言模型，该模型没有使用分布式特征表示。而是基于统计简短符号序列出现的频率增长到N（N-grams，N元文法）。可能的N-grams的数字接近于VN，其中V是词汇表的大小，考虑到文本内容包含成千上万个单词，所以需要一个非常大的语料库。N-grams将每个单词看成一个原子单元，因此不能在语义相关的单词序列中一概而论，然而神经网络语言模型可以，是因为他们关联每个词与真是特征值的向量，并且在向量空间中语义相关的词彼此靠近（图4）。</p><h1 id="递归神经网络"><a href="#递归神经网络" class="headerlink" title="递归神经网络"></a>递归神经网络</h1><p>首次引入反向传播算法时，最令人兴奋的便是使用递归神经网络（recurrent neural networks，下文简称RNNs）训练。对于涉及到序列输入的任务，比如语音和语言，利用RNNs能获得更好的效果。RNNs一次处理一个输入序列元素，同时维护网络中隐式单元中隐式的包含过去时刻序列元素的历史信息的“状态向量”。如果是深度多层网络不同神经元的输出，我们就会考虑这种在不同离散时间步长的隐式单元的输出，这将会使我们更加清晰怎么利用反向传播来训练RNNs（如图5，右）。</p><center><img width="50%" height="50%" src="https://raw.githubusercontent.com/machines-learning/image-repo/master/nature-review-deep-learning/12526b07.png"></center><center>图5 递归神经网络</center><p>RNNs是非常强大的动态系统，但是训练它们被证实存在问题的，因为反向传播的梯度在每个时间间隔内是增长或下降的，所以经过一段时间后将导致结果的激增或者降为零。</p><p>由于先进的架构和训练方式，RNNs被发现可以很好的预测文本中下一个字符或者句子中下一个单词，并且可以应用于更加复杂的任务。例如在某时刻阅读英语句子中的单词后，将会训练一个英语的“编码器”网络，使得隐式单元的最终状态向量能够很好地表征句子所要表达的意思或思想。这种“思想向量”（thought vector）可以作为联合训练一个法语“编码器”网络的初始化隐式状态（或者额外的输入），其输出为法语翻译首单词的概率分布。如果从分布中选择一个特殊的首单词作为编码网络的输入，将会输出翻译的句子中第二个单词的概率分布，并直到停止选择为止。总体而言，这一过程是根据英语句子的概率分布而产生的法语词汇序列。这种简单的机器翻译方法的表现甚至可以和最先进的（state-of-the-art）的方法相媲美，同时也引起了人们对于理解句子是否需要像使用推理规则操作内部符号表示质疑。这与日常推理中同时涉及到根据合理结论类推的观点是匹配的。</p><p>类比于将法语句子的意思翻译成英语句子，同样可以学习将图片内容“翻译”为英语句子（如图3）。这种编码器是可以在最后的隐层将像素转换为活动向量的深度卷积网络（ConvNet）。解码器与RNNs用于机器翻译和神经网络语言模型的类似。近来，已经掀起了一股深度学习的巨大兴趣热潮（参见文献[86]提到的例子）。</p><p>RNNs一旦展开（如图5），可以将之视为一个所有层共享同样权值的深度前馈神经网络。虽然它们的目的是学习长期的依赖性，但理论的和经验的证据表明很难学习并长期保存信息。</p><p>为了解决这个问题，一个增大网络存储的想法随之产生。采用了特殊隐式单元的LSTM（long short-termmemory networks）被首先提出，其自然行为便是长期的保存输入。一种称作记忆细胞的特殊单元类似累加器和门控神经元：它在下一个时间步长将拥有一个权值并联接到自身，拷贝自身状态的真实值和累积的外部信号，但这种自联接是由另一个单元学习并决定何时清除记忆内容的乘法门控制的。</p><p>LSTM网络随后被证明比传统的RNNs更加有效，尤其当每一个时间步长内有若干层时，整个语音识别系统能够完全一致的将声学转录为字符序列。目前LSTM网络或者相关的门控单元同样用于编码和解码网络，并且在机器翻译中表现良好。</p><p>过去几年中，几位学者提出了不同的提案用于增强RNNs的记忆模块。提案中包括神经图灵机，其中通过加入RNNs可读可写的“类似磁带”的存储来增强网络，而记忆网络中的常规网络通过联想记忆来增强。记忆网络在标准的问答基准测试中表现良好，记忆是用来记住稍后要求回答问题的事例。</p><p>除了简单的记忆化，神经图灵机和记忆网络正在被用于那些通常需要推理和符号操作的任务，还可以教神经图灵机“算法”。除此以外，他们可以从未排序的输入符号序列（其中每个符号都有与其在列表中对应的表明优先级的真实值）中，学习输出一个排序的符号序列。可以训练记忆网络用来追踪一个设定与文字冒险游戏和故事的世界的状态，回答一些需要复杂推理的问题。在一个测试例子中，网络能够正确回答15句版的《指环王》中诸如“Frodo现在在哪？”的问题。</p><h1 id="深度学习的未来展望"><a href="#深度学习的未来展望" class="headerlink" title="深度学习的未来展望"></a>深度学习的未来展望</h1><p>无监督学习对于重新点燃深度学习的热潮起到了促进的作用，但是纯粹的有监督学习的成功盖过了无监督学习。在本篇综述中虽然这不是我们的重点，我们还是期望无监督学习在长期内越来越重要。无监督学习在人类和动物的学习中占据主导地位：我们通过观察能够发现世界的内在结构，而不是被告知每一个客观事物的名称。</p><p>人类视觉是一个智能的、基于特定方式的利用小或大分辨率的视网膜中央窝与周围环绕区域对光线采集成像的活跃的过程。我们期望未来在机器视觉方面会有更多的进步，这些进步来自那些端对端的训练系统，并结合ConvNets和RNNs，采用增强学习来决定走向。结合了深度学习和增强学习的系统正处在初期，但已经在分类任务中超过了被动视频系统，并在学习操作视频游戏中产生了令人印象深刻的效果。</p><p>在未来几年，自然语言理解将是深度学习做出巨大影响的另一个领域。我们预测那些利用了RNNs的系统将会更好地理解句子或者整个文档，当它们选择性地学习了某时刻部分加入的策略。</p><p>最终，在人工智能方面取得的重大进步将来自那些结合了复杂推理表示学习（representation learning ）的系统。尽管深度学习和简单推理已经应用于语音和手写字识别很长一段时间了，我们仍需要通过操作大量向量的新范式来代替基于规则的字符表达式操作。</p><blockquote><p>这篇论文发表在2015年五月27日的 <a href="https://www.nature.com/articles/nature14539" target="_blank" rel="noopener">Nature</a> 上，译文首次发表在 <a href="https://www.csdn.net/article/2015-06-01/2824811" target="_blank" rel="noopener">CSDN</a> 上，由@kevin和@刘志远翻译，京东DNN实验室首席科学家@李成华友情审校。由于年代久远，无人维护，原翻译链接图片全部失效，同时阅读体验又不是很好，本人将图片补全，同时做了一些排版上的工作。</p></blockquote>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;深度学习可以让那些拥有多个处理层的计算模型来学习具有多层次抽象的数据的表示。这些方法在许多方面都带来了显著的改善，包括最先进的语音识别、视觉对象识别、对象检测和许多其它领域，例如药物发现和基因组学等。深度学习能够发现大数据中的复杂结构。它是利用BP算法来完成这个发现过程的。BP算法能够指导机器如何从前一层获取误差而改变本层的内部参数，这些内部参数可以用于计算表示。深度卷积网络在处理图像、视频、语音和音频方面带来了突破，而递归网络在处理序列数据，比如文本和语音方面表现出了闪亮的一面。&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>PyTorch 深度学习：60分钟速成</title>
    <link href="https://artificial-intelligence.net.cn/2019/06/04/1559642400/"/>
    <id>https://artificial-intelligence.net.cn/2019/06/04/1559642400/</id>
    <published>2019-06-04T10:00:00.000Z</published>
    <updated>2019-06-05T06:03:41.174Z</updated>
    
    <content type="html"><![CDATA[<p>本教程是PyTorch官方教程Getting Started篇的第一篇教程，更多教程请点击下方链接查看：</p><ul><li>PyTorch深度学习：60分钟速成</li><li><a href>数据加载与处理</a></li><li><a href>迁移学习</a></li><li><a href>使用混合前端部署Seq2Seq模型</a></li><li><a href>保存与加载模型</a></li><li><a href>torch.nn是什么</a></li></ul><a id="more"></a><h2 id="教学目标"><a href="#教学目标" class="headerlink" title="教学目标"></a>教学目标</h2><ul><li>高维度地理解PyTorch的Tensor库和神经网络</li><li>训练一个小的神经网络对图像进行分类</li></ul><blockquote><p>本教程假设您对 <a href="http://www.numpy.org" target="_blank" rel="noopener">NumPy</a> 已经有了基本的了解，请确保您已经安装了 <a href="https://github.com/pytorch/pytorch" target="_blank" rel="noopener">torch</a> 和 <a href="https://github.com/pytorch/vision" target="_blank" rel="noopener">torchvision</a> ，没有安装请前往 <a href="https://pytorch.org/get-started/locally/" target="_blank" rel="noopener">安装向导页面</a> 进行安装，也可以点击 <a href="https://drive.google.com/open?id=1ZBnuiwwkmKLQ6Xkb9mIOHXOeuAaRwUGj" target="_blank" rel="noopener">PyTorch 深度学习：60分钟速成</a> 前往Google Colab阅读教程并运行代码<br><strong>注意</strong>：本教程在原有教程的基础上增加了 Numpy 的对照代码，增加了部分补充说明与代码</p></blockquote><h2 id="目录"><a href="#目录" class="headerlink" title="目录"></a>目录</h2><ul><li><a href="#index1">一、什么是PyTorch ？</a><ul><li><a href="#index11">1 开始</a></li><li><a href="#index12">2 Numpy桥</a></li><li><a href="#index13">3 CUDA张量</a></li></ul></li><li><a href="#index2">二、Autograd：自动求导</a></li><li><a href="#index3">三、神经网络</a></li><li><a href="#index4">四、训练一个分类器</a></li><li><a href="#index5">五、选读：数据并行</a></li></ul><p><a name="index1"></a></p><h2 id="什么是PyTorch-？"><a href="#什么是PyTorch-？" class="headerlink" title="什么是PyTorch ？"></a>什么是PyTorch ？</h2><p>PyTorch是基于Python的科学计算包，它的特点如下：</p><ul><li>NumPy的替代品，可以使用GPU的强大功能</li><li>深度学习研究平台，提供最大的灵活性和速度<br><a name="index11"></a></li></ul><h3 id="开始"><a href="#开始" class="headerlink" title="开始"></a>开始</h3><h4 id="张量"><a href="#张量" class="headerlink" title="张量"></a>张量</h4><p>Torch中的张量tensor与NumPy中的ndarray类似，另外tensor还可以使用 GPU 加速计算。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> __future__ <span class="keyword">import</span> print_function</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br></pre></td></tr></table></figure><h5 id="构造一个未初始化的5x3的矩阵"><a href="#构造一个未初始化的5x3的矩阵" class="headerlink" title="构造一个未初始化的5x3的矩阵"></a>构造一个未初始化的5x3的矩阵</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">x = torch.empty(<span class="number">5</span>, <span class="number">3</span>)</span><br><span class="line">x</span><br></pre></td></tr></table></figure><p>输出：<br><img src="/.cn//06/04/1559642400/61584c35.png" title="61584c35.png"></p><p>使用NumPy构造一个未初始化的5x3的矩阵</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">x = np.empty((<span class="number">5</span>, <span class="number">3</span>))</span><br><span class="line">x</span><br></pre></td></tr></table></figure><p>输出：<br><img src="/.cn//06/04/1559642400/624451cc.png" title="624451cc.png"></p><h5 id="构造一个随机初始化的矩阵"><a href="#构造一个随机初始化的矩阵" class="headerlink" title="构造一个随机初始化的矩阵"></a>构造一个随机初始化的矩阵</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">x = torch.rand(<span class="number">5</span>, <span class="number">3</span>)</span><br><span class="line">x</span><br></pre></td></tr></table></figure><p>输出：<br><img src="/.cn//06/04/1559642400/8e67cc1f.png" title="8e67cc1f.png"></p><p>使用Numpy构造一个随机初始化的矩阵：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">x = np.random.rand(<span class="number">5</span>, <span class="number">3</span>)</span><br><span class="line">x</span><br></pre></td></tr></table></figure><p>输出：<br><img src="/.cn//06/04/1559642400/f2f5f7b6.png" title="f2f5f7b6.png"></p><h5 id="构造一个用0填充的矩阵，数据类型为long"><a href="#构造一个用0填充的矩阵，数据类型为long" class="headerlink" title="构造一个用0填充的矩阵，数据类型为long"></a>构造一个用0填充的矩阵，数据类型为long</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">x = torch.zeros(<span class="number">5</span>, <span class="number">3</span>, dtype=torch.long)</span><br><span class="line">x</span><br></pre></td></tr></table></figure><p>输出：<br><img src="/.cn//06/04/1559642400/a89ee470.png" title="a89ee470.png"></p><p>用NumPy构造一个用0填充的矩阵，数据类型为long </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">x = np.zeros((<span class="number">5</span>, <span class="number">3</span>), dtype=np.long)</span><br><span class="line">x</span><br></pre></td></tr></table></figure><p>输出：<br><img src="/.cn//06/04/1559642400/84d4e2f5.png" title="84d4e2f5.png"></p><h5 id="构造一个用已有数据初始化的tensor"><a href="#构造一个用已有数据初始化的tensor" class="headerlink" title="构造一个用已有数据初始化的tensor"></a>构造一个用已有数据初始化的tensor</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">x = torch.tensor([<span class="number">5.5</span>, <span class="number">3</span>])</span><br><span class="line">x</span><br></pre></td></tr></table></figure><p>输出：<br><img src="/.cn//06/04/1559642400/db7a767b.png" title="db7a767b.png"></p><p>用NumPy构造一个用已有数据初始化的ndarray </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">x = np.array([<span class="number">5.5</span>, <span class="number">3</span>])</span><br><span class="line">x</span><br></pre></td></tr></table></figure><p>输出：<br><img src="/.cn//06/04/1559642400/d2dad8b5.png" title="d2dad8b5.png"></p><h5 id="或者基于已有的tensor创建新的tensor，这些方法将重用输入的tensor的属性，例如，dtype，除非用户提供新值"><a href="#或者基于已有的tensor创建新的tensor，这些方法将重用输入的tensor的属性，例如，dtype，除非用户提供新值" class="headerlink" title="或者基于已有的tensor创建新的tensor，这些方法将重用输入的tensor的属性，例如，dtype，除非用户提供新值"></a>或者基于已有的tensor创建新的tensor，这些方法将重用输入的tensor的属性，例如，dtype，除非用户提供新值</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">x = torch.tensor([<span class="number">5.5</span>, <span class="number">3</span>])</span><br><span class="line">print(x)</span><br><span class="line">x = x.new_ones(<span class="number">5</span>, <span class="number">3</span>, dtype=torch.double)</span><br><span class="line">print(x)</span><br><span class="line">x = torch.randn_like(x, dtype=torch.float)</span><br><span class="line">print(x)</span><br></pre></td></tr></table></figure><p>输出：<br><img src="/.cn//06/04/1559642400/83edc98f.png" title="83edc98f.png"></p><p>查看tensor的size：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">x.size()</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">x.shape</span><br></pre></td></tr></table></figure><blockquote><p><strong>注意</strong>：</p><ul><li>官方教程并没有列出第二种写法，但是第二种写法确实存在并且能够正常运行。.shape 是 .size() 的别名，两种写法输出相同。具体可以看这两个issue：<ul><li><a href="https://github.com/pytorch/pytorch/issues/5544" target="_blank" rel="noopener">.size() vs .shape, which one should be used? · Issue #5544 · pytorch/pytorch · GitHub</a></li><li><a href="https://github.com/pytorch/pytorch/pull/1983" target="_blank" rel="noopener">add shape alias by hughperkins · Pull Request #1983 · pytorch/pytorch · GitHub</a></li></ul></li><li>torch.Size 实际上是一个元祖，因此它支持所有元祖操作。</li></ul></blockquote><p>输出：<br><img src="/.cn//06/04/1559642400/deb8d58b.png" title="deb8d58b.png"></p><p>使用 NumPy 基于已有的ndarray创造新的ndarray，</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">x = np.array([<span class="number">5.5</span>, <span class="number">3</span>])</span><br><span class="line">print(x)</span><br><span class="line">x = np.ones((<span class="number">5</span>, <span class="number">3</span>), dtype=np.double)</span><br><span class="line">print(x)</span><br><span class="line">x = np.random.rand(*x.shape)</span><br><span class="line">print(x)</span><br></pre></td></tr></table></figure><p>输出：<br><img src="/.cn//06/04/1559642400/1bff7a87.png" title="1bff7a87.png"></p><p>查看ndarray的shape ：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">x.shape</span><br></pre></td></tr></table></figure><p>输出：<br><img src="/.cn//06/04/1559642400/8a7007d8.png" title="8a7007d8.png"></p><h4 id="操作"><a href="#操作" class="headerlink" title="操作"></a>操作</h4><p>操作有多种语法，在下面的示例中，我们将研究加法操作。</p><h5 id="加法：语法1"><a href="#加法：语法1" class="headerlink" title="加法：语法1"></a>加法：语法1</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">x = torch.rand(<span class="number">5</span>, <span class="number">3</span>)</span><br><span class="line">print(x)</span><br><span class="line">y = torch.rand(<span class="number">5</span>, <span class="number">3</span>)</span><br><span class="line">print(y)</span><br><span class="line">print(x + y)</span><br></pre></td></tr></table></figure><p>输出：<br><img src="/.cn//06/04/1559642400/7d036c4b.png" title="7d036c4b.png"></p><p>NumPy 中的加法：语法1</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">x = np.random.rand(<span class="number">5</span>, <span class="number">3</span>)</span><br><span class="line">print(x)</span><br><span class="line">y = np.random.rand(<span class="number">5</span>, <span class="number">3</span>)</span><br><span class="line">print(y)</span><br><span class="line">print(x + y)</span><br></pre></td></tr></table></figure><p>输出：<br><img src="/.cn//06/04/1559642400/c6ccba98.png" title="c6ccba98.png"></p><h5 id="加法：语法2"><a href="#加法：语法2" class="headerlink" title="加法：语法2"></a>加法：语法2</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">x = torch.rand(<span class="number">5</span>, <span class="number">3</span>)</span><br><span class="line">print(x)</span><br><span class="line">y = torch.rand(<span class="number">5</span>, <span class="number">3</span>)</span><br><span class="line">print(y)</span><br><span class="line">print(torch.add(x, y))</span><br></pre></td></tr></table></figure><p>输出：<br><img src="/.cn//06/04/1559642400/777b8966.png" title="777b8966.png"></p><p>NumPy中的加法：语法2</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">x = np.random.rand(<span class="number">5</span>, <span class="number">3</span>)</span><br><span class="line">print(x)</span><br><span class="line">y = np.random.rand(<span class="number">5</span>, <span class="number">3</span>)</span><br><span class="line">print(y)</span><br><span class="line">print(np.add(x, y))</span><br></pre></td></tr></table></figure><p>输出：<br><img src="/.cn//06/04/1559642400/1d972dea.png" title="1d972dea.png"></p><h5 id="加法：提供一个输出tensor作为参数"><a href="#加法：提供一个输出tensor作为参数" class="headerlink" title="加法：提供一个输出tensor作为参数"></a>加法：提供一个输出tensor作为参数</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">x = torch.rand(<span class="number">5</span>, <span class="number">3</span>)</span><br><span class="line">print(x)</span><br><span class="line">y = torch.rand(<span class="number">5</span>, <span class="number">3</span>)</span><br><span class="line">print(y)</span><br><span class="line">result = torch.empty(<span class="number">5</span>, <span class="number">3</span>)</span><br><span class="line">print(result)</span><br><span class="line">torch.add(x, y, out=result)</span><br><span class="line">print(result)</span><br></pre></td></tr></table></figure><p>输出：<br><img src="/.cn//06/04/1559642400/6f84d7d0.png" title="6f84d7d0.png"></p><p>NumPy 加法：提供一个输出ndarray作为参数</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">x = np.random.rand(<span class="number">5</span>, <span class="number">3</span>)</span><br><span class="line">print(x)</span><br><span class="line">y = np.random.rand(<span class="number">5</span>, <span class="number">3</span>)</span><br><span class="line">print(y)</span><br><span class="line">result = np.empty((<span class="number">5</span>, <span class="number">3</span>))</span><br><span class="line">print(result)</span><br><span class="line">np.add(x, y, out=result)</span><br><span class="line">print(result)</span><br></pre></td></tr></table></figure><p>输出：<br><img src="/.cn//06/04/1559642400/36cb2728.png" title="36cb2728.png"></p><h5 id="加法：in-place（就地，在原来的位置改变）"><a href="#加法：in-place（就地，在原来的位置改变）" class="headerlink" title="加法：in-place（就地，在原来的位置改变）"></a>加法：in-place（就地，在原来的位置改变）</h5><blockquote><p><strong>注意</strong>：</p><ul><li>任何以“_”结尾的操作都会进行in-place操作，即在原来的tensor上操作并改变</li><li>通过打印id可以看出，in-place是在原来的tensor上进行操作并改变，并不是生成一个新的tensor然后赋值给原有的变量，也就是说tensor没有发生变化但tensor内的数字发生了变化</li></ul></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">x = torch.rand(<span class="number">5</span>, <span class="number">3</span>)</span><br><span class="line">print(x)</span><br><span class="line">y = torch.rand(<span class="number">5</span>, <span class="number">3</span>)</span><br><span class="line">print(y)</span><br><span class="line">print(id(y))</span><br><span class="line">y.add_(x)</span><br><span class="line">print(y)</span><br><span class="line">print(id(y))</span><br></pre></td></tr></table></figure><p>输出：<br><img src="/.cn//06/04/1559642400/beff2a6f.png" title="beff2a6f.png"></p><h5 id="您可以使用类似NumPy索引与切片的标准操作"><a href="#您可以使用类似NumPy索引与切片的标准操作" class="headerlink" title="您可以使用类似NumPy索引与切片的标准操作"></a>您可以使用类似NumPy索引与切片的标准操作</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">x = torch.rand(<span class="number">5</span>, <span class="number">3</span>)</span><br><span class="line">print(x)</span><br><span class="line">print(x[:, <span class="number">1</span>])</span><br></pre></td></tr></table></figure><p>输出：<br><img src="/.cn//06/04/1559642400/7d6ccc61.png" title="7d6ccc61.png"></p><p>NumPy中的索引与切片操作</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">x = np.random.rand(<span class="number">5</span>, <span class="number">3</span>)</span><br><span class="line">print(x)</span><br><span class="line">print(x[:, <span class="number">1</span>])</span><br></pre></td></tr></table></figure><p>输出：<br><img src="/.cn//06/04/1559642400/b0eaf859.png" title="b0eaf859.png"></p><h5 id="调整大小：如果您想要调整大小-改变形状，你可以使用-torch-view"><a href="#调整大小：如果您想要调整大小-改变形状，你可以使用-torch-view" class="headerlink" title="调整大小：如果您想要调整大小/改变形状，你可以使用 torch.view"></a>调整大小：如果您想要调整大小/改变形状，你可以使用 torch.view</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">x = torch.randn(<span class="number">4</span>, <span class="number">4</span>)</span><br><span class="line">print(x)</span><br><span class="line">y = x.view(<span class="number">16</span>)</span><br><span class="line">print(y)</span><br><span class="line">z = x.view(<span class="number">-1</span>, <span class="number">8</span>)</span><br><span class="line">print(z)</span><br><span class="line">print(x.size(), y.size(), z.size())</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">x = torch.randn(<span class="number">4</span>, <span class="number">4</span>)</span><br><span class="line">print(x)</span><br><span class="line">y = x.reshape(<span class="number">16</span>)</span><br><span class="line">print(y)</span><br><span class="line">z = x.reshape(<span class="number">-1</span>, <span class="number">8</span>)</span><br><span class="line">print(z)</span><br><span class="line">print(x.size(), y.size(), z.size())</span><br></pre></td></tr></table></figure><blockquote><p><strong>注意</strong>：</p><ul><li>Torch中tensor的view与reshape方法输出结果大部分情况下相同，view/reshape之后返回同样的数据与数据类型，只改变shape的值。</li><li>二者区别：当tensor都为contiguous类型（邻近模式）时，两个函数并无差异，使用原来的数据内容，不会复制一份新的出来；如果tensor不是，例如经过了transpose或permute之后，需要contiguous然后再使用view。reshape其实就是contiguous+view，这样不会开辟新的空间存放tensor，而是共享原来的数据内存。</li></ul></blockquote><p>输出：<br><img src="/.cn//06/04/1559642400/ff705f81.png" title="ff705f81.png"></p><h5 id="如果你的tensor只有一个元素，使用-item-得到一个Python-Number"><a href="#如果你的tensor只有一个元素，使用-item-得到一个Python-Number" class="headerlink" title="如果你的tensor只有一个元素，使用.item()得到一个Python Number"></a>如果你的tensor只有一个元素，使用.item()得到一个Python Number</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">x = torch.rand(<span class="number">1</span>)</span><br><span class="line">print(x)</span><br><span class="line">print(x.item())</span><br></pre></td></tr></table></figure><p>回答：<br><img src="/.cn//06/04/1559642400/21e8df91.png" title="21e8df91.png"></p><p>NumPy .item() 示例：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">x = np.random.rand(<span class="number">1</span>)</span><br><span class="line">print(x)</span><br><span class="line">print(x.item())</span><br></pre></td></tr></table></figure><p>回答：<br><img src="/.cn//06/04/1559642400/38e46887.png" title="38e46887.png"></p><h5 id="稍后阅读"><a href="#稍后阅读" class="headerlink" title="稍后阅读"></a><strong>稍后阅读</strong></h5><p><a href="https://pytorch.org/docs/torch" target="_blank" rel="noopener">这里</a>有100多个Tensor操作，包括置换, 索引, 切片, 数学运算, 线性代数, 随机数等等。</p><p><a name="index12"></a></p><h3 id="NumPy-桥"><a href="#NumPy-桥" class="headerlink" title="NumPy 桥"></a>NumPy 桥</h3><p>将Torch张量转换为NumPy数组是一件轻而易举的事情，反之亦然。<br>Torch张量和NumPy数组将共享它们的底层内存位置(如果Torch张量位于CPU上)，更改其中一个将会改变另一个。</p><h5 id="将Torch张量转换为NumPy数组"><a href="#将Torch张量转换为NumPy数组" class="headerlink" title="将Torch张量转换为NumPy数组"></a>将Torch张量转换为NumPy数组</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">a = torch.ones(<span class="number">5</span>)</span><br><span class="line">a</span><br></pre></td></tr></table></figure><p>输出：<br><img src="/.cn//06/04/1559642400/4037d43b.png" title="4037d43b.png"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">b = a.numpy()</span><br><span class="line">b</span><br></pre></td></tr></table></figure><p>输出：<br><img src="/.cn//06/04/1559642400/b7f7314c.png" title="b7f7314c.png"></p><h5 id="查看NumPy数组的值是如何变化的："><a href="#查看NumPy数组的值是如何变化的：" class="headerlink" title="查看NumPy数组的值是如何变化的："></a>查看NumPy数组的值是如何变化的：</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">a.add_(<span class="number">1</span>)</span><br><span class="line">a</span><br></pre></td></tr></table></figure><p>输出：<br><img src="/.cn//06/04/1559642400/7001b1a8.png" title="7001b1a8.png"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">b</span><br></pre></td></tr></table></figure><p>输出：<br><img src="/.cn//06/04/1559642400/4b42b954.png" title="4b42b954.png"></p><blockquote><p><strong>注意</strong>：</p><ul><li>只有Torch张量位于CPU上时，才会与NumPy数组共享内存地址，因为NumPy数组位于CPU上</li><li>如果Torch张量位于GPU上，需要先将Torch张量移到CPU上，才能获取NumPy数组</li></ul></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">a = torch.ones(<span class="number">5</span>, device=<span class="string">'cuda'</span>)</span><br><span class="line">a</span><br></pre></td></tr></table></figure><p>输出：<br><img src="/.cn//06/04/1559642400/3000f0cd.png" title="3000f0cd.png"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">b = a.numpy()</span><br></pre></td></tr></table></figure><p>输出：<br><img src="/.cn//06/04/1559642400/28a69493.png" title="28a69493.png"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">b = a.cpu().numpy()</span><br><span class="line">b</span><br></pre></td></tr></table></figure><p>输出：<br><img src="/.cn//06/04/1559642400/5884b5e9.png" title="5884b5e9.png"></p><h5 id="查看NumPy数组的值是如何变化的：-1"><a href="#查看NumPy数组的值是如何变化的：-1" class="headerlink" title="查看NumPy数组的值是如何变化的："></a>查看NumPy数组的值是如何变化的：</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">a.add_(<span class="number">1</span>)</span><br><span class="line">a</span><br></pre></td></tr></table></figure><p>输出：<br><img src="/.cn//06/04/1559642400/c717c186.png" title="c717c186.png"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">b</span><br></pre></td></tr></table></figure><p>输出：<br><img src="/.cn//06/04/1559642400/de0e4923.png" title="de0e4923.png"></p><h5 id="将NumPy数组转换为Torch张量"><a href="#将NumPy数组转换为Torch张量" class="headerlink" title="将NumPy数组转换为Torch张量"></a>将NumPy数组转换为Torch张量</h5><h5 id="查看如何改变NumPy数组自动改变Torch张量："><a href="#查看如何改变NumPy数组自动改变Torch张量：" class="headerlink" title="查看如何改变NumPy数组自动改变Torch张量："></a>查看如何改变NumPy数组自动改变Torch张量：</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">a = np.ones(<span class="number">5</span>)</span><br><span class="line">b = torch.from_numpy(a)</span><br><span class="line">np.add(a, <span class="number">1</span>, out=a)</span><br><span class="line">print(a)</span><br><span class="line">print(b)</span><br></pre></td></tr></table></figure><p>输出：<br><img src="/.cn//06/04/1559642400/a163b2f7.png" title="a163b2f7.png"></p><h5 id="查看改变NumPy数组，是否会改变位于GPU上的Torch张量："><a href="#查看改变NumPy数组，是否会改变位于GPU上的Torch张量：" class="headerlink" title="查看改变NumPy数组，是否会改变位于GPU上的Torch张量："></a>查看改变NumPy数组，是否会改变位于GPU上的Torch张量：</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">a = np.ones(<span class="number">5</span>)</span><br><span class="line">b = torch.from_numpy(a).cuda()</span><br><span class="line">np.add(a, <span class="number">1</span>, out=a)</span><br><span class="line">print(a)</span><br><span class="line">print(b)</span><br></pre></td></tr></table></figure><p>输出：<br><img src="/.cn//06/04/1559642400/bf5c7507.png" title="bf5c7507.png"></p><p><a name="index13"></a></p><h3 id="CUDA-张量"><a href="#CUDA-张量" class="headerlink" title="CUDA 张量"></a>CUDA 张量</h3><p>张量可以使用.to方法移动到任何设备上。</p><blockquote><p><strong>注意</strong>：</p><ul><li>这里的设备指的是CPU或者GPU，每台设备必有一块CPU，可能没有GPU，也可能有1块甚至多块GPU</li><li>设备必须有一或多块GPU才能将tensor移到GPU上，然后使用GPU为tensor计算加速，使用torch.cuda.is_available() 判断当前设备是否有GPU</li><li>因每台设备必有一块CPU，所以 torch.cuda.is_available() 为 False 时，可以直接在CPU上进行计算，也可以什么都不做。</li></ul></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># let us run this cell only if CUDA is available</span></span><br><span class="line"><span class="comment"># We will use ``torch.device`` objects to move tensors in and out of GPU</span></span><br><span class="line">x = torch.rand(<span class="number">2</span>, <span class="number">3</span>)</span><br><span class="line"><span class="keyword">if</span> torch.cuda.is_available():</span><br><span class="line">    device = torch.device(<span class="string">"cuda"</span>)          <span class="comment"># a CUDA device object</span></span><br><span class="line">    y = torch.ones_like(x, device=device)  <span class="comment"># directly create a tensor on GPU</span></span><br><span class="line">    x = x.to(device)                       <span class="comment"># or just use strings ``.to("cuda")``</span></span><br><span class="line">    z = x + y</span><br><span class="line">    print(z)</span><br><span class="line">    print(z.to(<span class="string">"cpu"</span>, torch.double))       <span class="comment"># ``.to`` can also change dtype together!</span></span><br></pre></td></tr></table></figure><p>输出：<br><img src="/.cn//06/04/1559642400/7f78e18f.png" title="7f78e18f.png"></p><p><a name="index2"></a></p><h2 id="Autograd：自动求导"><a href="#Autograd：自动求导" class="headerlink" title="Autograd：自动求导"></a>Autograd：自动求导</h2><p><a name="index3"></a></p><h2 id="神经网络"><a href="#神经网络" class="headerlink" title="神经网络"></a>神经网络</h2><p><a name="index4"></a></p><h2 id="训练一个分类器"><a href="#训练一个分类器" class="headerlink" title="训练一个分类器"></a>训练一个分类器</h2><p><a name="index5"></a></p><h2 id="选读：数据并行"><a href="#选读：数据并行" class="headerlink" title="选读：数据并行"></a>选读：数据并行</h2>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本教程是PyTorch官方教程Getting Started篇的第一篇教程，更多教程请点击下方链接查看：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;PyTorch深度学习：60分钟速成&lt;/li&gt;
&lt;li&gt;&lt;a href&gt;数据加载与处理&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href&gt;迁移学习&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href&gt;使用混合前端部署Seq2Seq模型&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href&gt;保存与加载模型&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href&gt;torch.nn是什么&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>NLP学习之路——资料搜集篇</title>
    <link href="https://artificial-intelligence.net.cn/2019/05/08/1557328260/"/>
    <id>https://artificial-intelligence.net.cn/2019/05/08/1557328260/</id>
    <published>2019-05-08T15:11:00.000Z</published>
    <updated>2019-06-05T06:03:41.174Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>NLP学习之路系列是笔者学习自然语言处理时自己摸索总结的一条学习路线，完全按照笔者个人的性格特点及学习习惯定制，并不适合每一位读者，对于读者仅仅作为参考即可。本系列文章同时也会作为笔者记录NLP相关资料的笔记，将不断保持更新。</p></blockquote><p>​    本篇文章是NLP学习之路系列文章的首篇。本篇首先会搜集一些NLP领域大牛的入门心得，通过学习揣摩大牛的入门心得对NLP的学习有一个大致的认识。<a id="more"></a>接着会列出一批NLP领域的经典书籍，读者可通过亚马逊、京东、当当等渠道购买阅读。然后会推荐一些国内外名校的公开课，<strong>强烈推荐以名校公开课为主进行学习</strong>。同时也会推荐一些付费课程作为补充资料，郑重声明笔者并没有收取广告费（如果有人愿意补上广告费也是可以的），也没有看过全部的课程，推荐的理由一般是老师讲课水平不错/听说这门课讲得不错，读者完全可以忽略这些内容。由于笔者有在学习的过程中看大量同类优秀资料的习惯，所以把这些课程列入到学习之路的文章里。最后会筛选一些Github上的优秀开源项目，这些优秀开源项目会成为我们学习NLP道路上的强大的工具。</p><p>​    本篇推荐的所有资料都具有时效性，随着时间的流逝过于老旧并没有参考价值的资料将会被剔除。</p><h2 id="如何入门"><a href="#如何入门" class="headerlink" title="如何入门"></a>如何入门</h2><ul><li><a href="https://www.zhihu.com/question/19895141" target="_blank" rel="noopener">知乎问答 “自然语言处理怎么最快入门？”</a></li><li><a href="https://www.zhihu.com/question/264352009" target="_blank" rel="noopener">知乎问答 “适合初学者学习的NLP开源项目有哪些？”</a></li><li><a href="https://www.zhihu.com/question/26391679" target="_blank" rel="noopener">知乎问答 “数据挖掘、机器学习、自然语言处理这三者是什么关系？这几个怎么入门啊？”</a></li><li><a href="https://www.zhihu.com/question/35381685" target="_blank" rel="noopener">知乎问答 “研究 NLP（自然语言处理）的大神有哪些值得推荐的语言学方面的书籍？”</a></li><li><a href="https://www.zhihu.com/question/19895141" target="_blank" rel="noopener">“NLP入门推荐书目（2019版）” by 清华大学老师 刘知远</a></li><li><a href="https://zhuanlan.zhihu.com/p/59184256" target="_blank" rel="noopener">“初入NLP领域的一些小建议” by 香侬科技CEO 李纪为</a></li><li><a href="https://zhuanlan.zhihu.com/p/64026730" target="_blank" rel="noopener">“我的NLP学习之路” by 北京科技大学研究生 机智的叉烧</a></li><li><a href="https://zhuanlan.zhihu.com/p/32951278" target="_blank" rel="noopener">“NLP自然语言处理从入门到迷茫” by 面包君</a></li><li><a href="https://zhuanlan.zhihu.com/p/56276227" target="_blank" rel="noopener">“自然语言处理初学者建议及干货推荐” by AI小白入门</a></li><li><a href="https://zhuanlan.zhihu.com/p/29791380" target="_blank" rel="noopener">“自然语言处理(NLP)入门学习资源清单” by 清华大学数据科学研究院</a></li><li><a href="https://zhuanlan.zhihu.com/p/54731715" target="_blank" rel="noopener">“《从零开始学习自然语言处理(NLP)》-基础准备(0)” by AI算法工程师(NLP) EddyLiu</a></li><li><a href="https://zhuanlan.zhihu.com/p/34688488" target="_blank" rel="noopener">“自然语言处理入门：手把手教你解决90%的NLP问题” by 阿里巴巴资深算法工程师 章华燕</a></li></ul><h2 id="书籍推荐"><a href="#书籍推荐" class="headerlink" title="书籍推荐"></a>书籍推荐</h2><table><thead><tr><th><b>自然语言处理综论(第二版)</b><br>译者：冯志伟作者：Daniel Jurafsky , James H. Martin<br>出版社: 电子工业出版社; 第2版 (2018年3月1日)；784页<br>外文书名: Speech and Language Processing<br></th><th><a href="https://www.amazon.cn/dp/B07BLHMY2V?qid=1557288088" target="_blank" rel="noopener"><img src="https://m.media-amazon.com/images/I/71OYPqZJmPL._AC_UL320_.jpg" width="30%"></a></th></tr></thead><tbody><tr><td><b>信息检索导论<br>译者：王斌<br>作者：Christopher D.Manning , Prabhakar Raghavan , Hinrich Schütze <br>出版社: 人民邮电出版社; 第1版 (2010年8月26日)；388页<br>外文书名: Introduction to Information Retrieval</b></td><td><a href="http://www.ituring.com.cn/book/127" target="_blank" rel="noopener"><img src="http://file.ituring.com.cn/ScreenShow/0100a7322d50dc7e4983" width="30%"></a></td></tr><tr><td><b>统计自然语言处理基础<br>译者：苑春法 , 李庆中 , 王昀 , 李伟 , 曹德芳<br>作者：Christopher D.Manning , Hinrich Schütze <br>出版社: 电子工业出版社; 第1版 (2005年1月)；418页<br>外文书名: Foundations of Statistical Natural Language Processing</b></td><td><a href="https://item.jd.com/10094301.html?dist=jd" target="_blank" rel="noopener"><img src="https://img11.360buyimg.com/n1/s200x200_18057/9bda917f-ebee-4a2e-8e50-58891cf7058d.jpg" width="45%"></a></td></tr><tr><td><b>统计自然语言处理(第2版)<br>作者：宗成庆 <br>出版社: 清华大学出版社; 第2版 (2013年8月1日)；569页</b></td><td><a href="https://www.amazon.cn/dp/B00EYSQLFM?qid=1557288088" target="_blank" rel="noopener"><img src="https://m.media-amazon.com/images/I/814GS-Gm7cL._AC_UL320_.jpg" width="30%"></a></td></tr><tr><td><b>基于深度学习的自然语言处理<br>译者：车万翔 , 郭江 , 张伟男 , 刘铭<br>作者：Yoav Goldberg <br>出版社: 机械工业出版社; 第1版 (2018年5月1日)；251页<br>外文书名: Neural Network Methods for Natural Language Processing</b></td><td><a href="https://www.amazon.cn/dp/B07D51TYG1?qid=1557288088" target="_blank" rel="noopener"><img src="https://m.media-amazon.com/images/I/61Z5hGL8DjL._AC_UL320_.jpg" width="30%"></a></td></tr><tr><td><b>Python自然语言处理<br>译者：张金超 , 刘舒曼<br>作者：Jalaj Thanaki <br>出版社: 机械工业出版社; 第1版 (2018年9月4日)；281页<br>外文书名: Python Natural Language Processing</b></td><td><a href="https://www.amazon.cn/dp/B07H5VXG6J?qid=1557288088" target="_blank" rel="noopener"><img src="https://m.media-amazon.com/images/I/61lZTjl+LDL._AC_UL320_.jpg" width="30%"></a></td></tr></tbody></table><h2 id="视频教程"><a href="#视频教程" class="headerlink" title="视频教程"></a>视频教程</h2><ul><li><strong><a href="https://ssvideo.superlib.com/cxvideo/play/page?sid=1586&amp;d=77edee6d216507e5ece667cef95799ea&amp;cid=236" target="_blank" rel="noopener">宗成庆：自然语言理解（ 2010 ）</a></strong> <em><a href="https://www.bilibili.com/video/av51794616/" target="_blank" rel="noopener">bilibili</a></em></li><li><strong><a href="http://www.chinahadoop.cn/course/1344" target="_blank" rel="noopener">秦曾昌：自然语言处理（ 2018 ）</a></strong> <em><a href="https://www.bilibili.com/video/av44918854" target="_blank" rel="noopener">bilibili</a></em></li><li><strong><a href="https://edu.hellobi.com/course/275" target="_blank" rel="noopener">aopu：自然语言处理之AI深度学习顶级实战课程（2018）</a></strong> <em><a href="https://www.bilibili.com/video/av42021445" target="_blank" rel="noopener">bilibili</a></em></li><li><strong><a href="https://www.greedyai.com/course/68/summary/introNlp" target="_blank" rel="noopener">李文哲：NLP自然语言处理集训营（2019）</a></strong></li><li><strong><a href="https://mooc.study.163.com/smartSpec/detail/1001477005.htm" target="_blank" rel="noopener">网易微专业：AI工程师（自然语言处理）（ 2019 ）</a></strong> </li><li><strong><a href="http://www.nlpr.ia.ac.cn/cip/introduction.htm" target="_blank" rel="noopener">中科院：自然语言处理（2018）</a></strong> <em><a href="https://www.bilibili.com/video/av46707588" target="_blank" rel="noopener">bilibili</a></em></li><li><strong><a href="http://www.nlpr.ia.ac.cn/cip/introduction.htm" target="_blank" rel="noopener">中科院：自然语言处理（2019）</a></strong> <em><a href="https://space.bilibili.com/303667813" target="_blank" rel="noopener">bilibili</a></em></li><li><strong><a href="https://www.cs.ox.ac.uk/teaching/courses/2016-2017/dl/" target="_blank" rel="noopener">Oxford &amp; DeepMind : Deep Learning for Natural Language Processing ( 2017 )</a></strong> <em><a href="https://www.youtube.com/playlist?list=PL613dYIGMXoZBtZhbyiBqb0QtgK6oJbpm" target="_blank" rel="noopener">YouTube</a></em> <em><a href="https://study.163.com/course/introduction.htm?courseId=1004336028" target="_blank" rel="noopener">网易云课堂</a></em> <em><a href="https://github.com/oxford-cs-deepnlp-2017" target="_blank" rel="noopener">Github</a></em></li><li><strong><a href="https://www.coursera.org/learn/nlp" target="_blank" rel="noopener">Natural Language Processing , Stanford University via Coursera</a></strong> <em><a href="https://www.classcentral.com/course/coursera-natural-language-processing-836" target="_blank" rel="noopener">Overview</a></em> <em><a href="https://www.youtube.com/playlist?list=PLQiyVNMpDLKnZYBTUOlSI9mi9wAErFtFm" target="_blank" rel="noopener">YouTube</a></em> <em><a href="https://web.stanford.edu/~jurafsky/NLPCourseraSlides.html" target="_blank" rel="noopener">Lecture</a></em></li><li><strong><a href="https://web.stanford.edu/class/archive/cs/cs224n/cs224n.1174/" target="_blank" rel="noopener">Stanford CS224n : Natural Language Processing with Deep Learning ( Winter 2017 )</a></strong> <em><a href="https://www.youtube.com/playlist?list=PL3FW7Lu3i5Jsnh1rnUwq_TcylNr7EkRe6" target="_blank" rel="noopener">YouTube</a></em> <em><a href="https://www.bilibili.com/video/av41393758" target="_blank" rel="noopener">bilibili</a></em> <em><a href="http://www.mooc.ai/course/494" target="_blank" rel="noopener">AI研习社</a></em></li><li><strong><a href="http://web.stanford.edu/class/cs224n/" target="_blank" rel="noopener">Stanford CS224n : Natural Language Processing with Deep Learning ( Winter 2019 )</a></strong>  <em><a href="https://www.youtube.com/playlist?list=PLoROMvodv4rOhcuXMZkNm7j3fVwBBY42z" target="_blank" rel="noopener">YouTube</a></em> </li><li><strong><a href="http://www.phontron.com/class/nn4nlp2019/" target="_blank" rel="noopener">CMU CS 11-747 Neural Networks for NLP ( Spring 2019 )</a></strong> <em><a href="https://www.youtube.com/playlist?list=PL8PYTP1V4I8Ajj7sY6sdtmjgkt7eo2VMs" target="_blank" rel="noopener">YouTube</a></em> <em><a href="https://www.bilibili.com/video/av45971299" target="_blank" rel="noopener">bilibili</a></em> <em><a href="http://www.mooc.ai/course/638" target="_blank" rel="noopener">AI研习社</a></em></li></ul><h2 id="优秀开源项目"><a href="#优秀开源项目" class="headerlink" title="优秀开源项目"></a>优秀开源项目</h2><ul><li><a href="https://github.com/sebastianruder/NLP-progress" target="_blank" rel="noopener"><strong>sebastianruder / NLP-progress</strong> : Repository to track the progress in Natural Language Processing (NLP), including the datasets and the current state-of-the-art for the most common NLP tasks. </a></li><li><a href="https://github.com/crownpku/awesome-chinese-nlp" target="_blank" rel="noopener"><strong>crownpku / Awesome-Chinese-NLP</strong> : A curated list of resources for Chinese NLP / 中文自然语言处理相关资料</a></li><li><a href="https://github.com/yandexdataschool/nlp_course" target="_blank" rel="noopener"><strong>yandexdataschool / nlp_course</strong> : YSDA course in Natural Language Processing</a></li><li><a href="https://github.com/jacobeisenstein/gt-nlp-class" target="_blank" rel="noopener"><strong>jacobeisenstein / gt-nlp-class</strong> : Course materials for Georgia Tech CS 4650 and 7650, Natural Language Processing</a></li><li><a href="https://github.com/brightmart/nlp_chinese_corpus" target="_blank" rel="noopener"><strong>brightmart / nlp_chinese_corpus</strong> : Large Scale Chinese Corpus for NLP / 大规模中文自然语言处理语料</a></li><li><a href="https://github.com/graykode/nlp-tutorial" target="_blank" rel="noopener"><strong>graykode / nlp-tutorial</strong> : Natural Language Processing Tutorial for Deep Learning Researchers</a></li><li><a href="https://github.com/JustFollowUs/Natural-Language-Processing" target="_blank" rel="noopener"><strong>JustFollowUs / Natural-Language-Processing</strong> : 一份自然语言处理学习资料合集</a></li><li><a href="https://github.com/mhagiwara/100-nlp-papers" target="_blank" rel="noopener"><strong>mhagiwara / 100-nlp-papers</strong> : This is a list of 100 important natural language processing (NLP) papers that serious students and researchers working in the field should probably know about and read. </a></li><li><a href="https://github.com/duoergun0729/nlp" target="_blank" rel="noopener"><strong>duoergun0729 / nlp</strong> : 兜哥出品 &lt;一本开源的NLP入门书籍&gt;</a></li><li><a href="https://github.com/zibuyu/research_tao" target="_blank" rel="noopener"><strong>zibuyu / research_tao</strong> : 清华大学刘知远教授开源书籍《NLP研究入门之道》，系统地介绍了如何入门NLP科研</a></li><li><a href="https://github.com/SophonPlus/ChineseNlpCorpus" target="_blank" rel="noopener"><strong>SophonPlus / ChineseNlpCorpus</strong> : 搜集、整理、发布中文自然语言处理语料/数据集</a></li><li><a href="https://github.com/neubig/nlptutorial" target="_blank" rel="noopener"><strong>neubig / nlptutorial</strong> : A Tutorial about Programming for Natural Language Processing</a></li><li><a href="https://github.com/lpty/nlp_base" target="_blank" rel="noopener"><strong>lpty / nlp_base</strong> : 一些关于自然语言的基本模型，这个项目里面是作者调研时写的一些简单demo。</a></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;NLP学习之路系列是笔者学习自然语言处理时自己摸索总结的一条学习路线，完全按照笔者个人的性格特点及学习习惯定制，并不适合每一位读者，对于读者仅仅作为参考即可。本系列文章同时也会作为笔者记录NLP相关资料的笔记，将不断保持更新。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;​    本篇文章是NLP学习之路系列文章的首篇。本篇首先会搜集一些NLP领域大牛的入门心得，通过学习揣摩大牛的入门心得对NLP的学习有一个大致的认识。&lt;/p&gt;
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>达观杯文本智能处理挑战赛——简介篇</title>
    <link href="https://artificial-intelligence.net.cn/2019/05/07/1557242890/"/>
    <id>https://artificial-intelligence.net.cn/2019/05/07/1557242890/</id>
    <published>2019-05-07T15:28:10.000Z</published>
    <updated>2019-06-05T06:03:41.175Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>本文是针对深度之眼 “达观杯” 文本智能处理挑战赛讲解视频所做的笔记，示例代码讲解等内容请关注后续文章。</p></blockquote><p>​    同学们大家好，今天我们讲的正是文本智能处理挑战赛。我们将会对挑战赛的任务背景、算法的流程、比赛所用的数据等进行简单的介绍。针对这个比赛，我们制作了一个参赛手册。大家按照手册一步一步操作，即可在大约两个小时内完成比赛的报名，数据集的下载，开发环境的安装，代码的编写和执行，提交比赛结果并查看自己的排名。<br><a id="more"></a></p><h2 id="挑战赛任务背景"><a href="#挑战赛任务背景" class="headerlink" title="挑战赛任务背景"></a>挑战赛任务背景</h2><p>​    这个比赛是由达观数据公司举办的。这个比赛的本质就是文本分类，文本分类是自然语言处理（NLP）领域里最最基本的任务，在互联网领域里的应用极为广泛，相对也比较简单。那么，达观公司为什么要耗费大量的人力物力举办这个比赛呢？那是因为其所涉及的问题是业内比较难的问题。为什么这么说呢？通常，我们做的分类任务都是短文本分类任务，一篇文章只有几百个词，而词比赛的文本，每篇大概有3000个词，所以说是属于长文本分类，自然而然带来很多问题，对现有的技术也是一个重大的挑战。</p><p>​    在这里给大家普及一个大家日常生活中很少关注的概念：就是字和词的概念。我们知道，字组成词，词才具有意义。举一个例子，”垃圾”这个词具有完整的意义，而单独的字”垃”或”圾”便没有单独的意思了。我们日常生活中所看到的文章都是由字组成的，那么我们怎样将其变成由词组成的呢？这里涉及的概念就是中文分词。一句由字组成的句子经过中文分词工具，可以变成由词组成的句子。</p><h2 id="算法的流程"><a href="#算法的流程" class="headerlink" title="算法的流程"></a>算法的流程</h2><p>​    下面我们以图为例介绍一下用传统的监督学习模型对一段文本进行分类的基本过程（在介绍之前先假设我们已经有了一个训练好的文本分类机器学习模型f）：</p><ul><li>数据预处理：将一段原始文本经过数据预处理后得到处理后的文本。例如文本中的表情符号不是汉字内容，会影响后续文字处理，所以在数据预处理的阶段我们将其删除。</li><li>特征工程：对经过数据预处理的文本做特征工程，把文本数字化成一个向量。</li><li>训练模型：将经过特征工程后得到的向量送到机器模型f得到类别。函数输出的类别是我们事先人为约定好，比如我让数字4代表政治类，数字3代表科技类等等。</li></ul><p><img src="http://pqdosd4w6.bkt.clouddn.com/IMG_3717.jpg" alt="img"></p><p>​    最后我们要强调一下，特征工程的步骤是整个机器学习工程中最重要的步骤。坊间有句话说：数据和特征决定了机器学习的上限，而模型和算法只是逼近这个上限而已。由此可见特征工程的重要性，这是大家提高比赛成绩最为关键的一个步骤。</p><h2 id="比赛数据之训练数据集"><a href="#比赛数据之训练数据集" class="headerlink" title="比赛数据之训练数据集"></a>比赛数据之训练数据集</h2><p>​    我们再来看一下比赛官方提供的数据集。一眼看上去，竟然没有一个汉字，所以大家就疑惑了：不是文本分类吗？那么汉字呢，文章呢？</p><p>​    其实表格中的一个数字就代表一个文字或者一个词。因为达观公司提供的数据集很有可能是各大公司受法律合同保护的文件，不易公开，所以就将文件中的每个汉字或词用一个数字来代替。我们通常将这样的数据成为”脱敏数据“。在涉及自然语音处理的比赛中，大家会经常看到脱敏之后的数据。经过脱敏后的数据，表格中的数字不是数字，没有大小之间的比较，数字只是代表一个字或一个词。</p><p>​    我们来具体看一下表格中的数据。表格中总共有十万多个样本，每个样本包含哪些信息呢？</p><p><img src="http://pqdosd4w6.bkt.clouddn.com/%E2%80%9C%E8%BE%BE%E8%A7%82%E6%9D%AF%E2%80%9D%E6%96%87%E6%9C%AC%E6%99%BA%E8%83%BD%E5%A4%84%E7%90%86%E6%8C%91%E6%88%98%E8%B5%9B2" alt="img"></p><p>​    第一个信息：id，这个不用解释了。第二个信息：article，就是一篇由字组成的文章。第三个信息：word_seg，就是一篇由词组成的文章。第四个信息：class，就是类别信息，即这篇文章属于哪一类，也就是机器学习工程中常常说的label。每个样本就包含这四类信息。</p><p>​    在图片中我们用方框标注出来的单元格就是一篇文章。那么这篇文章是由哪些词组成的呢？在表格的右上方，我们可以看到许多数字，正是由这些数字组成了这篇文章，每个数字都代表一个词。</p><h2 id="比赛数据之测试数据集"><a href="#比赛数据之测试数据集" class="headerlink" title="比赛数据之测试数据集"></a>比赛数据之测试数据集</h2><p>​    这个任务的本质就是用官方提供的数据，我们用机器学习的方法去写出一个机器学习的模型来，让这个模型具有文本分类的能力，分类越精准则模型越优秀。那么达观数据公司怎么去评价每个人模型的好坏呢？这里就要用到测试数据集了。</p><p>​    比赛官方除了提供前面介绍的训练用数据集外，又给我们提供了另外一个数据集。这个数据集与前面介绍的训练数据集不同的地方在于：这个数据集的每一个样本都没有类别信息，但是达观数据公司会有这个数据集中每个样本的类别信息，就相当于”标准答案“。我们会用我们学习好的模型，对这些样本的类别进行预测，然后提交预测结果给官方，和官方的“标准答案”进行比较，即可算出模型的准确率。我们通常把这个数据集称为测试集，把之前用到的数据集称为训练集。训练集用来提供给我们的模型训练，而测试集用来检验我们的模型的好坏。</p><h2 id="机器学习算法"><a href="#机器学习算法" class="headerlink" title="机器学习算法"></a>机器学习算法</h2><p>​    针对这个比赛，我们用来学习的机器学习算法主要有传统的监督学习算法和深度学习。传统的监督学习算法供我们选择的有很多，比如，我们可以选择对数几率回归、支持向量机、朴素贝叶斯、决策树、集成学习等。究竟哪个算法适合我们的问题呢？具体问题还得具体分析。这些算法在西瓜书上都有介绍。这里只给出一个简单的机器学习模型的代码示例，程序最终会生成一个result.csv文件，便于我们将结果提交到比赛网页，提交之后我们就可以看到自己的分数和排名。</p><p><img src="http://pqdosd4w6.bkt.clouddn.com/%E2%80%9C%E8%BE%BE%E8%A7%82%E6%9D%AF%E2%80%9D%E6%96%87%E6%9C%AC%E6%99%BA%E8%83%BD%E5%A4%84%E7%90%86%E6%8C%91%E6%88%98%E8%B5%9B3" alt="img"></p><h2 id="如何提高模型性能"><a href="#如何提高模型性能" class="headerlink" title="如何提高模型性能"></a>如何提高模型性能</h2><p>​    最后一个问题，我们怎样在比赛中取得好的成绩呢？也就是如何提高模型的性能呢？对于一般的机器学习问题来讲，主要可以从以下几个方面着手：</p><ul><li>数据预处理：因为有很多比赛中由实际的工程问题所提供的数据有可能有缺陷，缺胳膊少腿，其中也可能会有错误的数据，所以需要我们对数据进行预处理，这是属于数据的本源问题。但是这次比赛我们很幸运，不用做任何的数据预处理操作，所以在这个比赛中，我们不需要关注这个方面。</li><li>特征工程：毋庸置疑，无论在任何机器学习问题中，特征工程都比较关键，这个比赛也同样如此，做好特征工程是提分的关键。特征做得好，分数将会有质的提升。特征工程需要的技巧性非常强，需要我们不断的积累经验来提升技巧。这里通过一个简单的例子来说明特征工程的重要性：当我们识别一个人时，我们可以提取这个人的面部特征，也可以提取这个人的身材特征。这里提取哪个特征比较好呢？当然是脸部的特征比较好了。我们通过看人的脸来识别这个人大概率不会认错；如果仅仅通过看身材，我们大概率会认错这个人。</li><li>机器学习算法：不同的机器学习算法适合不同的问题，复杂的算法不一定适合当前的问题。对于这个问题，最简单的逻辑回归模型就能起到很好的效果。</li><li>模型集成：这个毫无意问是提分的关键，是大家都会做的一件工作。简单来讲，就是将几个模型融合在一起。</li><li>数据增强：官方提供给我们十万条数据，我们要想办法增多数据，比如把它变成二十万条。通俗来讲，机器学习模型是数据喂起来的，数据越多，可供学习的样本就越多，学习到的模型的性能越好，这个部分也是提分的关键。</li></ul><p>​    对于一般的机器学习的工程问题，要求每一步都要做好，最终模型性能才会好。而在我们这次比赛中，我们应该着重关注特征工程、模型集成、数据增强这几个方面来提高我们的比赛成绩。</p>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;本文是针对深度之眼 “达观杯” 文本智能处理挑战赛讲解视频所做的笔记，示例代码讲解等内容请关注后续文章。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;​    同学们大家好，今天我们讲的正是文本智能处理挑战赛。我们将会对挑战赛的任务背景、算法的流程、比赛所用的数据等进行简单的介绍。针对这个比赛，我们制作了一个参赛手册。大家按照手册一步一步操作，即可在大约两个小时内完成比赛的报名，数据集的下载，开发环境的安装，代码的编写和执行，提交比赛结果并查看自己的排名。&lt;br&gt;&lt;/p&gt;
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>2019年04月29日—2019年05月05日周报</title>
    <link href="https://artificial-intelligence.net.cn/2019/05/04/1556950868/"/>
    <id>https://artificial-intelligence.net.cn/2019/05/04/1556950868/</id>
    <published>2019-05-04T06:21:08.000Z</published>
    <updated>2019-06-05T06:03:41.171Z</updated>
    
    <content type="html"><![CDATA[<h2 id="知乎优秀问答推荐"><a href="#知乎优秀问答推荐" class="headerlink" title="知乎优秀问答推荐"></a>知乎优秀问答推荐</h2><ul><li><a href="https://www.zhihu.com/question/322191738" target="_blank" rel="noopener">为什么深度学习中的图像分割要先编码再解码？</a></li><li><a href="https://www.zhihu.com/question/321677995" target="_blank" rel="noopener">如何看待2019年发布的中国计算机学会(CCF)推荐国际学术会议和期刊目录？</a><a id="more"></a></li><li><a href="https://www.zhihu.com/question/318267715" target="_blank" rel="noopener">2019年，mxnet会如何进行推广来吸引用户？</a></li><li><a href="https://www.zhihu.com/question/321773456" target="_blank" rel="noopener">为什么深度学习中的模型基本用3x3和5x5的卷积（奇数），而不是2x2和4x4的卷积（偶数）？</a></li><li><a href="https://www.zhihu.com/question/322468467" target="_blank" rel="noopener">在人工智能大背景下，FPGA和ASIC谁发展潜力更大？</a></li><li><a href="https://www.zhihu.com/question/322569258" target="_blank" rel="noopener">Tishby的文章《利用信息论打开机器学习的黑箱》在业内风评如何？</a></li><li><a href="https://www.zhihu.com/question/24964987" target="_blank" rel="noopener">互联网公司最常见的面试算法题有哪些？</a></li><li><a href="https://www.zhihu.com/question/319315868" target="_blank" rel="noopener">学习优化算法需要哪些数学基础？</a></li><li><a href="https://www.zhihu.com/question/264417928" target="_blank" rel="noopener">人工智能（AI）是如何处理数据的？</a></li><li><a href="https://www.zhihu.com/question/264528062" target="_blank" rel="noopener">机器学习如何才能避免「只是调参数」？</a></li><li><a href="https://www.zhihu.com/question/25097993" target="_blank" rel="noopener">深度学习调参有哪些技巧？</a></li><li><a href="https://www.zhihu.com/question/47158818" target="_blank" rel="noopener">如何理解卷积神经网络中的权值共享？</a></li></ul><h2 id="名校公开课推荐（Berkeley专场）"><a href="#名校公开课推荐（Berkeley专场）" class="headerlink" title="名校公开课推荐（Berkeley专场）"></a>名校公开课推荐（Berkeley专场）</h2><h2 id="优秀开源书籍推荐"><a href="#优秀开源书籍推荐" class="headerlink" title="优秀开源书籍推荐"></a>优秀开源书籍推荐</h2><h2 id="技术文章推荐（15篇）"><a href="#技术文章推荐（15篇）" class="headerlink" title="技术文章推荐（15篇）"></a>技术文章推荐（15篇）</h2><ul><li>整理一份详细的数据预处理方法 <em><a href="https://zhuanlan.zhihu.com/p/51131210" target="_blank" rel="noopener">原文</a></em> </li><li>[ML] 提升方法：AdaBoost, 提升树, GBDT, XGBoost, LightGBM <em><a href="https://zhuanlan.zhihu.com/p/60909244" target="_blank" rel="noopener">原文</a></em></li><li>中文文本分类：你需要了解的10项关键内容 <em><a href="https://mp.weixin.qq.com/s/Li50A5ymlBX9iB3YFtZTnA" target="_blank" rel="noopener">原文</a></em> </li><li>文本挖掘，带你看金庸笔下不一样的恩怨情仇 _<a href="https://mp.weixin.qq.com/s/vAn_rnUt036-3jy7ZahF8w" target="_blank" rel="noopener">原文</a>_</li><li>Google BERT 中文应用之《红楼梦》对话人物提取 <em><a href="https://mp.weixin.qq.com/s/vPBE8jxalbbvi7wFAtaFPA" target="_blank" rel="noopener">原文</a></em></li><li>用文本挖掘分析了5万首《全唐诗》，竟然发现这些秘密 <em><a href="https://mp.weixin.qq.com/s/0bOg1kvSR66-8rvFDqtLIQ" target="_blank" rel="noopener">原文</a></em></li><li>如何用深度学习做好长文本分类与法律文书智能化处理 <em><a href="https://mp.weixin.qq.com/s/tts6UzF-ziyNp0s31Qiqtg" target="_blank" rel="noopener">原文</a></em></li><li>一文详解深度学习在命名实体识别(NER)中的应用 <em><a href="https://mp.weixin.qq.com/s/c1rSq8vY9fiXjV-WsxLVQw" target="_blank" rel="noopener">原文</a></em>  </li><li>深度学习技术如何应用于文本智能处理？ <em><a href="https://mp.weixin.qq.com/s/12RA4Bd86SJS6Q4CAvOEWw" target="_blank" rel="noopener">原文</a></em> </li><li>如何利用NLP技术从海量文本中提取观点？ <em><a href="https://mp.weixin.qq.com/s/jTzkD8PoHnkvdwdbPxNblQ" target="_blank" rel="noopener">原文</a></em></li><li>计算广告系统算法与架构综述 <em><a href="https://mp.weixin.qq.com/s/iEMGS1tbNPYXYIM7pKkq4A" target="_blank" rel="noopener">原文</a></em></li><li>使用Python实现机器学习特征选择的4种方法 <em><a href="https://hub.packtpub.com/4-ways-implement-feature-selection-python-machine-learning/" target="_blank" rel="noopener">原文</a></em> _<a href="https://mp.weixin.qq.com/s/heZmIjGx6thH_ELQ7LY9Vg" target="_blank" rel="noopener">翻译</a>_</li><li>一文了解自然语言生成演变史！ <em><a href="https://medium.com/sfu-big-data/evolution-of-natural-language-generation-c5d7295d6517" target="_blank" rel="noopener">原文</a></em> <em><a href="https://mp.weixin.qq.com/s/S2SH8MUmreciZRc0Ff-qlw" target="_blank" rel="noopener">翻译</a></em></li><li>MIT计算机大牛：如何写好一篇顶会论文 <em><a href="https://billf.mit.edu/talks" target="_blank" rel="noopener">原文</a></em> <em><a href="https://mp.weixin.qq.com/s/MdgXLi674Y46z-dQm0VlIA" target="_blank" rel="noopener">翻译</a></em></li><li>写给机器学习从业者的12条宝贵建议 <em><a href="https://towardsml.com/2019/04/09/12-key-lessons-from-ml-researchers-and-practitioners/" target="_blank" rel="noopener">原文</a></em> <em><a href="https://mp.weixin.qq.com/s/AANo0iliShDavXgZZIsQ7g" target="_blank" rel="noopener">翻译</a></em></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;知乎优秀问答推荐&quot;&gt;&lt;a href=&quot;#知乎优秀问答推荐&quot; class=&quot;headerlink&quot; title=&quot;知乎优秀问答推荐&quot;&gt;&lt;/a&gt;知乎优秀问答推荐&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;https://www.zhihu.com/question/322191738&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;为什么深度学习中的图像分割要先编码再解码？&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://www.zhihu.com/question/321677995&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;如何看待2019年发布的中国计算机学会(CCF)推荐国际学术会议和期刊目录？&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>2019年04月22日—2019年04月28日周报</title>
    <link href="https://artificial-intelligence.net.cn/2019/04/27/1556346043/"/>
    <id>https://artificial-intelligence.net.cn/2019/04/27/1556346043/</id>
    <published>2019-04-27T06:20:43.000Z</published>
    <updated>2019-06-05T06:03:41.171Z</updated>
    
    <content type="html"><![CDATA[<h2 id="知乎优秀问答推荐"><a href="#知乎优秀问答推荐" class="headerlink" title="知乎优秀问答推荐"></a>知乎优秀问答推荐</h2><ul><li><a href="https://www.zhihu.com/question/321068146" target="_blank" rel="noopener">自然语言处理（NLP）领域中有哪些值得复现的论文（模型）？</a></li><li><a href="https://www.zhihu.com/question/305256736" target="_blank" rel="noopener">自然语言处理中有哪些常用的数据增强的方式呢？</a><a id="more"></a></li><li><a href="https://www.zhihu.com/question/320845192" target="_blank" rel="noopener">理论计算机科学(TCS) 与 机器学习(Machine Learning) 之间有哪些联系？</a></li><li><a href="https://www.zhihu.com/question/320606353" target="_blank" rel="noopener">BERT模型在NLP中目前取得如此好的效果，那下一步NLP该何去何从？</a></li><li><a href="https://www.zhihu.com/question/34878706" target="_blank" rel="noopener">LSTM如何来避免梯度弥散和梯度爆炸？</a></li><li><a href="https://www.zhihu.com/question/321088108" target="_blank" rel="noopener">word2vec中的负例采样为什么可以得到和softmax一样的效果？</a></li><li><a href="https://www.zhihu.com/question/65816885" target="_blank" rel="noopener">MacBook Pro适合深度学习吗？</a></li><li><a href="https://www.zhihu.com/question/318758024" target="_blank" rel="noopener">GPU并行计算工程师人才缺口大吗？需要掌握哪些知识？</a></li><li><a href="https://www.zhihu.com/question/317250430" target="_blank" rel="noopener">在你的领域，如何培养好的科研品味？</a></li><li><a href="https://www.zhihu.com/question/316175486" target="_blank" rel="noopener">现在模拟退火算法、粒子群优化算法、遗传算法和蚁群优化算法现在用的还多吗？</a></li><li><a href="https://www.zhihu.com/question/288234614" target="_blank" rel="noopener">深度强化学习与深度学习的的区别是啥？</a></li><li><a href="https://www.zhihu.com/question/314023997" target="_blank" rel="noopener">0基础自学图像处理→机器视觉→深度学习，应该怎么入门？</a></li></ul><h2 id="名校公开课推荐（CMU专场）"><a href="#名校公开课推荐（CMU专场）" class="headerlink" title="名校公开课推荐（CMU专场）"></a>名校公开课推荐（CMU专场）</h2><ul><li>11-485/785 Introduction to Deep Learning</li><li></li></ul><h2 id="优秀开源书籍推荐"><a href="#优秀开源书籍推荐" class="headerlink" title="优秀开源书籍推荐"></a>优秀开源书籍推荐</h2><h2 id="技术文章推荐（15篇）"><a href="#技术文章推荐（15篇）" class="headerlink" title="技术文章推荐（15篇）"></a>技术文章推荐（15篇）</h2><ul><li>kNN 的花式用法 <em><a href="https://zhuanlan.zhihu.com/p/62450795" target="_blank" rel="noopener">原文</a></em> </li><li>简单的交叉熵，你真的懂了吗？ <em><a href="https://zhuanlan.zhihu.com/p/61944055" target="_blank" rel="noopener">原文</a></em></li><li>PyTorch1.0基本概念与实践 <em><a href="https://zhuanlan.zhihu.com/p/58821816" target="_blank" rel="noopener">原文</a></em></li><li>进一步改进GPT和BERT：使用Transformer的语言模型 <em><a href="https://www.jiqizhixin.com/articles/2019-04-30-9" target="_blank" rel="noopener">原文</a></em></li><li>自然语言处理中的语言模型预训练方法 <em><a href="https://mp.weixin.qq.com/s/A-PKyZcXwOz-2lL-hBmjsA" target="_blank" rel="noopener">原文</a></em></li><li>从Word Embedding到Bert模型——自然语言处理预训练技术发展史 <em><a href="https://mp.weixin.qq.com/s/NqMUHFc4IUgYZzbwa33jRA" target="_blank" rel="noopener">原文</a></em></li><li>中文对比英文自然语言处理NLP的区别综述 _<a href="https://mp.weixin.qq.com/s/LQU_HJ4q74lL5oCIk7w5RA" target="_blank" rel="noopener">原文</a>_</li><li>如何做好文本关键词提取？从三种算法说起 <em><a href="https://mp.weixin.qq.com/s/yTLiw9am0wzeJ-O3m0xUoQ" target="_blank" rel="noopener">原文</a></em></li><li>fastText原理及实践 <em><a href="https://mp.weixin.qq.com/s/mWlBNjWU7VarFibwxnQOxQ" target="_blank" rel="noopener">原文</a></em></li><li>个性化推荐系统实践应用 _<a href="https://mp.weixin.qq.com/s/lnCe_3ssP6IEOHEX-N4vyg" target="_blank" rel="noopener">原文</a>_</li><li>一文详解维基百科的开放性问答系统 <em><a href="https://mp.weixin.qq.com/s/Mzycy0chQUNWjJYdpERBOw" target="_blank" rel="noopener">原文</a></em></li><li>简谈马尔可夫模型在个性化推荐中的应用 _<a href="https://mp.weixin.qq.com/s/nQAqEZW_TJSsbVp-kVcKGA" target="_blank" rel="noopener">原文</a>_</li><li>几分钟构建自己的深度学习开发环境 <em><a href="https://towardsdatascience.com/build-your-own-robust-deep-learning-environment-in-minutes-354cf140a5a6" target="_blank" rel="noopener">原文</a></em> _<a href="https://mp.weixin.qq.com/s/-y_01EBYVxiCwLddCvyFfg" target="_blank" rel="noopener">翻译</a>_</li><li>分布式入门，怎样用PyTorch实现多GPU分布式训练 <em><a href="https://medium.com/intel-student-ambassadors/distributed-training-of-deep-learning-models-with-pytorch-1123fa538848" target="_blank" rel="noopener">原文</a></em> <em><a href="https://www.jiqizhixin.com/articles/2019-04-30-8" target="_blank" rel="noopener">翻译</a></em></li><li>PyTorch最佳实践，怎样才能写出一手风格优美的代码 <em><a href="https://github.com/IgorSusmelj/pytorch-styleguide" target="_blank" rel="noopener">原文</a></em> <em><a href="https://www.jiqizhixin.com/articles/2019-04-29-5" target="_blank" rel="noopener">翻译</a></em></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;知乎优秀问答推荐&quot;&gt;&lt;a href=&quot;#知乎优秀问答推荐&quot; class=&quot;headerlink&quot; title=&quot;知乎优秀问答推荐&quot;&gt;&lt;/a&gt;知乎优秀问答推荐&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;https://www.zhihu.com/question/321068146&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;自然语言处理（NLP）领域中有哪些值得复现的论文（模型）？&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://www.zhihu.com/question/305256736&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;自然语言处理中有哪些常用的数据增强的方式呢？&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>2019年04月15日—2019年04月21日周报</title>
    <link href="https://artificial-intelligence.net.cn/2019/04/20/1555741208/"/>
    <id>https://artificial-intelligence.net.cn/2019/04/20/1555741208/</id>
    <published>2019-04-20T06:20:08.000Z</published>
    <updated>2019-06-05T06:03:41.173Z</updated>
    
    <content type="html"><![CDATA[<h2 id="知乎优秀问答推荐"><a href="#知乎优秀问答推荐" class="headerlink" title="知乎优秀问答推荐"></a>知乎优秀问答推荐</h2><ul><li><a href="https://www.zhihu.com/question/60241622" target="_blank" rel="noopener">哪些你看了以后大呼过瘾的数据分析书？</a></li><li><a href="https://www.zhihu.com/question/314002073" target="_blank" rel="noopener">能用比较通俗有趣的语言解释RNN和LSTM？</a><a id="more"></a></li><li><a href="https://www.zhihu.com/question/27343287" target="_blank" rel="noopener">熵增理论为什么让好多人一下子顿悟了？</a></li><li><a href="https://www.zhihu.com/question/320508897" target="_blank" rel="noopener">CV博士第二年，不知未来何去何从？</a></li><li><a href="https://zhuanlan.zhihu.com/p/29704017" target="_blank" rel="noopener">如何用3个月零基础入门「机器学习」？</a></li><li><a href="https://www.zhihu.com/question/320041467" target="_blank" rel="noopener">你们都是怎么学习计算机视觉的？</a></li><li><a href="https://www.zhihu.com/question/264189719" target="_blank" rel="noopener">如何理解随机梯度下降(Stochastic gradient descent，SGD)？</a></li><li><a href="https://www.zhihu.com/question/294635686" target="_blank" rel="noopener">有哪些「魔改」loss函数，曾经拯救了你的深度学习模型？</a></li><li><a href="https://www.zhihu.com/question/286925266" target="_blank" rel="noopener">2019 秋招的 AI 岗位竞争激烈吗？</a></li><li><a href="https://www.zhihu.com/question/314800147" target="_blank" rel="noopener">有哪些好用的机器学习和数据挖掘工具？</a></li><li><a href="https://www.zhihu.com/question/280696035" target="_blank" rel="noopener">数据挖掘中常见的「异常检测」算法有哪些？</a></li><li><a href="https://www.zhihu.com/question/318749276" target="_blank" rel="noopener">迁移学习如何落地？如何判定你的任务需要迁移学习？迁移学习目前存在的问题？</a></li></ul><h2 id="名校公开课推荐（MIT专场）"><a href="#名校公开课推荐（MIT专场）" class="headerlink" title="名校公开课推荐（MIT专场）"></a>名校公开课推荐（MIT专场）</h2><ul><li>MIT 18.01 : Single Variable Calculus<ul><li><a href="https://ocw.mit.edu/courses/mathematics/18-01-single-variable-calculus-fall-2006/" target="_blank" rel="noopener">MIT Open Course Ware</a></li><li><a href="https://www.youtube.com/playlist?list=PL590CCC2BC5AF3BC1" target="_blank" rel="noopener">YouTube（Fall 2006，英文）</a></li><li><a href="http://open.163.com/special/sp/singlevariablecalculus.html" target="_blank" rel="noopener">网易公开课（Fall 2006，中文）</a></li><li><a href="http://open.163.com/special/opencourse/calculus.html" target="_blank" rel="noopener">网易公开课（Fall 2006，中文，习题课）</a></li></ul></li><li>MIT 18.02 : Multivariable Calculus <ul><li><a href="https://ocw.mit.edu/courses/mathematics/18-02-multivariable-calculus-fall-2007/" target="_blank" rel="noopener">MIT Open Course Ware</a></li><li><a href="https://www.youtube.com/playlist?list=PL4C4C8A7D06566F38" target="_blank" rel="noopener">YouTube（Fall 2007，英文）</a></li><li><a href="https://www.youtube.com/watch?v=X9t-u87df3o&amp;list=PLBE9407EA64E2C318" target="_blank" rel="noopener">YouTube（Highlights of Calculus，英文）</a></li><li><a href="https://www.bilibili.com/video/av42204912" target="_blank" rel="noopener">Bilibili（Fall 2007，中文）</a></li><li><a href="http://open.163.com/special/opencourse/multivariable.html" target="_blank" rel="noopener">网易公开课（Fall 2007，中文）</a></li><li><a href="http://open.163.com/special/opencourse/duobianliangweijifen.html" target="_blank" rel="noopener">网易公开课（Fall 2007，中文，习题课）</a></li><li><a href="http://open.163.com/special/opencourse/weijifen.html" target="_blank" rel="noopener">网易公开课（微积分重点，中文）</a></li></ul></li><li>MIT 18.03 : Differential Equations<ul><li><a href="https://ocw.mit.edu/courses/mathematics/18-03-differential-equations-spring-2010/" target="_blank" rel="noopener">MIT Open Course Ware</a></li><li><a href="https://www.youtube.com/playlist?list=PLEC88901EBADDD980" target="_blank" rel="noopener">YouTube（Spring 2006，英文）</a></li><li><a href="https://www.youtube.com/playlist?list=PL64BDFBDA2AF24F7E" target="_blank" rel="noopener">YouTube（Fall 2011，英文）</a></li><li><a href="http://open.163.com/special/opencourse/equations.html" target="_blank" rel="noopener">网易公开课（Spring 2006，中文）</a></li></ul></li><li>MIT 18.06 : Linear Algebra <ul><li><a href="http://ocw.mit.edu/18-06SCF11" target="_blank" rel="noopener">MIT Open Course Ware</a></li><li><a href="https://www.youtube.com/playlist?list=PL221E2BBF13BECF6C" target="_blank" rel="noopener">YouTube（Fall 2011，英文）</a></li><li><a href="https://www.youtube.com/playlist?list=PL221E2BBF13BECF6C" target="_blank" rel="noopener">YouTube（Fall 2011，英文，习题课）</a></li><li><a href="https://www.youtube.com/playlist?list=PL751FDC0AD3A71B3E" target="_blank" rel="noopener">YouTube（Fall 2011，中文，习题课）</a></li><li><a href="https://www.bilibili.com/video/av41855698" target="_blank" rel="noopener">Bilibili（ Spring 2010，中文）</a></li><li><a href="http://open.163.com/special/opencourse/daishu.html" target="_blank" rel="noopener">网易公开课（Spring 2010，中文）</a></li><li><a href="http://open.163.com/special/opencourse/mitxianxingdaishuxitike.html" target="_blank" rel="noopener">网易公开课（Spring 2010，中文，习题课）</a></li></ul></li><li>MIT 18.650 : Statistics for Applications<ul><li><a href="http://ocw.mit.edu/18-650F16" target="_blank" rel="noopener">MIT Open Course Ware</a></li><li><a href="https://www.youtube.com/playlist?list=PLUl4u3cNGP60uVBMaoNERc6knT_MgPKS0" target="_blank" rel="noopener">YouTube（Fall 2016，英文）</a></li></ul></li><li>MIT RES.6-007 : Signals and Systems<ul><li><a href="https://ocw.mit.edu/resources/res-6-007-signals-and-systems-spring-2011/" target="_blank" rel="noopener">MIT Open Course Ware</a></li><li><a href="https://www.bilibili.com/video/av46126932" target="_blank" rel="noopener">Bilibili（1987，中文）</a></li><li>[][]<a href="http://open.163.com/special/opencourse/signals.html" target="_blank" rel="noopener">网易公开课（1987，中文）</a></li></ul></li><li>MIT RES.6-008 : Digital Signal Processing<ul><li><a href="http://ocw.mit.edu/RES6-008S11" target="_blank" rel="noopener">MIT Open Course Ware</a></li><li><a href="https://www.youtube.com/playlist?list=PL8157CA8884571BA2" target="_blank" rel="noopener">YouTube（1975，英文）</a></li></ul></li><li>MIT RES.6-012 : Introduction to Probability<ul><li><a href="https://ocw.mit.edu/RES-6-012S18" target="_blank" rel="noopener">MIT Open Course Ware</a></li><li><a href="https://www.youtube.com/playlist?list=PLUl4u3cNGP60hI9ATjSFgLZpbNJ7myAg6" target="_blank" rel="noopener">YouTube（Spring 2018，英文）</a></li></ul></li><li>MIT 6.041 : Probabilistic Systems Analysis and Applied Probability<ul><li><a href="https://ocw.mit.edu/courses/electrical-engineering-and-computer-science/6-041-probabilistic-systems-analysis-and-applied-probability-fall-2010/" target="_blank" rel="noopener">MIT Open Course Ware</a></li><li><a href="https://www.youtube.com/playlist?list=PLUl4u3cNGP61MdtwGTqZA0MreSaDybji8" target="_blank" rel="noopener">YouTube（Spring 2010，英文）</a></li></ul></li><li>MIT 6.041SC : Probabilistic Systems Analysis and Applied Probability<ul><li><a href="http://ocw.mit.edu/6-041SCF13" target="_blank" rel="noopener">MIT Open Course Ware</a></li><li><a href="https://www.youtube.com/playlist?list=PLUl4u3cNGP60A3XMwZ5sep719_nh95qOe" target="_blank" rel="noopener">YouTube（Fall 2013，英文）</a></li></ul></li><li>MIT 6.042J : Mathematics for Computer Science<ul><li><a href="https://ocw.mit.edu/courses/electrical-engineering-and-computer-science/6-042j-mathematics-for-computer-science-spring-2015/" target="_blank" rel="noopener">MIT Open Course Ware</a></li><li><a href="https://www.youtube.com/playlist?list=PLB7540DEDD482705B" target="_blank" rel="noopener">YouTube（Spring 2010，英文）</a></li><li><a href="https://www.youtube.com/playlist?list=PLUl4u3cNGP60UlabZBeeqOuoLuj_KNphQ" target="_blank" rel="noopener">YouTube（Spring 2015，英文）</a></li></ul></li><li>MIT 6.006 : Introduction to Algorithms<ul><li><a href="https://ocw.mit.edu/courses/electrical-engineering-and-computer-science/6-006-introduction-to-algorithms-fall-2011/" target="_blank" rel="noopener">MIT Open Course Ware</a></li><li><a href="https://www.youtube.com/playlist?list=PLUl4u3cNGP61Oq3tWYp6V_F-5jb5L2iHb" target="_blank" rel="noopener">YouTube（Fall 2011，英文）</a></li></ul></li><li>MIT 6.046j : Design and Analysis of Algorithms<ul><li><a href="https://ocw.mit.edu/courses/electrical-engineering-and-computer-science/6-046j-design-and-analysis-of-algorithms-spring-2015/" target="_blank" rel="noopener">MIT Open Course Ware</a></li><li><a href="https://www.youtube.com/playlist?list=PL8B24C31197EC371C" target="_blank" rel="noopener">YouTube（Fall 2005，英文）</a></li><li><a href="https://www.youtube.com/playlist?list=PLUl4u3cNGP6317WaSNfmCvGym2ucw3oGp" target="_blank" rel="noopener">YouTube（Spring 2015，英文）</a></li><li><a href="http://open.163.com/special/opencourse/algorithms.html" target="_blank" rel="noopener">网易公开课（Fall 2005，中文）</a></li></ul></li><li>MIT 6.050j : Information and Entropy<ul><li><a href="https://ocw.mit.edu/courses/electrical-engineering-and-computer-science/6-050j-information-and-entropy-spring-2008/" target="_blank" rel="noopener">MIT Open Course Ware</a></li><li><a href="https://www.youtube.com/playlist?list=PLDDE03B3BDCA1D9B1" target="_blank" rel="noopener">YouTube（Spring 2008，英文）</a></li><li><a href="http://open.163.com/special/opencourse/information.html" target="_blank" rel="noopener">网易公开课（Spring 2008，中文）</a></li></ul></li><li>MIT 6.034 : Artificial Intelligence<ul><li><a href="https://ocw.mit.edu/courses/electrical-engineering-and-computer-science/6-034-artificial-intelligence-fall-2010/" target="_blank" rel="noopener">MIT Open Course Ware</a></li><li><a href="https://www.youtube.com/playlist?list=PLnvKubj2-I2LhIibS8TOGC42xsD3-liux" target="_blank" rel="noopener">YouTube（Fall 2010，英文）</a></li><li><a href="http://open.163.com/movie/2017/9/Q/S/MCTMNN3UI_MCTMNR8QS.html" target="_blank" rel="noopener">网易公开课（Fall 2010，中文）</a></li></ul></li><li>MIT 6.832 : Underactuated Robotics<ul><li><a href="http://ocw.mit.edu/6-832s09" target="_blank" rel="noopener">MIT Open Course Ware</a></li><li><a href="https://www.youtube.com/playlist?list=PL58F1D0056F04CF8C" target="_blank" rel="noopener">Youtube（Spring 2009，英文）</a></li></ul></li><li>MIT 6.867 : Machine Learning<ul><li><a href="https://ocw.mit.edu/courses/electrical-engineering-and-computer-science/6-867-machine-learning-fall-2006/" target="_blank" rel="noopener">MIT Open Course Ware</a></li><li><a href="https://www.youtube.com/playlist?list=PL6ogFv-ieghdHwSbJBmsWm5ZY2X0Bn4Nx" target="_blank" rel="noopener">YouTube（Fall 2016，英文）</a></li></ul></li><li>MIT 6.S094 : Deep Learning for Self-Driving Cars<ul><li><a href="https://deeplearning.mit.edu" target="_blank" rel="noopener">官网</a></li><li><a href="https://www.youtube.com/playlist?list=PLrAXtmErZgOeiKm4sgNOknGvNjby9efdf" target="_blank" rel="noopener">YouTube（2018，英文）</a></li><li><a href="http://www.mooc.ai/course/483" target="_blank" rel="noopener">AI研习社（2018，中文）</a></li></ul></li><li>MIT 6.S191 : Introduction to Deep Learning<ul><li><a href="http://introtodeeplearning.com" target="_blank" rel="noopener">官网</a></li><li><a href="https://www.youtube.com/playlist?list=PLtBw6njQRU-rwp5__7C0oIVt26ZgjG9NI" target="_blank" rel="noopener">YouTube（2019，英文）</a></li><li><a href="https://www.bilibili.com/video/av45552854" target="_blank" rel="noopener">Bilibili（2019，英文）</a></li></ul></li><li>MIT 6.819/6.869 : Advances in Computer Vision<ul><li><a href="http://6.869.csail.mit.edu" target="_blank" rel="noopener">官网</a></li></ul></li><li>MIT 6.864 : Advanced Natural Language Processing<ul><li><a href="http://people.csail.mit.edu/regina/6864/" target="_blank" rel="noopener">官网</a></li><li><a href="https://ocw.mit.edu/courses/electrical-engineering-and-computer-science/6-864-advanced-natural-language-processing-fall-2005/" target="_blank" rel="noopener">MIT Open Course Ware</a></li></ul></li></ul><h2 id="优秀开源书籍推荐"><a href="#优秀开源书籍推荐" class="headerlink" title="优秀开源书籍推荐"></a>优秀开源书籍推荐</h2><ul><li><a href="https://web.stanford.edu/~jurafsky/slp3/" target="_blank" rel="noopener">斯坦福大学教授 Dan Jurafsky 与科罗拉多大学波德分校 James H. Martin 教授共同撰写的《Speech and Language Processing》第三版</a></li><li><a href="http://huaxiaozhuan.com" target="_blank" rel="noopener">前阿里巴巴资深算法工程师，现智易科技首席算法研究员华校专所著《AI算法工程师手册》</a></li><li><a href="https://zhuanlan.zhihu.com/p/59507137" target="_blank" rel="noopener">知乎NLP爱好者Doit所写《TensorFlow 2.0中文教程》</a></li></ul><h2 id="技术文章推荐（15篇）"><a href="#技术文章推荐（15篇）" class="headerlink" title="技术文章推荐（15篇）"></a>技术文章推荐（15篇）</h2><ul><li>斯坦福大学博士李纪为谈初入NLP小建议 <em><a href="https://mp.weixin.qq.com/s/VFjRRHiD9RX45FkfkmSuNw" target="_blank" rel="noopener">原文</a></em> </li><li>如何解决自然语言处理中 90% 的问题？ <em><a href="https://mp.weixin.qq.com/s/Q1AgUF4NgX_6cWI3Pt1JMQ" target="_blank" rel="noopener">原文</a></em></li><li>最大熵马尔可夫模型 <em><a href="https://zhuanlan.zhihu.com/p/60651380" target="_blank" rel="noopener">原文</a></em></li><li>常见神经网络模型在自然语言处理领域中的应用 <em><a href="https://zhuanlan.zhihu.com/p/60976912" target="_blank" rel="noopener">原文</a></em></li><li>深度学习、机器学习与NLP的前世今生 <em><a href="https://mp.weixin.qq.com/s/RSWF5XxH3B1eQvY_DlEwew" target="_blank" rel="noopener">原文</a></em></li><li>GAN万字长文综述 <em><a href="https://zhuanlan.zhihu.com/p/58812258" target="_blank" rel="noopener">原文</a></em></li><li>医学图像领域的GANs <em><a href="https://zhuanlan.zhihu.com/p/59521772" target="_blank" rel="noopener">原文</a></em></li><li>XGBoost算法梳理 <em><a href="https://zhuanlan.zhihu.com/p/58292935" target="_blank" rel="noopener">原文</a></em></li><li>集成学习算法(Ensemble Method)浅析 <em><a href="https://mp.weixin.qq.com/s/hGoprRIeyoXPt5OnzgV-bg" target="_blank" rel="noopener">原文</a></em></li><li>CNN卷积神经网络 <em><a href="https://zhuanlan.zhihu.com/p/61510829" target="_blank" rel="noopener">原文</a></em></li><li>PyTorch 训练 RNN 时，序列长度不固定怎么办？ <em><a href="https://zhuanlan.zhihu.com/p/59772104" target="_blank" rel="noopener">原文</a></em> </li><li>手把手教你用卷积神经网络搞定识别 <em><a href="https://towardsdatascience.com/simple-introduction-to-convolutional-neural-networks-cdf8d3077bac" target="_blank" rel="noopener">原文</a></em> <em><a href="https://mp.weixin.qq.com/s/ieRSaujQXDK2DffWv2C2-Q" target="_blank" rel="noopener">翻译</a></em></li><li>使用 Python 通过基于颜色的图像分割进行物体检测 <em><a href="https://towardsdatascience.com/object-detection-via-color-based-image-segmentation-using-python-e9b7c72f0e11" target="_blank" rel="noopener">原文</a></em> <em><a href="https://mp.weixin.qq.com/s/nBPBvobavG5RfP4o6GftFg" target="_blank" rel="noopener">翻译</a></em></li><li>特征工程自动化如何为机器学习带来重大变化 <em><a href="https://towardsdatascience.com/why-automated-feature-engineering-will-change-the-way-you-do-machine-learning-5c15bf188b96" target="_blank" rel="noopener">原文</a></em> <em><a href="https://mp.weixin.qq.com/s/rSIMbB-4nZkvh5Ys0Pl98A" target="_blank" rel="noopener">翻译</a></em></li><li>非平衡数据集 focal loss 多类分类 <em><a href="https://medium.com/swlh/multi-class-classification-with-focal-loss-for-imbalanced-datasets-c478700e65f5" target="_blank" rel="noopener">原文</a></em> <em><a href="https://mp.weixin.qq.com/s/1khjy026s5fy8sQqoRwS_w" target="_blank" rel="noopener">翻译</a></em></li></ul><h2 id="面经、赛经"><a href="#面经、赛经" class="headerlink" title="面经、赛经"></a>面经、赛经</h2><ul><li><a href="https://zhuanlan.zhihu.com/p/60439147" target="_blank" rel="noopener">腾讯算法实习面试总结——论面试官虐我的一百种方式 </a></li><li><a href="https://zhuanlan.zhihu.com/p/58199302" target="_blank" rel="noopener">如何在Kaggle排行榜取得好成绩——Kaggle新人参加Digit Recognition的经历</a></li><li><a href="https://zhuanlan.zhihu.com/p/60970716" target="_blank" rel="noopener">kaggle 首战拿金牌总结</a></li><li><a href="https://mp.weixin.qq.com/s/HemBoi7I-mjLde1_bChq-g" target="_blank" rel="noopener">作为一个深度学习新手团队，我是如何拿到 Kaggle 比赛第三名的？</a></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;知乎优秀问答推荐&quot;&gt;&lt;a href=&quot;#知乎优秀问答推荐&quot; class=&quot;headerlink&quot; title=&quot;知乎优秀问答推荐&quot;&gt;&lt;/a&gt;知乎优秀问答推荐&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;https://www.zhihu.com/question/60241622&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;哪些你看了以后大呼过瘾的数据分析书？&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://www.zhihu.com/question/314002073&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;能用比较通俗有趣的语言解释RNN和LSTM？&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>人工智能领域的“顶会顶刊”指的是什么</title>
    <link href="https://artificial-intelligence.net.cn/2019/04/19/1555677328/"/>
    <id>https://artificial-intelligence.net.cn/2019/04/19/1555677328/</id>
    <published>2019-04-19T12:35:28.000Z</published>
    <updated>2019-06-05T06:03:41.175Z</updated>
    
    <content type="html"><![CDATA[<p>我们在学习人工智能时，免不了要在各种书籍、公众号、新闻上看到”顶会顶刊“这样的字词。甚至在找工作之前，我们观看大神们的面经时，也会经常看到”顶会顶刊“这样的字词。由于这些字词经常与某些知名大神，某些知名教授，或者某些知名公司联系在一起，不禁在各位初入人工智能领域的小白们心中蒙上了一层神秘的色彩。</p><a id="more"></a><p>什么是人工智能领域的“顶会顶刊”？我们为什么要在“顶会顶刊”上发表论文？怎样在“顶会顶刊”上发表论文？</p><p>带着这样的疑问，笔者查阅了大量的资料，并将资料中的内容提炼、总结，写下了本文。</p><h4 id="1-什么是论文？什么是学术论文？"><a href="#1-什么是论文？什么是学术论文？" class="headerlink" title="1. 什么是论文？什么是学术论文？"></a>1. 什么是论文？什么是学术论文？</h4><p><strong>论文</strong>是探讨问题进行学术研究的一种手段，又是描述学术研究成果进行学术交流的一种工具。</p><p>参加过本科、硕士、博士、及部分大专学习的同学，在毕业时都需要进行论文答辩，所以绝大部分学习人工智能或将要从事人工智能领域工作的的同学，都有过写论文的经历，这一类论文称为<strong>毕业论文</strong>。那么学术论文又有什么不同呢？</p><p><strong>学术论文</strong>是某一学术课题在实验性、理论性或预测性上具有的新的科学研究成果或创新见解和知识的科学记录，或是某种已知原理应用于实际上取得新进展的科学总结，用以提供<strong>学术会议</strong>上宣读、交流、讨论或<strong>学术刊物</strong>上发表，或用作其他用途的书面文件。</p><h4 id="2-什么是学术会议？什么是会议论文？什么是学术期刊？什么是期刊论文？"><a href="#2-什么是学术会议？什么是会议论文？什么是学术期刊？什么是期刊论文？" class="headerlink" title="2. 什么是学术会议？什么是会议论文？什么是学术期刊？什么是期刊论文？"></a>2. 什么是学术会议？什么是会议论文？什么是学术期刊？什么是期刊论文？</h4><p><strong>学术会议</strong>是一种以促进科学发展、学术交流、课题研究等学术性话题为主题的会议。学术会议一般都具有国际性、权威性、高知识性、高互动性等特点，其参会者一般为科学家、学者、教师等具有高学历的研究人员。由于学术会议是一种交流的，互动的会议，因此参会者往往都会将自己的研究成果以学术展板的形式展示出来，使得互动交流更加直观、效果更好。</p><p><strong>会议论文</strong>就是在<strong>学术会</strong>这样的正式场合宣读并且是首次发表的论文。会议论文是属于公开发表的论文，一般正式的学术交流会议都会出版会议论文集，这样发表的论文一般也会作为职称评定等考核内容。</p><p><strong>学术期刊</strong>是一种经过同行评审的期刊，发表在学术期刊上的文章，通常涉及特定的学科。学术期刊展示了研究领域的成果，并起到了公示的作用，其内容主要以原创研究、综述文章。</p><p><strong>期刊论文</strong>是指各学科领域中的专业人员为介绍自己的科研成果，发表在<strong>学术期刊</strong>上的学术论文。这类论文具有专门化的特点，针对性强，发表量大，反映各学科领域的新进展、新技术和新成果。</p><h4 id="3-会议论文与期刊论文有什么区别？为什么在CS（Computer-Science）领域，会议论文比期刊论文更重要呢？"><a href="#3-会议论文与期刊论文有什么区别？为什么在CS（Computer-Science）领域，会议论文比期刊论文更重要呢？" class="headerlink" title="3. 会议论文与期刊论文有什么区别？为什么在CS（Computer Science）领域，会议论文比期刊论文更重要呢？"></a>3. 会议论文与期刊论文有什么区别？为什么在CS（Computer Science）领域，会议论文比期刊论文更重要呢？</h4><p>CS领域有一个非常有趣的现象，极度重视学术会议，而期刊通常大部分期刊论文都是会议论文的扩展版，首发就在期刊上的相对较少。也正因为如此，CS期刊的影响因子都低到惊人的程度，顶级刊物往往也只有1到2左右，被引用的通常都是会议论文，而不是很久以后才出版的期刊版。</p><p>CS领域之所以更重视学术会议，就是因为CS技术更新速度快，而期刊需要等待漫长的时间，等待期刊发表时，当时最新的技术早已不是最新，</p><h3 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h3><ol><li><a href="https://baike.baidu.com/item/论文" target="_blank" rel="noopener">百度百科“论文”词条</a></li><li><a href="https://baike.baidu.com/item/毕业论文" target="_blank" rel="noopener">百度百科“毕业论文”词条</a></li><li><a href="https://baike.baidu.com/item/学术论文" target="_blank" rel="noopener">百度百科“学术论文”词条</a></li><li><a href="https://baike.baidu.com/item/学术会议" target="_blank" rel="noopener">百度百科“学术会议”词条</a></li><li><a href="https://baike.baidu.com/item/会议论文" target="_blank" rel="noopener">百度百科“会议论文”词条</a></li><li><a href="https://baike.baidu.com/item/学术期刊" target="_blank" rel="noopener">百度百科“学术期刊”词条</a></li><li><a href="https://baike.baidu.com/item/杂志论文" target="_blank" rel="noopener">百度百科“杂志论文”词条</a></li><li><a href="http://dy.163.com/v2/article/detail/DIMMT3P40516C52N.html" target="_blank" rel="noopener">网易号“学术圈的你，是时候了解一些期刊基本常识了！” by 百纳知识</a></li><li><a href="https://www.zhihu.com/question/54142181" target="_blank" rel="noopener">知乎问答“为什么计算机学术界认可顶级会议论文，而其他领域几乎都是只认可顶级期刊？”</a></li><li><a href="https://zhuanlan.zhihu.com/p/49521821" target="_blank" rel="noopener">知乎专栏“会议论文和期刊论文的区别” by 志鹏学术华</a></li><li><a href="https://zhuanlan.zhihu.com/p/40551730" target="_blank" rel="noopener">知乎专栏“发表过的会议论文还能发期刊论文吗？” by Dr.Wu</a></li><li><a href="https://academia.stackexchange.com/questions/38086/why-are-conference-papers-so-important-in-computer-science-cs" target="_blank" rel="noopener">StackExchange Academia Question: “Why are conference papers so important in computer  science (CS) ?”</a></li><li><a href="https://homes.cs.washington.edu/~mernst/advice/conferences-vs-journals.html" target="_blank" rel="noopener">“Choosing a venue: conference or journal?” by Michael Ernst (mernst@cs.washington.edu)</a></li><li><a href="https://cacm.acm.org/magazines/2009/5/24632-conferences-vs-journals-in-computing-research/fulltext" target="_blank" rel="noopener">“Conferences vs. Journals in Computing Research” by Moshe Y.Vardi</a></li><li><a href="https://www.zhihu.com/question/68486939" target="_blank" rel="noopener">知乎问答“如何评价新Nature子刊Nature Machine Interlligence的出现？”</a></li><li><a href="https://www.zhihu.com/question/275233006" target="_blank" rel="noopener">知乎问答“如何评价机器学习社区对Nature Machine Intelligence的抵制行动？”</a></li><li><a href="https://zhuanlan.zhihu.com/p/37115978" target="_blank" rel="noopener">知乎专栏“为什么AI学者们都不约而同地抵制这个《Nature》子刊？”</a></li><li><a href="https://www.jiqizhixin.com/articles/2019-01-08-19?from=synced&amp;keyword=然历遭抵制，Nature机器智能子刊最终上线" target="_blank" rel="noopener">“虽然历遭抵制，Nature机器智能子刊最终上线” by 机器之心</a></li><li><a href="https://zhuanlan.zhihu.com/p/41261967" target="_blank" rel="noopener">“2018谷歌学术影响因子发布：CVPR排名泛领域第一” by 量子位</a></li><li><a href="https://zhuanlan.zhihu.com/p/41268000" target="_blank" rel="noopener">“2018谷歌学术期刊&amp;出版物排名公布：CVPR挤进前20”</a></li><li><a href="https://scholar.google.com/citations?view_op=top_venues&amp;hl=en&amp;vq=eng_artificialintelligence" target="_blank" rel="noopener">“Top pulications in Artificial intelligence” by Google Scholar</a></li><li><a href="https://www.ccf.org.cn/xspj/rgzn/" target="_blank" rel="noopener">“中国计算机学会推荐国际学术刊物、学术会议（人工智能方向）” by 中国计算机学会</a></li><li><a href="https://tryolabs.com/blog/machine-learning-deep-learning-conferences/" target="_blank" rel="noopener">tryolabs: “List of Machine Learning and Deep Learning conferences in 2019 / 2020” by Esther</a></li><li><a href="https://zhuanlan.zhihu.com/p/55565999" target="_blank" rel="noopener">知乎专栏“机器学习各领域顶会和实验室”</a></li><li><a href="https://zhuanlan.zhihu.com/p/28037164" target="_blank" rel="noopener">知乎专栏“国际顶尖计算机视觉、机器学习会议大搜罗——附排名&amp;接受率</a></li><li><a href="https://zhuanlan.zhihu.com/p/38595692" target="_blank" rel="noopener">知乎专栏“计算机视觉世界三大顶级会议介绍” by 裴若希</a></li><li><a href="https://www.zhihu.com/question/38424588" target="_blank" rel="noopener">知乎问答“数据挖掘、机器学习领域有哪些知名的期刊或会议？”</a></li><li><a href="https://www.zhihu.com/question/54878177" target="_blank" rel="noopener">知乎问答“什么才算计算机的顶级会议？”</a></li><li><a href="https://www.zhihu.com/question/33977252" target="_blank" rel="noopener">知乎问答“计算机科学各个方向有哪些顶刊和顶会？”</a></li><li><a href="https://www.zhihu.com/question/37687006" target="_blank" rel="noopener">知乎问答“计算机视觉顶尖期刊和会议有哪些？”</a></li><li><a href="http://muchong.com/html/201902/13173436.html" target="_blank" rel="noopener">小木虫帖子“计算机顶会和顶刊哪个难发，比如AI方向”</a></li><li><a href="http://muchong.com/html/201012/2659795.html" target="_blank" rel="noopener">小木虫帖子“为什么不去读顶级会议上的论文？适应于机器学习、计算机视觉和人工智能”</a></li><li><a href="https://www.jianshu.com/p/e5d352910e8e" target="_blank" rel="noopener">简书“人工智能顶级会议” by yansicing</a></li><li><a href="https://blog.csdn.net/xiaoshengforever/article/details/22193389" target="_blank" rel="noopener">CSDN“几个人工智能会议” by xiaoshengforever</a></li><li><a href="https://blog.csdn.net/u011447369/article/details/70172048" target="_blank" rel="noopener">CSDN转载“世界顶级人工智能会议的总结” by 周志华</a></li><li><a href="http://edwinren.blogspot.com/2008/07/blog-post_30.html" target="_blank" rel="noopener">“计算机学科发表论文介绍” by Edwin</a></li><li><a href="https://www.cnblogs.com/X-knight/p/9281538.html" target="_blank" rel="noopener">“发表一篇顶会论文的经验分享” by 勋爵</a></li><li><a href="https://www.quora.com/What-are-the-top-AI-conferences" target="_blank" rel="noopener">Quora Question: “What are the top AI conferences?”</a></li><li><a href="https://www.zhihu.com/question/308457941" target="_blank" rel="noopener">知乎问答“为什么没有中文的顶级学术期刊？”</a></li><li><a href="https://zhuanlan.zhihu.com/p/35866253" target="_blank" rel="noopener">“机器学习顶会NIPS要改名了” by 量子位</a></li><li><a href="https://www.zhihu.com/question/265066766" target="_blank" rel="noopener">知乎问答”对于应聘人工智能相关算法岗来说，顶会论文和算法比赛哪个更重要？”</a></li><li><a href="https://www.zhihu.com/question/277910619" target="_blank" rel="noopener">知乎问答“计算机视觉研究生怎样发表一篇顶会论文？”</a></li><li><a href="https://www.zhihu.com/question/277497549" target="_blank" rel="noopener">知乎问答“暑期科研，如何发CVPR？要做到什么程度？”</a></li><li><a href="https://www.zhihu.com/question/49781124" target="_blank" rel="noopener">知乎问答“怎样才能在NIPS上面发论文？”</a></li><li><a href="https://www.zhihu.com/question/295084174" target="_blank" rel="noopener">知乎问答”发Conference on Learning Theory (COLT)有多难？”</a></li><li><a href="https://www.zhihu.com/question/33630593" target="_blank" rel="noopener">知乎问答“从零基础开始想发一篇深度学习的论文要提前准备什么？写论文的周期大概多久？”</a></li><li><a href="https://www.zhihu.com/question/41537788" target="_blank" rel="noopener">知乎问答“本科生想发出彩的机器学习a类论文，如何准备？”</a></li><li><a href="https://www.zhihu.com/question/278569254" target="_blank" rel="noopener">知乎问答“机器学习的会议如何灌水？”</a></li><li><a href="https://www.zhihu.com/question/30912132" target="_blank" rel="noopener">知乎问答“本人现在研一，想水篇论文毕业，请问怎么在机器学习方面水paper？”</a></li><li><a href="https://www.zhihu.com/question/21427261" target="_blank" rel="noopener">知乎问答“如何能在本科三年级就发表论文？”</a></li><li><a href="https://www.zhihu.com/question/39625348" target="_blank" rel="noopener">知乎问答“机器学习方面如何发表论文？”</a></li><li><a href="https://www.zhihu.com/question/313590513" target="_blank" rel="noopener">知乎问答”博士生在没有导师指导的情况下，该如何自己选题发 CVPR ？”</a></li><li><a href="https://www.zhihu.com/question/277126432" target="_blank" rel="noopener">知乎问答“发论文需要导师吗？”</a></li><li><a href="https://www.zhihu.com/question/31555786" target="_blank" rel="noopener">知乎问答“机器学习方面高质量的论文怎么找？”</a></li><li><a href="https://zhuanlan.zhihu.com/p/29828510" target="_blank" rel="noopener">知乎专栏“如何看待「不靠谱」的机器学习论文？”</a></li><li><a href="https://www.jiqizhixin.com/articles/070102" target="_blank" rel="noopener">“学完了在线课程？如何开启深度学习论文的阅读模式” by 机器之心</a></li><li><a href="https://towardsdatascience.com/getting-started-with-reading-deep-learning-research-papers-the-why-and-the-how-dfd1ac15dbc0" target="_blank" rel="noopener">Towards Data Science: “Getting started with reading Deep Learning Research papers: The Why and the How” by Nityesh Agarwal</a></li><li><a href="https://www.zhihu.com/question/50967184" target="_blank" rel="noopener">知乎问答“看机器学习论文时，看不懂数学公式怎么办？”</a></li><li><a href="https://www.zhihu.com/question/304334959" target="_blank" rel="noopener">知乎问答“研究新生要怎么看论文？”</a></li><li><a href="https://www.zhihu.com/question/299141026" target="_blank" rel="noopener">知乎问答“人工智能机器学习方面，有什么好的论文推荐？”</a></li><li><a href="https://www.zhihu.com/question/313632967" target="_blank" rel="noopener">知乎问答“机器学习方面的论文应该在哪找？”</a></li><li><a href="https://www.zhihu.com/question/27393375" target="_blank" rel="noopener">知乎问答“把深度学习作为自己的硕士课题，有什么可以做的？”</a></li><li><a href="https://www.zhihu.com/question/294551853" target="_blank" rel="noopener">知乎问答”关于深度学习方向选择问题？”</a></li><li><a href="https://www.zhihu.com/question/27458331" target="_blank" rel="noopener">知乎问答”计算机硕士论文方向求助，图片方向和深度学习方向的？”</a></li><li><a href="https://www.zhihu.com/question/52500200" target="_blank" rel="noopener">知乎问答”深度学习领域没人带没导师，全靠自己，想有idea最后发paper到底有多难？”</a></li><li><a href="https://www.zhihu.com/question/58343851" target="_blank" rel="noopener">知乎问答”新入学的计算机研究生怎么安排三年学习深度学习？”</a></li><li><a href="https://www.zhihu.com/question/308928022" target="_blank" rel="noopener">知乎问答”想问一下作为研究深度学习的研究生，毕业的时候掌握到什么程度才能算合格？”</a></li><li><a href="https://zhuanlan.zhihu.com/p/47036594" target="_blank" rel="noopener">知乎专栏“文科生用机器学习做论文，该写什么？” by 王树义</a></li><li><a href="https://www.jiqizhixin.com/articles/2019-04-19-6" target="_blank" rel="noopener">“投稿量激增56%，CVPR 2019接收论文的关键词是什么？” by 机器之心</a></li><li><a href="https://zhuanlan.zhihu.com/p/62540817" target="_blank" rel="noopener">知乎专栏”顶会论文接收量再破纪录，计算机视觉创新正在发生什么变化？”</a></li><li><a href="https://zhuanlan.zhihu.com/p/44304016" target="_blank" rel="noopener">知乎专栏“顶级会议的疯狂投稿下的AI现状” by Zahnxing Zhu</a></li><li><a href="https://zhuanlan.zhihu.com/p/50263270" target="_blank" rel="noopener">知乎专栏“关于本次 CVPR 2019 投稿的一些感想“  by Guosheng Hu</a></li><li><a href="https://www.zhihu.com/question/271293179" target="_blank" rel="noopener">知乎问答”大二就中 CVPR/ICCV/ECCV/NIPS 是一种怎样的体验？”</a></li><li><a href="https://www.zhihu.com/question/313590513" target="_blank" rel="noopener">知乎问答”如何看待CVPR 2019录用结果？”</a></li><li><a href="https://www.zhihu.com/question/54025865" target="_blank" rel="noopener">知乎问答“CV/ML顶级会议上的灌水文都有哪些特征？如何快速判断顶会论文是在灌水？”</a></li><li><a href="https://www.zhihu.com/question/39367974" target="_blank" rel="noopener">知乎问答“你读过的 机器学习/数据挖掘 顶级会议论文中最水的是哪一篇？”</a></li><li><a href="https://zhuanlan.zhihu.com/p/39971180" target="_blank" rel="noopener">知乎专栏“参加国际机器学习顶级会议ICML 2018有感” by 王晋东不在家</a></li><li><a href="https://zhuanlan.zhihu.com/p/26641697" target="_blank" rel="noopener">知乎专栏”ICLR小总结” by 罗若天</a></li><li><a href="https://bookdown.org/wshuyi/intro-to-scientific-writings4/#" target="_blank" rel="noopener">“毕业论文新手入坑手册” by 王树义</a></li><li><a href="https://www.zhihu.com/question/21083751" target="_blank" rel="noopener">知乎问答”如何研读一篇论文？”</a></li><li><a href="https://www.zhihu.com/question/23684933" target="_blank" rel="noopener">知乎问答”如何开始写英文论文？”</a></li><li><a href="https://www.zhihu.com/question/20829666" target="_blank" rel="noopener">知乎问答”第一次写学术论文无从下手怎么办？”</a></li><li><a href="https://www.zhihu.com/question/29784060" target="_blank" rel="noopener">知乎问答”国外有些学者在学生时期发论文横扫science nature，但独立后却只能在一般刊物上发论文。怎么看这个现象？”</a></li><li><a href="https://www.zhihu.com/question/304334959" target="_blank" rel="noopener">知乎问答”研究生新生要怎么看论文？”</a></li><li><a href="https://www.zhihu.com/question/20188973" target="_blank" rel="noopener">知乎问答”在家里如何免费使用中国知网？”</a></li><li><a href="https://www.zhihu.com/question/265953178" target="_blank" rel="noopener">知乎问答”如何评价深度学习相关顶级期刊论文难复现的问题？”</a></li><li><a href="https://www.zhihu.com/question/298891488" target="_blank" rel="noopener">知乎问答”想要复现一篇深度学习的论文需要做到哪些？”</a></li><li><a href="https://zhuanlan.zhihu.com/p/32084636" target="_blank" rel="noopener">知乎专栏”想在「国际期刊」发论文吗”</a></li><li><a href="https://zhuanlan.zhihu.com/p/44348312" target="_blank" rel="noopener">知乎专栏”硕士研究生论文投稿技巧”</a></li><li><a href="https://www.quora.com/Why-do-we-write-research-papers" target="_blank" rel="noopener">Quora “Why do we write research papers?”</a></li><li><a href="https://wenku.baidu.com/view/0ca4d0a44b35eefdc8d333f3.html" target="_blank" rel="noopener">“为什么要发表论文呢？” by 中华星火教育</a></li><li><a href="https://mp.weixin.qq.com/s/P_QEBNm70a8M0u79C-JT2w" target="_blank" rel="noopener">“126篇殿堂级深度学习论文分类整理 从入门到应用（上）” by songrotek</a></li><li><a href="https://mp.weixin.qq.com/s/SnOeb-qdWH0MCN4vH99j-A" target="_blank" rel="noopener">“126篇殿堂级深度学习论文分类整理 从入门到应用（下）” by songrotek</a></li><li><a href="https://zhuanlan.zhihu.com/p/42509495" target="_blank" rel="noopener">“十大深度学习热门论文（2018年版）” by 论智</a></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;我们在学习人工智能时，免不了要在各种书籍、公众号、新闻上看到”顶会顶刊“这样的字词。甚至在找工作之前，我们观看大神们的面经时，也会经常看到”顶会顶刊“这样的字词。由于这些字词经常与某些知名大神，某些知名教授，或者某些知名公司联系在一起，不禁在各位初入人工智能领域的小白们心中蒙上了一层神秘的色彩。&lt;/p&gt;
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>2019年04月08日—2019年04月14日周报</title>
    <link href="https://artificial-intelligence.net.cn/2019/04/14/1555208643/"/>
    <id>https://artificial-intelligence.net.cn/2019/04/14/1555208643/</id>
    <published>2019-04-14T02:24:03.000Z</published>
    <updated>2019-06-05T06:03:41.170Z</updated>
    
    <content type="html"><![CDATA[<h2 id="知乎优秀问答推荐"><a href="#知乎优秀问答推荐" class="headerlink" title="知乎优秀问答推荐"></a>知乎优秀问答推荐</h2><ul><li><a href="https://www.zhihu.com/question/266368852" target="_blank" rel="noopener">在实际业务里，在工作中有什么用的到深度学习的例子吗？用到GPU了吗？</a></li><li><a href="https://www.zhihu.com/question/299068775" target="_blank" rel="noopener">当前（2019年）机器学习中有哪些研究方向特别的坑？</a></li></ul><a id="more"></a><ul><li><a href="https://www.zhihu.com/question/274878965" target="_blank" rel="noopener">深度学习和计算机视觉领域在2018年还有哪些能突破/兴起或者全新的发展方向？</a></li><li><a href="https://www.zhihu.com/question/319621879" target="_blank" rel="noopener">以前的算法工程师是什么样的存在？</a></li><li><a href="https://www.zhihu.com/question/310484101" target="_blank" rel="noopener">算法工程师在工作中必须要手写算法吗？</a></li><li><a href="https://www.zhihu.com/question/267135168" target="_blank" rel="noopener">机器学习包含哪些学习思想？</a></li><li><a href="https://www.zhihu.com/question/30371867" target="_blank" rel="noopener">机器学习里的kernel是指什么？</a></li><li><a href="https://www.zhihu.com/question/313111229" target="_blank" rel="noopener">tf.keras 和 keras有什么区别？</a></li><li><a href="https://www.zhihu.com/question/319208407" target="_blank" rel="noopener">NLP历史上有哪些华人的研究是开创性的或者高引用率的？</a></li><li><a href="https://www.zhihu.com/question/35931336" target="_blank" rel="noopener">你写论文时发现了哪些神网站？</a></li><li><a href="https://www.zhihu.com/question/304334959" target="_blank" rel="noopener">研究生新生要怎么看论文？</a></li><li><a href="https://www.zhihu.com/question/311086366" target="_blank" rel="noopener">研究生计算机视觉/深度学习方向，如何准备即将到来的秋招？</a></li></ul><h2 id="名企公开课推荐（Google专场）"><a href="#名企公开课推荐（Google专场）" class="headerlink" title="名企公开课推荐（Google专场）"></a>名企公开课推荐（Google专场）</h2><ul><li><a href="https://www.youtube.com/watch?list=PLOU2XLYxmsIIuiBfYad6rFYQU_jL2ryal" target="_blank" rel="noopener">Machine Learning Recipes with Josh Gordon</a></li><li><a href="https://www.youtube.com/playlist?list=PLIivdWyY5sqJxnwJhe3etaK7utrBiPBQ2" target="_blank" rel="noopener">AI Adventures</a></li><li><a href="https://developers.google.com/machine-learning/crash-course" target="_blank" rel="noopener">Google 机器学习速成课程</a></li><li><a href="https://developers.google.com/machine-learning/practica" target="_blank" rel="noopener">Google 机器学习实战课程</a></li><li><a href="https://www.youtube.com/playlist?list=PLQY2H8rRoyvyK5aEDAI3wUUqC_F0oEroL" target="_blank" rel="noopener">TensorFlow in Google Colaboratory</a></li><li><a href="https://www.youtube.com/playlist?list=PLQY2H8rRoyvwLbzbnKJ59NkZvQAW9wLbx" target="_blank" rel="noopener">Coding TensorFlow</a></li><li><a href="https://www.deeplearning.ai/tensorflow-specialization" target="_blank" rel="noopener">TensorFlow: From Basics to Mastery（与deeplearning.ai合作）</a></li><li><a href="https://cn.udacity.com/course/intro-to-tensorflow-for-deep-learning--ud187" target="_blank" rel="noopener">深度学习工具 TensorFlow 入门（与Udacity合作）</a></li><li><a href="https://www.coursera.org/specializations/machine-learning-tensorflow-gcp" target="_blank" rel="noopener">Machine Learning with TensorFlow on Google Cloud Platform 专项课程</a></li><li><a href="https://www.coursera.org/specializations/advanced-machine-learning-tensorflow-gcp" target="_blank" rel="noopener">Advanced Machine Learning with TensorFlow on Google Cloud Platform 专项课程</a></li><li><a href="https://www.coursera.org/specializations/gcp-data-machine-learning" target="_blank" rel="noopener">Data Engineering on Google Cloud Platform 专项课程</a></li></ul><h2 id="优秀开源书籍推荐"><a href="#优秀开源书籍推荐" class="headerlink" title="优秀开源书籍推荐"></a>优秀开源书籍推荐</h2><ul><li><a href="http://lamda.nju.edu.cn/weixs/book/CNN_book.html" target="_blank" rel="noopener">旷视南京研究院魏秀参博士所著《解析深度学习——卷积神经网络原理与视觉实践》</a></li><li><a href="http://fancyerii.github.io/2019/03/14/dl-book/" target="_blank" rel="noopener">环信人工智能研发中心vp李理所著《深度学习理论与实战：提高篇》</a></li></ul><h2 id="技术文章推荐（15篇）"><a href="#技术文章推荐（15篇）" class="headerlink" title="技术文章推荐（15篇）"></a>技术文章推荐（15篇）</h2><ul><li>用分布式训练神经网络模型 <em><a href="https://mp.weixin.qq.com/s/mYp8Ueulshviwwbg-5LzDA" target="_blank" rel="noopener">原文</a></em></li><li>可能是最容易理解的EM算法入门文章 <em><a href="https://zhuanlan.zhihu.com/p/61768577" target="_blank" rel="noopener">原文</a></em></li><li>卷积网络中层应该怎么连？<em><a href="https://zhuanlan.zhihu.com/p/61746672" target="_blank" rel="noopener">原文</a></em></li><li>深度学习中的两种不确定性（上）<em><a href="https://zhuanlan.zhihu.com/p/56986840" target="_blank" rel="noopener">原文</a></em></li><li>对ResNet本质的一些思考 <em><a href="https://zhuanlan.zhihu.com/p/60668529" target="_blank" rel="noopener">原文</a></em></li><li>Pytorch模型训练特征图可视化（TensorboardX）<em><a href="https://zhuanlan.zhihu.com/p/60753993" target="_blank" rel="noopener">原文</a></em></li><li>机器学习入门：神经网络（简单介绍工作原理并用Python从头实现）<em><a href="https://victorzhou.com/blog/intro-to-neural-networks/" target="_blank" rel="noopener">原文</a> <a href="https://mp.weixin.qq.com/s/TqApITmWn8STgLnkjWeDog" target="_blank" rel="noopener">翻译</a></em></li><li>PyTorch进阶之路（一）：张量与梯度 <em><a href="https://medium.com/jovian-io/pytorch-basics-tensors-and-gradients-eb2f6e8a6eee" target="_blank" rel="noopener">原文</a> <a href="https://www.jiqizhixin.com/articles/2019-03-12-9" target="_blank" rel="noopener">翻译</a></em></li><li>PyTorch进阶之路（二）：如何实现线性回归 <em><a href="https://medium.com/jovian-io/linear-regression-with-pytorch-3dde91d60b50" target="_blank" rel="noopener">原文</a> <a href="https://www.jiqizhixin.com/articles/2019-03-15-5" target="_blank" rel="noopener">翻译</a></em></li><li>PyTorch进阶之路（三）：使用logistic回归实现图像分类 <em><a href="https://medium.com/jovian-io/image-classification-using-logistic-regression-in-pytorch-ebb96cc9eb79" target="_blank" rel="noopener">原文</a> <a href="https://www.jiqizhixin.com/articles/2019-03-15-17" target="_blank" rel="noopener">翻译</a></em></li><li>PyTorch进阶之路（四）：在GPU上训练神经网络 <em><a href="https://medium.com/jovian-io/training-deep-neural-networks-on-a-gpu-with-pytorch-11079d89805" target="_blank" rel="noopener">原文</a> <a href="https://www.jiqizhixin.com/articles/2019-04-09-9" target="_blank" rel="noopener">翻译</a></em></li><li>用PyTorch构建Faster RCNN <em><a href="https://medium.com/@fractaldle/guide-to-build-faster-rcnn-in-pytorch-95b10c273439" target="_blank" rel="noopener">原文</a> <a href="https://mp.weixin.qq.com/s/Vqdm9Tnqls45XBFWmQZLpA" target="_blank" rel="noopener">翻译</a></em></li><li>用TensorFlow 2.0构建简单图像分类器 <em><a href="https://towardsdatascience.com/easy-image-classification-with-tensorflow-2-0-f734fee52d13" target="_blank" rel="noopener">原文</a> <a href="https://www.jiqizhixin.com/articles/2019-04-10-11" target="_blank" rel="noopener">翻译</a></em></li><li>机器学习解释模型：黑盒VS白盒 <em><a href="https://towardsdatascience.com/machine-learning-interpretability-techniques-662c723454f3" target="_blank" rel="noopener">原文</a> <a href="https://mp.weixin.qq.com/s/R8caHrnfvCVHwuRI48bKag" target="_blank" rel="noopener">翻译</a></em></li><li>GAN的七个开放问题 <em><a href="https://distill.pub/2019/gan-open-problems/" target="_blank" rel="noopener">原文</a> <a href="https://www.jiqizhixin.com/articles/2019-04-12-14" target="_blank" rel="noopener">翻译</a></em></li></ul><h2 id="比赛方案、心得分享（TIANCHI-全球城市计算AI挑战赛专场）"><a href="#比赛方案、心得分享（TIANCHI-全球城市计算AI挑战赛专场）" class="headerlink" title="比赛方案、心得分享（TIANCHI-全球城市计算AI挑战赛专场）"></a>比赛方案、心得分享（<a href="https://tianchi.aliyun.com/competition/entrance/231708/introduction" target="_blank" rel="noopener">TIANCHI-全球城市计算AI挑战赛</a>专场）</h2><ul><li><a href="https://zhuanlan.zhihu.com/p/59998657" target="_blank" rel="noopener">TIANCHI全球城市计算AI挑战赛baseline</a></li><li><a href="https://zhuanlan.zhihu.com/p/61145712" target="_blank" rel="noopener">全球城市计算AI挑战赛赛后总结</a></li><li><a href="https://zhuanlan.zhihu.com/p/61576682" target="_blank" rel="noopener">阿里天池全球城市计算AI挑战赛</a></li><li><a href="https://zhuanlan.zhihu.com/p/62257700" target="_blank" rel="noopener">TIANCHI-全球城市计算挑战赛-完成方案及关键代码分享（季军）</a></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;知乎优秀问答推荐&quot;&gt;&lt;a href=&quot;#知乎优秀问答推荐&quot; class=&quot;headerlink&quot; title=&quot;知乎优秀问答推荐&quot;&gt;&lt;/a&gt;知乎优秀问答推荐&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;https://www.zhihu.com/question/266368852&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;在实际业务里，在工作中有什么用的到深度学习的例子吗？用到GPU了吗？&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://www.zhihu.com/question/299068775&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;当前（2019年）机器学习中有哪些研究方向特别的坑？&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>2019年04月01日—2019年04月07日周报</title>
    <link href="https://artificial-intelligence.net.cn/2019/04/10/1554831231/"/>
    <id>https://artificial-intelligence.net.cn/2019/04/10/1554831231/</id>
    <published>2019-04-09T17:33:51.000Z</published>
    <updated>2019-06-05T06:03:41.172Z</updated>
    
    <content type="html"><![CDATA[<h2 id="知乎优秀问答推荐"><a href="#知乎优秀问答推荐" class="headerlink" title="知乎优秀问答推荐"></a>知乎优秀问答推荐</h2><ul><li><a href="https://www.zhihu.com/question/318496731/answer/639932313" target="_blank" rel="noopener">2019年知乎值得关注的机器学习领域大佬</a></li><li><a href="https://www.zhihu.com/question/23647187" target="_blank" rel="noopener">没有导师的指导，研究生如何阅读文献、提出创见、写论文？</a></li></ul><a id="more"></a><ul><li><a href="https://zhuanlan.zhihu.com/p/58253325" target="_blank" rel="noopener">主成分分析（PCA）原理详解</a></li><li><a href="https://zhuanlan.zhihu.com/p/60745850" target="_blank" rel="noopener">关于做科研的态度和方法的一点感想</a></li><li><a href="https://www.zhihu.com/question/20829666" target="_blank" rel="noopener">第一次写学术论文无从下手怎么办？</a></li><li><a href="https://zhuanlan.zhihu.com/p/60146525" target="_blank" rel="noopener">那些酷炫的深度学习网络图怎么画出来的</a></li></ul><h2 id="名校公开课推荐"><a href="#名校公开课推荐" class="headerlink" title="名校公开课推荐"></a>名校公开课推荐</h2><ul><li><p><strong>Stanford CS229 : Machine Learning</strong></p><ul><li><a href="http://cs229.stanford.edu" target="_blank" rel="noopener">官方网站</a></li><li><a href="http://scpd.stanford.edu/search/publicCourseSearchDetails.do?method=load&amp;courseId=11763" target="_blank" rel="noopener">SCPD</a></li><li><a href="https://www.youtube.com/playlist?list=PLA89DCFA6ADACE599" target="_blank" rel="noopener">Youtube (2008)</a></li><li><a href="https://www.bilibili.com/video/av43634683" target="_blank" rel="noopener">Bilibili (2008，中文)</a></li><li><a href="http://open.163.com/special/opencourse/machinelearning.html" target="_blank" rel="noopener">网易公开课 (2008，中文)</a><br><br></li></ul></li><li><p><strong>Stanford CS229A : Applied Machine Learning</strong></p><ul><li><a href="https://web.stanford.edu/class/cs229a/" target="_blank" rel="noopener">官方网站</a></li><li><a href="https://www.coursera.org/learn/machine-learning" target="_blank" rel="noopener">Coursera</a></li><li><a href="https://study.163.com/course/introduction.htm?courseId=1004570029" target="_blank" rel="noopener">网易云课堂 (中文)</a><br><br></li></ul></li><li><p><strong>Stanford CS230 : Deep Learning</strong></p><ul><li><a href="http://cs230.stanford.edu" target="_blank" rel="noopener">官方网站</a></li><li><a href="http://scpd.stanford.edu/search/publicCourseSearchDetails.do?method=load&amp;courseId=82209222" target="_blank" rel="noopener">SCPD</a></li><li><a href="https://www.youtube.com/playlist?list=PLoROMvodv4rOABXSygHTsbvUz4G_YQhOb" target="_blank" rel="noopener">Youtube (Autumn 2018)</a></li><li><a href="https://www.bilibili.com/video/av47055599" target="_blank" rel="noopener">Bilibili (Autumn 2018，英文)</a></li><li><a href="https://www.coursera.org/specializations/deep-learning" target="_blank" rel="noopener">Coursera</a></li><li><a href="https://mooc.study.163.com/smartSpec/detail/1001319001.htm" target="_blank" rel="noopener">网易云课堂 (Coursera，中文)</a><br><br></li></ul></li><li><p><strong>Stanford CS224n : Natural Language Processing with Deep Learning</strong></p><ul><li><a href="http://web.stanford.edu/class/cs224n/" target="_blank" rel="noopener">官方网站</a></li><li><a href="http://scpd.stanford.edu/search/publicCourseSearchDetails.do?method=load&amp;courseId=11754" target="_blank" rel="noopener">SCPD</a></li><li><a href="https://www.youtube.com/playlist?list=PLoROMvodv4rOhcuXMZkNm7j3fVwBBY42z" target="_blank" rel="noopener">Youtube (Winter 2019)</a></li><li><a href="https://www.youtube.com/playlist?list=PL3FW7Lu3i5Jsnh1rnUwq_TcylNr7EkRe6" target="_blank" rel="noopener">Youtube (Winter 2017)</a></li><li><a href="http://www.mooc.ai/course/494" target="_blank" rel="noopener">AI研习社 (Winter 2017，中文)</a></li><li><a href="https://www.bilibili.com/video/av41393758" target="_blank" rel="noopener">Bilibili (Winter 2017，中文)</a></li><li><a href="https://www.bilibili.com/video/av46166627" target="_blank" rel="noopener">Bilibili (Winter 2019，英文)</a><br><br></li></ul></li><li><p><strong>Stanford CS231n : Convolutional Neural Networks for Visual Recognition</strong></p><ul><li><a href="http://cs231n.stanford.edu" target="_blank" rel="noopener">官方网站</a></li><li><a href="http://scpd.stanford.edu/search/publicCourseSearchDetails.do?method=load&amp;courseId=42262144" target="_blank" rel="noopener">SCPD</a></li><li><a href="https://www.youtube.com/playlist?list=PLkt2uSq6rBVctENoVBg1TpCC7OQi31AlC" target="_blank" rel="noopener">Youtube (Winter 2016)</a></li><li><a href="https://www.youtube.com/playlist?list=PL3FW7Lu3i5JvHM8ljYj-zLfQRF3EO8sYv" target="_blank" rel="noopener">Youtube (Spring 2017)</a></li><li><a href="https://study.163.com/course/courseMain.htm?courseId=1003223001&amp;_trace_c_p_k2_=6578bdc30f764b939dba5527117ec688" target="_blank" rel="noopener">网易云课堂 (Winter 2016，中文)</a></li><li><a href="http://www.mooc.ai/course/268" target="_blank" rel="noopener">AI研习社 (Spring 2017，中文)</a><br><br></li></ul></li><li><p><strong>Stanford CS234 : Reinforcement Learning</strong></p><ul><li><a href="http://web.stanford.edu/class/cs234/index.html" target="_blank" rel="noopener">官方网站</a></li><li><a href="http://scpd.stanford.edu/search/publicCourseSearchDetails.do?method=load&amp;courseId=75632440" target="_blank" rel="noopener">SCPD</a></li><li><a href="https://www.youtube.com/playlist?list=PLoROMvodv4rOSOPzutgyCTapiGlY2Nd8u" target="_blank" rel="noopener">Youtube (Winter 2019)</a></li><li><a href="https://www.bilibili.com/video/av47903063" target="_blank" rel="noopener">Bilibili (Winter 2019，英文)</a></li></ul></li></ul><h2 id="优秀开源书籍推荐"><a href="#优秀开源书籍推荐" class="headerlink" title="优秀开源书籍推荐"></a>优秀开源书籍推荐</h2><ul><li><p><strong>Andrew Ng《Machine Learning Yearning》开源书籍更新完毕</strong></p><ul><li><a href="https://www.mlyearning.org" target="_blank" rel="noopener">官方网站</a></li><li><a href="https://github.com/AcceptedDoge/machine-learning-yearning-cn" target="_blank" rel="noopener">官方中文翻译项目</a></li><li><a href="https://accepteddoge.com/machine-learning-yearning-cn/" target="_blank" rel="noopener">官方中文在线阅读地址</a></li><li><a href="https://github.com/AcceptedDoge/machine-learning-yearning-cn/blob/master/MLY-zh-cn.pdf" target="_blank" rel="noopener">官方中文pdf下载地址</a><br><br></li></ul></li><li><p><strong>机器学习经典书籍《Pattern Recognition and Machine Learning》开源</strong></p><ul><li><a href="https://www.microsoft.com/en-us/research/people/cmbishop/%23%21prml-book" target="_blank" rel="noopener">官方网站</a></li><li><a href="https://www.microsoft.com/en-us/research/uploads/prod/2006/01/Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf" target="_blank" rel="noopener">官方原版pdf下载地址</a></li><li><a href="http://prml.github.io/" target="_blank" rel="noopener">官方代码地址（Matlab）</a></li><li><a href="https://github.com/ctgk/PRML" target="_blank" rel="noopener">Python版开源项目</a></li><li><a href="https://pan.baidu.com/s/1tKtTYohp9ukLEXMucd_XpA" target="_blank" rel="noopener">中文版百度云盘 提取码: rn25</a><br><br></li></ul></li><li><p><strong>深度学习经典书籍花书《深度学习》开源</strong></p><ul><li><a href="http://www.deeplearningbook.org" target="_blank" rel="noopener">官方网站</a></li><li><a href="https://github.com/exacity/deeplearningbook-chinese" target="_blank" rel="noopener">官方中文翻译项目</a></li><li><a href="https://exacity.github.io/deeplearningbook-chinese/" target="_blank" rel="noopener">官方中文在线阅读地址</a></li><li><a href="https://github.com/exacity/deeplearningbook-chinese#%E9%9D%A2%E5%90%91%E7%9A%84%E8%AF%BB%E8%80%85" target="_blank" rel="noopener">官方中文pdf下载地址</a><br><br></li></ul></li><li><p><strong>李沐《动手学深度学习》开源书籍，中科大、Berkely等多所大学将其作为教材或参考书</strong></p><ul><li><a href="https://github.com/d2l-ai/d2l-zh" target="_blank" rel="noopener">官方网站</a></li><li><a href="https://zh.d2l.ai" target="_blank" rel="noopener">官方在线阅读地址</a></li><li><a href="https://zh.d2l.ai/d2l-zh.pdf" target="_blank" rel="noopener">官方pdf下载地址</a></li><li><a href="https://space.bilibili.com/209599371/channel/detail?cid=23541" target="_blank" rel="noopener">官方视频教程地址</a><br><br></li></ul></li><li><p><strong>复旦大学计算机科学技术学院副教授邱锡鹏新书《神经网络与深度学习》开源</strong></p><ul><li><a href="https://nndl.github.io" target="_blank" rel="noopener">书本相关信息及资料</a></li><li><a href="https://nndl.github.io/nndl-book.pdf" target="_blank" rel="noopener">全书内容 pdf 下载地址</a><br><br></li></ul></li><li><p><strong>Y Combinator Research 的研究员 Michael Nielsen开源书籍——《Neural Network and Deep Learning》</strong></p><ul><li><a href="http://neuralnetworksanddeeplearning.com/about.html" target="_blank" rel="noopener">官方在线阅读地址</a></li><li><a href="https://github.com/mnielsen/neural-networks-and-deep-learning" target="_blank" rel="noopener">官方代码地址</a></li><li><a href="https://github.com/zhanggyb/nndl/releases" target="_blank" rel="noopener">中文版下载地址</a></li></ul></li></ul><h2 id="优秀开源项目推荐"><a href="#优秀开源项目推荐" class="headerlink" title="优秀开源项目推荐"></a>优秀开源项目推荐</h2><ul><li><a href="https://github.com/zziz/pwc" target="_blank" rel="noopener">深度学习2008年至今顶会论文代码开源项目——Papers with Code</a></li><li><a href="https://github.com/hoya012/deep_learning_object_detection" target="_blank" rel="noopener">深度学习2014年至今目标检测资料开源项目——deep learning object detection</a></li><li><a href="https://datawhalechina.github.io/pumpkin-book" target="_blank" rel="noopener">周志华老师西瓜书《机器学习》公式推导开源项目——南瓜书PumpkinBook</a></li><li><a href="https://github.com/zergtant/pytorch-handbook" target="_blank" rel="noopener">Pytorch 1.0开源手册——PyTorch 中文手册（pytorch handbook）</a></li></ul><h2 id="技术文章推荐（15篇）"><a href="#技术文章推荐（15篇）" class="headerlink" title="技术文章推荐（15篇）"></a>技术文章推荐（15篇）</h2><ul><li><p>2019深度学习框架发展趋势及学习建议 <i><a href="https://towardsdatascience.com/which-deep-learning-framework-is-growing-fastest-3f77f14aa318" target="_blank" rel="noopener">原文</a> <a href="https://www.jiqizhixin.com/articles/2019-04-04-19" target="_blank" rel="noopener">翻译</a></i></p></li><li><p>理解梯度下降背后的数学原理 <i><a href="https://towardsdatascience.com/understanding-the-mathematics-behind-gradient-descent-dde5dc9be06e" target="_blank" rel="noopener">原文</a> <a href="https://www.jiqizhixin.com/articles/2019-04-07-6" target="_blank" rel="noopener">翻译</a></i></p></li><li><p>如何使用Google Colab免费TPU 20倍加速训练Keras模型 <i><a href="https://www.kdnuggets.com/2019/03/train-keras-model-20x-faster-tpu-free.html" target="_blank" rel="noopener">原文</a> <a href="https://www.jiqizhixin.com/articles/2019-04-04-18" target="_blank" rel="noopener">翻译</a></i></p></li><li><p>深度学习中的卷积核介绍 <i><a href="https://towardsdatascience.com/a-comprehensive-introduction-to-different-types-of-convolutions-in-deep-learning-669281e58215" target="_blank" rel="noopener">原文</a> <a href="https://www.jiqizhixin.com/articles/19022402" target="_blank" rel="noopener">翻译</a></i></p></li><li><p>高斯过程的可视化探索 <i><a href="https://www.jgoertler.com/visual-exploration-gaussian-processes/" target="_blank" rel="noopener">原文</a> <a href="https://www.jiqizhixin.com/articles/2019-02-12-3" target="_blank" rel="noopener">翻译</a></i></p></li><li><p>直观理解深度学习卷积部分 <i><a href="https://towardsdatascience.com/intuitively-understanding-convolutions-for-deep-learning-1f6f42faee1" target="_blank" rel="noopener">原文</a> <a href="https://mp.weixin.qq.com/s/f1y03f9OSrgda7EVwinXhQ" target="_blank" rel="noopener">翻译</a></i></p></li><li><p>理解神经网络：从神经元到RNN、CNN以及深度学习 <i><a href="https://towardsdatascience.com/understanding-neural-networks-from-neuron-to-rnn-cnn-and-deep-learning-cd88e90e0a90" target="_blank" rel="noopener">原文</a> <a href="https://mp.weixin.qq.com/s/_tg-tyle8ATZ6YgthMAiIA" target="_blank" rel="noopener">翻译</a></i></p></li><li><p>手动计算深度学习模型中的参数数量 <i><a href="https://towardsdatascience.com/counting-no-of-parameters-in-deep-learning-models-by-hand-8f1716241889" target="_blank" rel="noopener">原文</a> <a href="https://mp.weixin.qq.com/s/GN-5bAx1uL_KhyfmjxN_sQ" target="_blank" rel="noopener">翻译</a></i></p></li><li><p>用Pytorch实现一个图像分类器I <i><a href="https://medium.com/udacity/implementing-an-image-classifier-with-pytorch-part-1-cf5444b8e9c9" target="_blank" rel="noopener">原文</a> <a href="https://mp.weixin.qq.com/s/82PSV1J6oey5KqHEMW0ApQ" target="_blank" rel="noopener">翻译</a></i></p></li><li><p>用Pytorch实现一个图像分类器II <i><a href="https://medium.com/udacity/implementing-an-image-classifier-with-pytorch-part-2-ae4dd7b2f48" target="_blank" rel="noopener">原文</a> <a href="https://mp.weixin.qq.com/s/vIftCb6iDo62aWUZ4vif9g" target="_blank" rel="noopener">翻译</a></i></p></li><li><p>用Pytorch实现一个图像分类器III <i><a href="https://medium.com/udacity/implementing-an-image-classifier-with-pytorch-part-3-6ff66106ba89" target="_blank" rel="noopener">原文</a></i></p></li><li><p>用不到30行代码实现YOLO <i><a href="https://towardsdatascience.com/you-only-look-once-yolo-implementing-yolo-in-less-than-30-lines-of-python-code-97fb9835bfd2" target="_blank" rel="noopener">原文</a> <a href="https://mp.weixin.qq.com/s/SCHPOjXAxn5XsVw5zzjcjg" target="_blank" rel="noopener">翻译</a></i></p></li><li><p>简单实用的数据清洗代码 <i><a href="https://towardsdatascience.com/the-simple-yet-practical-data-cleaning-codes-ad27c4ce0a38" target="_blank" rel="noopener">原文</a> <a href="https://mp.weixin.qq.com/s/0roHUqigHq_k3eauTstESg" target="_blank" rel="noopener">翻译</a></i></p></li><li><p>Python数据清洗实战 <i><a href="https://medium.com/machine-intelligence-team/data-cleaning-with-python-d0ca811d6cdf" target="_blank" rel="noopener">原文</a> <a href="https://mp.weixin.qq.com/s/reoizZ3YNH4OHEGc_-ZKHQ" target="_blank" rel="noopener">翻译</a></i></p></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;知乎优秀问答推荐&quot;&gt;&lt;a href=&quot;#知乎优秀问答推荐&quot; class=&quot;headerlink&quot; title=&quot;知乎优秀问答推荐&quot;&gt;&lt;/a&gt;知乎优秀问答推荐&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;https://www.zhihu.com/question/318496731/answer/639932313&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;2019年知乎值得关注的机器学习领域大佬&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://www.zhihu.com/question/23647187&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;没有导师的指导，研究生如何阅读文献、提出创见、写论文？&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>Hello World</title>
    <link href="https://artificial-intelligence.net.cn/2019/04/09/hello-world/"/>
    <id>https://artificial-intelligence.net.cn/2019/04/09/hello-world/</id>
    <published>2019-04-09T14:42:20.421Z</published>
    <updated>2019-06-05T06:03:41.172Z</updated>
    
    <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/" target="_blank" rel="noopener">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html" target="_blank" rel="noopener">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues" target="_blank" rel="noopener">GitHub</a>.</p><a id="more"></a><h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo new <span class="string">"My New Post"</span></span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/writing.html" target="_blank" rel="noopener">Writing</a></p><h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/server.html" target="_blank" rel="noopener">Server</a></p><h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/generating.html" target="_blank" rel="noopener">Generating</a></p><h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/deployment.html" target="_blank" rel="noopener">Deployment</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Welcome to &lt;a href=&quot;https://hexo.io/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Hexo&lt;/a&gt;! This is your very first post. Check &lt;a href=&quot;https://hexo.io/docs/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;documentation&lt;/a&gt; for more info. If you get any problems when using Hexo, you can find the answer in &lt;a href=&quot;https://hexo.io/docs/troubleshooting.html&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;troubleshooting&lt;/a&gt; or you can ask me on &lt;a href=&quot;https://github.com/hexojs/hexo/issues&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;GitHub&lt;/a&gt;.&lt;/p&gt;
    
    </summary>
    
    
  </entry>
  
</feed>
